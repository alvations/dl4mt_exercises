{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import pylab\n",
    "import sys\n",
    "# making sure the dl4mt module is on the path \n",
    "# -- this path depends upon the location where the notebook is running\n",
    "# here it is assumed to be the day1/ directory\n",
    "sys.path.append('../..')\n",
    "\n",
    "pylab.rcParams['figure.figsize'] = (10.0, 8.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ../../dl4mt/logistic_regression.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ../../dl4mt/logistic_regression.py\n",
    "# Uncomment to save this cell to file in order to import it in later notebooks\n",
    "\n",
    "import cPickle\n",
    "import gzip\n",
    "import os\n",
    "import sys\n",
    "import timeit\n",
    "\n",
    "import numpy\n",
    "\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "\n",
    "\n",
    "class LogisticRegression(object):\n",
    "    \"\"\"Multi-class Logistic Regression Class\n",
    "\n",
    "    The logistic regression is fully described by a weight matrix :math:`W`\n",
    "    and bias vector :math:`b`. Classification is done by projecting data\n",
    "    points onto a set of hyperplanes, the distance to which is used to\n",
    "    determine a class membership probability.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input, n_in, n_out):\n",
    "        \"\"\" Initialize the parameters of the logistic regression\n",
    "\n",
    "        :type input: theano.tensor.TensorType\n",
    "        :param input: symbolic variable that describes the input of the\n",
    "                      architecture (one minibatch)\n",
    "\n",
    "        :type n_in: int\n",
    "        :param n_in: number of input units, the dimension of the space in\n",
    "                     which the datapoints lie\n",
    "\n",
    "        :type n_out: int\n",
    "        :param n_out: number of output units, the dimension of the space in\n",
    "                      which the labels lie\n",
    "\n",
    "        \"\"\"\n",
    "  \n",
    "        # initialize with 0 the weights W as a matrix of shape (n_in, n_out)\n",
    "        self.W = theano.shared(\n",
    "            value=numpy.zeros(\n",
    "                (n_in, n_out),\n",
    "                dtype=theano.config.floatX\n",
    "            ),\n",
    "            name='W',\n",
    "            borrow=True\n",
    "        )\n",
    "        # initialize the biases b as a vector of n_out 0s\n",
    "        self.b = theano.shared(\n",
    "            value=numpy.zeros(\n",
    "                (n_out,),\n",
    "                dtype=theano.config.floatX\n",
    "            ),\n",
    "            name='b',\n",
    "            borrow=True\n",
    "        )\n",
    "\n",
    "        # symbolic expression for computing the matrix of class-membership\n",
    "        # probabilities\n",
    "        # Where:\n",
    "        # W is a matrix where column-k represent the separation hyperplane for\n",
    "        # class-k\n",
    "        # x is a matrix where row-j  represents input training sample-j\n",
    "        # b is a vector where element-k represents the free parameter of\n",
    "        # hyperplane-k\n",
    "        self.p_y_given_x = T.nnet.softmax(T.dot(input, self.W) + self.b)\n",
    "\n",
    "        # symbolic description of how to map p_y_given_x to the class with the maximum probability\n",
    "        self.y_pred = T.argmax(self.p_y_given_x, axis=1)\n",
    "    \n",
    "        # parameters of the model\n",
    "        self.params = [self.W, self.b]\n",
    "\n",
    "        # keep track of model input -- this will be useful for creating theano functions\n",
    "        self.input = input\n",
    "\n",
    "    def predict(self, X):\n",
    "        # predict X using the parameters of this model\n",
    "        return T.nnet.softmax(T.dot(X, self.W) + self.b)\n",
    "        \n",
    "    # TODO: put math in a separate cell with LaTeX + markdown -- \n",
    "    # TODO: implement this as a challenge -- add tests in the following cell to see if it's correct\n",
    "    def negative_log_likelihood(self, y):\n",
    "        \"\"\"Return the mean of the negative log-likelihood of the prediction\n",
    "        of this model under a given target distribution.\n",
    "\n",
    "        :type y: theano.tensor.TensorType\n",
    "        :param y: corresponds to a vector that gives for each example the\n",
    "                  correct label\n",
    "\n",
    "        Note: we use the mean instead of the sum so that\n",
    "              the learning rate is less dependent on the batch size\n",
    "        \"\"\"\n",
    "\n",
    "        # y.shape[0] is (symbolically) the number of rows in y, i.e.,\n",
    "        # number of examples (call it n) in the minibatch\n",
    "        # T.arange(y.shape[0]) is a symbolic vector which will contain\n",
    "        # [0,1,2,... n-1] T.log(self.p_y_given_x) is a matrix of\n",
    "        # Log-Probabilities (call it LP) with one row per example and\n",
    "        # one column per class LP[T.arange(y.shape[0]),y] is a vector\n",
    "        # v containing [LP[0,y[0]], LP[1,y[1]], LP[2,y[2]], ...,\n",
    "        # LP[n-1,y[n-1]]] and T.mean(LP[T.arange(y.shape[0]),y]) is\n",
    "        # the mean (across minibatch examples) of the elements in v,\n",
    "        # i.e., the mean log-likelihood across the minibatch.\n",
    "        return -T.mean(T.log(self.p_y_given_x)[T.arange(y.shape[0]), y])\n",
    "\n",
    "    def errors(self, y):\n",
    "        \"\"\"Return a float representing the number of errors in the minibatch\n",
    "        over the total number of examples of the minibatch ; zero one\n",
    "        loss over the size of the minibatch\n",
    "\n",
    "        :type y: theano.tensor.TensorType\n",
    "        :param y: corresponds to a vector that gives for each example the\n",
    "                  correct label\n",
    "        \"\"\"\n",
    "\n",
    "        # check if y has same dimension of y_pred\n",
    "        if y.ndim != self.y_pred.ndim:\n",
    "            raise TypeError(\n",
    "                'y should have the same shape as self.y_pred',\n",
    "                ('y', y.type, 'y_pred', self.y_pred.type)\n",
    "            )\n",
    "        # check if y is of the correct datatype\n",
    "        if y.dtype.startswith('int'):\n",
    "            # the T.neq operator returns a vector of 0s and 1s, where 1\n",
    "            # represents a mistake in prediction\n",
    "            return T.mean(T.neq(self.y_pred, y))\n",
    "        else:\n",
    "            raise NotImplementedError()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from dl4mt.logistic_regression import LogisticRegression\n",
    "\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "\n",
    "# TODO: switch to google code doc style\n",
    "\n",
    "# this function is unique to this model, and initializes the functions and parameters we'll need for training\n",
    "def initialize_logistic_regression(train_dataset, dev_dataset, learning_rate=0.1, batch_size=10):\n",
    "    \n",
    "    \"\"\"Initializes the functions and parameters for a Logistic Regression model\n",
    "\n",
    "    :type learning_rate: float\n",
    "    :param learning_rate: learning rate used (factor for the stochastic\n",
    "                          gradient)\n",
    "\n",
    "    :type n_epochs: int\n",
    "    :param n_epochs: maximal number of epochs to run the optimizer\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    train_set_x, train_set_y = train_dataset\n",
    "    valid_set_x, valid_set_y = dev_dataset\n",
    "\n",
    "    # compute number of minibatches for training, validation and testing\n",
    "    n_train_batches = train_set_x.get_value(borrow=True).shape[0] / batch_size\n",
    "    n_valid_batches = valid_set_x.get_value(borrow=True).shape[0] / batch_size\n",
    "\n",
    "    # create the logistic regression model\n",
    "    # allocate symbolic variables for the data\n",
    "    index = T.lscalar()  # index to a [mini]batch\n",
    "\n",
    "    # generate symbolic variables for input (x and y represent a minibatch)\n",
    "    x = T.matrix('x')  # data\n",
    "    y = T.ivector('y')  # labels, presented as 1D vector of [int] labels\n",
    "\n",
    "    # TODO: parameterize  n_out outside the training function -- try to get directly from data\n",
    "    n_in = train_set_x.get_value().shape[1]\n",
    "    n_out = 12\n",
    "    \n",
    "    classifier = LogisticRegression(input=x, n_in=n_in, n_out=n_out)\n",
    "\n",
    "    # the cost we minimize during training is the negative log likelihood of the model\n",
    "    cost = classifier.negative_log_likelihood(y)\n",
    "\n",
    "    validate_model_func = theano.function(\n",
    "        inputs=[index],\n",
    "        outputs=classifier.errors(y),\n",
    "        givens={\n",
    "            x: valid_set_x[index * batch_size: (index + 1) * batch_size],\n",
    "            y: valid_set_y[index * batch_size: (index + 1) * batch_size]\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # compute the gradient of cost with respect to theta = (W,b)\n",
    "    g_W = T.grad(cost=cost, wrt=classifier.W)\n",
    "    g_b = T.grad(cost=cost, wrt=classifier.b)\n",
    "\n",
    "    # specify how to update the parameters of the model as a list of\n",
    "    # (variable, update expression) pairs.\n",
    "    updates = [(classifier.W, classifier.W - learning_rate * g_W),\n",
    "               (classifier.b, classifier.b - learning_rate * g_b)]\n",
    "\n",
    "    # compiling a Theano function `train_model` that returns the cost, but in\n",
    "    # the same time updates the parameter of the model based on the rules\n",
    "    # defined in `updates`\n",
    "    train_model_func = theano.function(\n",
    "        inputs=[index],\n",
    "        outputs=cost,\n",
    "        updates=updates,\n",
    "        givens={\n",
    "            x: train_set_x[index * batch_size: (index + 1) * batch_size],\n",
    "            y: train_set_y[index * batch_size: (index + 1) * batch_size]\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # return the classifier, the training and validation functions, and n_train_batches, n_valid_batches\n",
    "    return (classifier, train_model_func, validate_model_func, n_train_batches, n_valid_batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ../../dl4mt/training.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ../../dl4mt/training.py\n",
    "\n",
    "# this function is shared by all day1 models which are trained with SGD\n",
    "import timeit\n",
    "import numpy\n",
    "\n",
    "def train_model(train_model, n_train_batches, validate_model=None, n_valid_batches=0, training_epochs=25):\n",
    "    \n",
    "    # early-stopping parameters\n",
    "    patience = 5000  # look at this many examples regardless\n",
    "    patience_increase = 2  # wait this much longer when a new best is\n",
    "                                  # found\n",
    "    improvement_threshold = 0.995  # a relative improvement of this much is\n",
    "                                  # considered significant\n",
    "    validation_frequency = min(n_train_batches, patience / 2)\n",
    "                                  # go through this many\n",
    "                                  # minibatches before checking the network\n",
    "                                  # on the validation set; in this case we\n",
    "                                  # check every epoch\n",
    "\n",
    "    best_validation_loss = numpy.inf\n",
    "    test_score = 0.\n",
    "    start_time = timeit.default_timer()\n",
    "\n",
    "    done_looping = False\n",
    "    epoch = 0\n",
    "    while (epoch < training_epochs) and (not done_looping):\n",
    "        epoch = epoch + 1\n",
    "        mini_batch_costs = []\n",
    "        for minibatch_index in xrange(n_train_batches):\n",
    "            \n",
    "            # iteration number\n",
    "            iter = (epoch - 1) * n_train_batches + minibatch_index\n",
    "            \n",
    "            minibatch_avg_cost = train_model(minibatch_index)\n",
    "            mini_batch_costs.append(minibatch_avg_cost)\n",
    "            \n",
    "            if validate_model is not None:\n",
    "                if (iter + 1) % validation_frequency == 0:\n",
    "                    # compute zero-one loss on validation set\n",
    "                    validation_losses = [validate_model(i)\n",
    "                                         for i in xrange(n_valid_batches)]\n",
    "                    this_validation_loss = numpy.mean(validation_losses)\n",
    "\n",
    "                    print(\n",
    "                        'epoch %i, minibatch %i/%i, validation error %f %%' %\n",
    "                        (\n",
    "                            epoch,\n",
    "                            minibatch_index + 1,\n",
    "                            n_train_batches,\n",
    "                            this_validation_loss * 100.\n",
    "                        )\n",
    "                    )\n",
    "\n",
    "                    # if we got the best validation score until now\n",
    "                    if this_validation_loss < best_validation_loss:\n",
    "                        #improve patience if loss improvement is good enough\n",
    "                        if this_validation_loss < best_validation_loss *  \\\n",
    "                           improvement_threshold:\n",
    "                            patience = max(patience, iter * patience_increase)\n",
    "\n",
    "                        best_validation_loss = this_validation_loss\n",
    "\n",
    "\n",
    "                if patience <= iter:\n",
    "                    done_looping = True\n",
    "                    break\n",
    "         \n",
    "        print(\n",
    "                'epoch %i, average training cost %0.2f' %\n",
    "                (\n",
    "                    epoch,\n",
    "                    numpy.mean(mini_batch_costs)\n",
    "                )\n",
    "        )\n",
    "                \n",
    "\n",
    "    end_time = timeit.default_timer()\n",
    "    print('Optimization complete!')\n",
    "    if best_validation_loss < numpy.inf:\n",
    "        print(\"Best validation score: %f %%\") % (best_validation_loss * 100.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ../../dl4mt/datasets.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ../../dl4mt/datasets.py\n",
    "# Uncomment to save this cell to file in order to import it in later notebooks\n",
    "\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "import numpy as np\n",
    "from fuel.datasets import H5PYDataset\n",
    "\n",
    "def create_hstacked_vectors(index_path, seqs):\n",
    "    # load the index, map the seqs (word indices) through the index, hstack, then cast the results to np.array\n",
    "    index = np.load(index_path)\n",
    "    \n",
    "    working_index = np.load(index_path)\n",
    "    return np.array([np.hstack([index[idx] for idx in seq]) for seq in seqs])\n",
    "\n",
    "# load a portion of a fuel dataset\n",
    "def load_fuel_dataset(filename, which_sets=None):\n",
    "#     TODO: if which_sets is not None return all sets\n",
    "\n",
    "    X, y = H5PYDataset(\n",
    "        filename, which_sets=which_sets,\n",
    "        sources=['instances', 'targets'], load_in_memory=True).data_sources\n",
    "    \n",
    "    # make sure y is 0-dimensional\n",
    "    y = y.ravel()\n",
    "    \n",
    "    return (X,y)\n",
    "    \n",
    "    \n",
    "def shared_dataset(X, y, borrow=True, cast_y=True):\n",
    "    \"\"\" Function that loads the dataset into shared variables\n",
    "\n",
    "    The reason we store our dataset in shared variables is to allow\n",
    "    Theano to copy it into the GPU memory (when code is run on GPU).\n",
    "    Since copying data into the GPU is slow, copying a minibatch everytime\n",
    "    is needed (the default behaviour if the data is not in a shared\n",
    "    variable) would lead to a large decrease in performance.\n",
    "    \"\"\"\n",
    "\n",
    "    shared_x = theano.shared(np.asarray(X,\n",
    "                                           dtype=theano.config.floatX), borrow=borrow)\n",
    "    shared_y = theano.shared(np.asarray(y,\n",
    "                                           dtype=theano.config.floatX), borrow=borrow)\n",
    " \n",
    "    # note that shared_y is cast to int because it is used as a vector of indices\n",
    "    if cast_y:\n",
    "        return shared_x, T.cast(shared_y, 'int32')\n",
    "    return shared_x, shared_y\n",
    "\n",
    "def prep_dataset(path, index_path, which_sets=[], cutoff=None, cast_y=True):\n",
    "    X, y = load_fuel_dataset(path, which_sets=which_sets)\n",
    "    if cutoff is not None:\n",
    "        X = X[:cutoff]\n",
    "        y = y[:cutoff]\n",
    "\n",
    "    # extract windows\n",
    "    X = create_hstacked_vectors(index_path, X)\n",
    "\n",
    "    # make shared dataset\n",
    "    return shared_dataset(X, y, cast_y=cast_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, minibatch 1000/1000, validation error 18.452787 %\n",
      "epoch 1, average training cost 0.63\n",
      "epoch 2, minibatch 1000/1000, validation error 16.523322 %\n",
      "epoch 2, average training cost 0.41\n",
      "epoch 3, minibatch 1000/1000, validation error 15.877133 %\n",
      "epoch 3, average training cost 0.37\n",
      "epoch 4, minibatch 1000/1000, validation error 15.524460 %\n",
      "epoch 4, average training cost 0.35\n",
      "epoch 5, minibatch 1000/1000, validation error 15.265074 %\n",
      "epoch 5, average training cost 0.34\n",
      "epoch 6, minibatch 1000/1000, validation error 15.110353 %\n",
      "epoch 6, average training cost 0.33\n",
      "epoch 7, minibatch 1000/1000, validation error 14.994312 %\n",
      "epoch 7, average training cost 0.32\n",
      "epoch 8, minibatch 1000/1000, validation error 14.923777 %\n",
      "epoch 8, average training cost 0.32\n",
      "epoch 9, minibatch 1000/1000, validation error 14.846416 %\n",
      "epoch 9, average training cost 0.31\n",
      "epoch 10, minibatch 1000/1000, validation error 14.784983 %\n",
      "epoch 10, average training cost 0.31\n",
      "epoch 11, minibatch 1000/1000, validation error 14.744027 %\n",
      "epoch 11, average training cost 0.31\n",
      "epoch 12, minibatch 1000/1000, validation error 14.748578 %\n",
      "epoch 12, average training cost 0.30\n",
      "epoch 13, minibatch 1000/1000, validation error 14.730375 %\n",
      "epoch 13, average training cost 0.30\n",
      "epoch 14, minibatch 1000/1000, validation error 14.737201 %\n",
      "epoch 14, average training cost 0.30\n",
      "epoch 15, minibatch 1000/1000, validation error 14.707622 %\n",
      "epoch 15, average training cost 0.30\n",
      "epoch 16, minibatch 1000/1000, validation error 14.700796 %\n",
      "epoch 16, average training cost 0.30\n",
      "epoch 17, minibatch 1000/1000, validation error 14.693970 %\n",
      "epoch 17, average training cost 0.29\n",
      "epoch 18, average training cost 0.29\n",
      "Optimization complete!\n",
      "Best validation score: 14.693970 %\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "from dl4mt.datasets import prep_dataset\n",
    "from dl4mt.training import train_model\n",
    "\n",
    "# location of our datasets\n",
    "DATASET_LOCATION = '../../datasets/'\n",
    "\n",
    "# the pos dataset consists of windows around words\n",
    "POS_DATASET_NAME = 'brown_pos_dataset.hdf5'\n",
    "POS_DATASET_PATH = os.path.join(DATASET_LOCATION, POS_DATASET_NAME)\n",
    "\n",
    "WORD_BY_WORD_MATRIX = 'brown.word-by-word.normalized.npy'\n",
    "# LSI matrix instead of word-by-word\n",
    "# WORD_BY_WORD_MATRIX = 'brown.lsi-index.npy'\n",
    "VECTOR_INDEX_PATH = os.path.join(DATASET_LOCATION, WORD_BY_WORD_MATRIX)\n",
    "\n",
    "# the cutoff for the maximum number of training and dev examples\n",
    "CUTOFF = 50000\n",
    "  \n",
    "train_dataset = prep_dataset(POS_DATASET_PATH, VECTOR_INDEX_PATH, which_sets=['train'], cutoff=CUTOFF)\n",
    "dev_dataset = prep_dataset(POS_DATASET_PATH, VECTOR_INDEX_PATH, which_sets=['dev'], cutoff=CUTOFF)\n",
    "\n",
    "# get the functions and params that we need for our models\n",
    "initialization_data = initialize_logistic_regression(train_dataset, dev_dataset,\n",
    "                                                     learning_rate=0.1, batch_size=50)\n",
    "\n",
    "classifier, train_model_func, validate_model_func, n_train_batches, n_valid_batches = initialization_data\n",
    "    \n",
    "# load the training function and train the LR model \n",
    "# TODO: -- train_model should return the validation error as a list, then we can plot it\n",
    "# in general, train_model should return any useful information about the training process\n",
    "train_model(train_model_func,  n_train_batches, validate_model=validate_model_func,\n",
    "            n_valid_batches=n_valid_batches, training_epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ../../dl4mt/predict.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ../../dl4mt/predict.py\n",
    "\n",
    "import theano\n",
    "\n",
    "# TODO: how to do multiple types in docstring?\n",
    "# TODO -- we also want a prediction function for the test data -- should also be common to all models\n",
    "# TODO: model evaluation and visualization should also be shared across notebooks\n",
    "\n",
    "def predict(classifier, test_X):\n",
    "    \"\"\"Get a model's predictions for a given X\n",
    "    classifier is expected to be either: (1) a theano graph with `input` and `y_pred`,\n",
    "    or a path to a pickled model\n",
    "    \"\"\"\n",
    "\n",
    "    if type(classifier) is str:\n",
    "        # load the saved model at the path specified by 'classifier'\n",
    "        classifier = cPickle.load(open(classifier))\n",
    "\n",
    "    # compile a predictor function\n",
    "    predict_model = theano.function(\n",
    "        inputs=[classifier.input],\n",
    "        outputs=classifier.y_pred)\n",
    "\n",
    "    y_hat = predict_model(test_X)\n",
    "    \n",
    "    return y_hat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO: allow user to load another distributed representation for the same data, such as PCA, or autoencoder vecs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import cPickle\n",
    "from dl4mt.predict import predict\n",
    "\n",
    "test_dataset = prep_dataset(POS_DATASET_PATH, VECTOR_INDEX_PATH, which_sets=['test'], cutoff=CUTOFF, cast_y=False)\n",
    "\n",
    "test_X, test_y = test_dataset\n",
    "test_y = test_y.get_value().astype('int32')\n",
    "predictions = predict(classifier, test_X.get_value())\n",
    "\n",
    "CORPUS_INDICES = 'brown_pos_dataset.indices'\n",
    "# Indexes for mapping words and tags <--> ints\n",
    "with open(os.path.join(DATASET_LOCATION, CORPUS_INDICES)) as indices_file:\n",
    "    corpus_indices = cPickle.load(indices_file)\n",
    "\n",
    "# map tag ids back to strings\n",
    "y_test_actual = [corpus_indices['idx2tag'][tag_idx] for tag_idx in test_y]\n",
    "y_test_hat = [corpus_indices['idx2tag'][tag_idx] for tag_idx in predictions]\n",
    "# get class names\n",
    "class_names = list(set(y_test_actual))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.86169631887882792"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum([y==p for y,p in zip(predictions, test_y)]) / float(len(predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO: visualize the error with different training hyperparams\n",
    "# TODO: functions for mapping back to the original dataset to check what we got wrong and why\n",
    "# TODO: add a theano graph visualization since this model is relatively simple\n",
    "# TODO: save the model for later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAokAAAJJCAYAAADGP62VAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nTF/dKLVowwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f599ea79e90>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn import svm, datasets\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "def plot_confusion_matrix(cm, title='Confusion matrix', cmap=plt.cm.Blues):\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(class_names))\n",
    "    plt.xticks(tick_marks, class_names, rotation=45)\n",
    "    plt.yticks(tick_marks, class_names)\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "\n",
    "\n",
    "# Compute confusion matrix\n",
    "cm = confusion_matrix(y_test_actual, y_test_hat)\n",
    "np.set_printoptions(precision=2)\n",
    "\n",
    "# Normalize the confusion matrix by row (i.e by the number of samples\n",
    "# in each class)\n",
    "cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "# print('Normalized confusion matrix')\n",
    "# print(cm_normalized)\n",
    "plt.figure()\n",
    "plot_confusion_matrix(cm_normalized, title='Normalized confusion matrix')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from https://github.com/slavpetrov/universal-pos-tags\n",
    "\n",
    "VERB - verbs (all tenses and modes)\n",
    "NOUN - nouns (common and proper)\n",
    "PRON - pronouns \n",
    "ADJ - adjectives\n",
    "ADV - adverbs\n",
    "ADP - adpositions (prepositions and postpositions)\n",
    "CONJ - conjunctions\n",
    "DET - determiners\n",
    "NUM - cardinal numbers\n",
    "PRT - particles or other function words\n",
    "X - other: foreign words, typos, abbreviations\n",
    ". - punctuation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "                    if save_model_path is not None:\n",
    "                        # save the best model\n",
    "                        with open(save_model + '.pkl', 'w') as f:\n",
    "                            cPickle.dump(classifier, f)\n",
    "                        print(save)\n",
    "\n",
    "\n",
    "\n",
    "        .. math::\n",
    "\n",
    "            \\frac{1}{|\\mathcal{D}|} \\mathcal{L} (\\theta=\\{W,b\\}, \\mathcal{D}) =\n",
    "            \\frac{1}{|\\mathcal{D}|} \\sum_{i=0}^{|\\mathcal{D}|}\n",
    "                \\log(P(Y=y^{(i)}|x^{(i)}, W,b)) \\\\\n",
    "            \\ell (\\theta=\\{W,b\\}, \\mathcal{D})\n",
    "\n",
    "\n",
    "\n",
    "#     n_test_batches = test_set_x.get_value(borrow=True).shape[0] / batch_size\n",
    "\n",
    "# def predict():\n",
    "#     \"\"\"\n",
    "#     An example of how to load a trained model and use it\n",
    "#     to predict labels.\n",
    "#     \"\"\"\n",
    "\n",
    "#     # load the saved model\n",
    "#     classifier = cPickle.load(open('best_model.pkl'))\n",
    "\n",
    "#     # compile a predictor function\n",
    "#     predict_model = theano.function(\n",
    "#         inputs=[classifier.input],\n",
    "#         outputs=classifier.y_pred)\n",
    "\n",
    "#     # We can test it on some examples from test test\n",
    "#     dataset='mnist.pkl.gz'\n",
    "#     datasets = load_data(dataset)\n",
    "#     test_set_x, test_set_y = datasets[2]\n",
    "#     test_set_x = test_set_x.get_value()\n",
    "\n",
    "#     predicted_values = predict_model(test_set_x[:10])\n",
    "#     print (\"Predicted values for the first 10 examples in test set:\")\n",
    "#     print predicted_values\n",
    "\n",
    "\n",
    "# test it on the test set -- TODO: testing should be done outside of training -- move this\n",
    "#                     test_losses = [test_model(i)\n",
    "#                                    for i in xrange(n_test_batches)]\n",
    "#                     test_score = numpy.mean(test_losses)\n",
    "\n",
    "#                     print(\n",
    "#                         (\n",
    "#                             '     epoch %i, minibatch %i/%i, test error of'\n",
    "#                             ' best model %f %%'\n",
    "#                         ) %\n",
    "#                         (\n",
    "#                             epoch,\n",
    "#                             minibatch_index + 1,\n",
    "#                             n_train_batches,\n",
    "#                             test_score * 100.\n",
    "#                         )\n",
    "#                     )\n",
    "\n",
    "#     print 'The code run for %d epochs, with %f epochs/sec' % (\n",
    "#         epoch, 1. * epoch / (end_time - start_time))\n",
    "#     print >> sys.stderr, ('The code for file ' +\n",
    "#                           os.path.split(__file__)[1] +\n",
    "#                           ' ran for %.1fs' % ((end_time - start_time)))\n",
    "\n",
    "    # compiling a Theano function that computes the mistakes that are made by\n",
    "    # the model on a minibatch\n",
    "#     test_model = theano.function(\n",
    "#         inputs=[index],\n",
    "#         outputs=classifier.errors(y),\n",
    "#         givens={\n",
    "#             x: test_set_x[index * batch_size: (index + 1) * batch_size],\n",
    "#             y: test_set_y[index * batch_size: (index + 1) * batch_size]\n",
    "#         }\n",
    "#     )\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "This tutorial introduces logistic regression using Theano and stochastic\n",
    "gradient descent.\n",
    "\n",
    "Logistic regression is a probabilistic, linear classifier. It is parametrized\n",
    "by a weight matrix :math:`W` and a bias vector :math:`b`. Classification is\n",
    "done by projecting data points onto a set of hyperplanes, the distance to\n",
    "which is used to determine a class membership probability.\n",
    "\n",
    "Mathematically, this can be written as:\n",
    "\n",
    ".. math::\n",
    "  P(Y=i|x, W,b) &= softmax_i(W x + b) \\\\\n",
    "                &= \\frac {e^{W_i x + b_i}} {\\sum_j e^{W_j x + b_j}}\n",
    "\n",
    "\n",
    "The output of the model or prediction is then done by taking the argmax of\n",
    "the vector whose i'th element is P(Y=i|x).\n",
    "\n",
    ".. math::\n",
    "\n",
    "  y_{pred} = argmax_i P(Y=i|x,W,b)\n",
    "\n",
    "\n",
    "This tutorial presents a stochastic gradient descent optimization method\n",
    "suitable for large datasets.\n",
    "\n",
    "\n",
    "References:\n",
    "\n",
    "    - textbooks: \"Pattern Recognition and Machine Learning\" -\n",
    "                 Christopher M. Bishop, section 4.3.2\n",
    "\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
