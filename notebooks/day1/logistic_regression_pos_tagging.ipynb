{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from __future__ import division, print_function\n",
    "import codecs\n",
    "import re\n",
    "import json\n",
    "import random\n",
    "import math\n",
    "import os\n",
    "import cPickle\n",
    "from collections import Counter, defaultdict\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pylab\n",
    "import pandas as pd\n",
    "from scipy.stats import norm\n",
    "import nltk\n",
    "from fuel.datasets import H5PYDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# now create the fuel dataset\n",
    "DATASET_LOCATION = 'datasets/'\n",
    "DATASET_NAME = 'brown_pos_dataset.hdf5'\n",
    "DATASET_PATH = os.path.join(DATASET_LOCATION, DATASET_NAME)\n",
    "\n",
    "with open(os.path.join(DATASET_LOCATION, 'brown_pos_dataset.indices')) as indices_file:\n",
    "    corpus_indices = cPickle.load(indices_file)\n",
    "\n",
    "# in order to use Logistic Regression for POS tagging, we need some features for our words\n",
    "# so let's get them from an SVD \n",
    "# -- another option is to use a pre-trained index so that the input is the same for every model\n",
    "\n",
    "# for the NN examples, we'll either train our embeddings from scratch, or pre-initialize with Glove or W2V\n",
    "# build a sparse matrix for all of our instances in the train set\n",
    "\n",
    "# if a word in test or dev isn't in train, map it to u'_UNK_'\n",
    "# the training data dictates the words we know and don't know\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# ok lets load the brown corpus, and use the indexes to convert it to ints,\n",
    "# then build tfidf, then transpose to get w X d\n",
    "\n",
    "UNKNOWN_TOKEN = u'_UNK_'\n",
    "\n",
    "def map_to_index(tok, index):\n",
    "    if tok in index:\n",
    "        return index[tok]\n",
    "    else:\n",
    "        return index[UNKNOWN_TOKEN]\n",
    "\n",
    "brown_documents = [[w for p in d for w in p] for d in nltk.corpus.brown.paras()]\n",
    "brown_documents = [[map_to_index(w, corpus_indices['word2idx']) for w in d] for d in brown_documents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9769\n"
     ]
    }
   ],
   "source": [
    "brown_vocab_size = len(corpus_indices['word2idx'].keys())\n",
    "print(brown_vocab_size)\n",
    "\n",
    "# for each doc, fill in the word counts of that row\n",
    "# then take the transpose to get VxD\n",
    "# allocate np.array of 0s with dims DxV\n",
    "brown_doc_tf = np.zeros((len(brown_documents), brown_vocab_size), dtype='uint16')\n",
    "for doc_id, doc in enumerate(brown_documents):\n",
    "    counts = Counter(doc)\n",
    "    words, counts = zip(*counts.items())\n",
    "    brown_doc_tf[doc_id, words] = counts\n",
    "    \n",
    "brown_word_tf = brown_doc_tf.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# convert to sparse binary\n",
    "binary_word_by_doc = [[(d,1) for d in np.nonzero(row)[0]] for row in brown_word_tf]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# ok let's train some vector spaces to use as features for our words\n",
    "from gensim import corpora, models, similarities\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'corpus_indices' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-087e95d241a9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mthe_idx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcorpus_indices\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'word2idx'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'the'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mword_doc_counts\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mthe_idx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m# convert to sparse binary\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'corpus_indices' is not defined"
     ]
    }
   ],
   "source": [
    "the_idx = corpus_indices['word2idx']['the']\n",
    "\n",
    "word_doc_counts[the_idx]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:gensim.models.lsimodel:no word id mapping provided; initializing from corpus, assuming identity\n",
      "WARNING:gensim.similarities.docsim:scanning corpus to determine the number of features (consider setting `num_features` explicitly)\n"
     ]
    }
   ],
   "source": [
    "# this creates a dictionary mapping tokens in the corpus to integer ids \n",
    "# working -- check this implementation\n",
    "# global_dictionary = corpora.Dictionary(all_semeval_toks)\n",
    "\n",
    "# TODO: really we want the BOW to be built from the original Brown sentences, not from our windowed representations\n",
    "# TODO: but that wouldn't be fair, because the NNs will only have access to the windows\n",
    "# TODO: we need to flip the index in order to get word vectors \n",
    "\n",
    "# create a corpus from the documents -- (word_id, word_freq)\n",
    "# global_corpus = [Counter(instance).items() for instance in train_X_list]\n",
    "# global_corpus = train_X_list\n",
    "\n",
    "global_corpus = binary_word_by_doc\n",
    "\n",
    "# create a tfidf transformation from our corpus of counts\n",
    "global_tfidf_transformation = models.TfidfModel(global_corpus)\n",
    "global_corpus_tfidf = global_tfidf_transformation[global_corpus]\n",
    "# global_tfidf_index = similarities.MatrixSimilarity(global_corpus_tfidf)\n",
    "\n",
    "# dataset_names = all_semeval.get_dataset_names()\n",
    "\n",
    "# lsi = models.LsiModel(global_corpus_tfidf, id2word=global_dictionary, num_topics=50) # initialize an LSI transformation\n",
    "lsi = models.LsiModel(global_corpus_tfidf, num_topics=50) # initialize an LSI transformation\n",
    "\n",
    "lsi_index = similarities.MatrixSimilarity(lsi[global_corpus]) # transform corpus to LSI space and index it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Sanity check that the index models some distributional information\n",
    "TEST_WORD = 'company'\n",
    "test_idx = corpus_indices['word2idx'][TEST_WORD]\n",
    "binary_word_by_doc[test_idx]\n",
    "\n",
    "# lsi_index.index.shape\n",
    "test_vec = lsi_index.index[test_idx]\n",
    "\n",
    "# do a little transposition dance to stop numpy from making a copy of\n",
    "        # self.index internally in numpy.dot (very slow).\n",
    "result = np.dot(lsi_index.index, test_vec.T).T  # return #queries x #index\n",
    "most_similar = np.argsort(result)[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'company',\n",
       " u'firm',\n",
       " u'companies',\n",
       " u'union',\n",
       " u'five',\n",
       " u'business',\n",
       " u'trade',\n",
       " u'money',\n",
       " u'payroll',\n",
       " u'for',\n",
       " u'.',\n",
       " u'stock',\n",
       " u'expense',\n",
       " u'workers',\n",
       " u'_UNK_',\n",
       " u',',\n",
       " u'the',\n",
       " u'dollars',\n",
       " u'market',\n",
       " u'insurance']"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N = 20\n",
    "top_N = [corpus_indices['idx2word'][idx] for idx in most_similar[:N]]\n",
    "top_N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9769,)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.sha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15667\n"
     ]
    }
   ],
   "source": [
    "print(len(brown_documents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9769, 15667)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "brown_word_tf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['idx2tag', 'idx2word', 'word2idx', 'tag2idx']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_indices.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "131862\n",
      "43954\n",
      "43954\n"
     ]
    }
   ],
   "source": [
    "train_set = H5PYDataset(DATASET_PATH, which_sets=('train',))\n",
    "print(train_set.num_examples)\n",
    "\n",
    "test_set = H5PYDataset(DATASET_PATH, which_sets=('test',))\n",
    "print(test_set.num_examples)\n",
    "\n",
    "dev_set = H5PYDataset(DATASET_PATH, which_sets=('dev',))\n",
    "print(dev_set.num_examples)\n",
    "\n",
    "in_memory_train = H5PYDataset(\n",
    "    DATASET_PATH, which_sets=('train',),\n",
    "    sources=['instances', 'targets'], load_in_memory=True)\n",
    "\n",
    "train_X, train_y = in_memory_train.data_sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_X_list = [[i for i in row] for row in train_X]\n",
    "# train_X_list[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# sanity check a few words to ensure that their vectors are actually similar\n",
    "lsi_index.index.shape\n",
    "lsi_index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
