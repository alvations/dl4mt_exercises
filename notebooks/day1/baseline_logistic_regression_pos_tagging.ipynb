{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from __future__ import division, print_function\n",
    "import codecs\n",
    "import re\n",
    "import json\n",
    "import random\n",
    "import math\n",
    "import os\n",
    "import cPickle\n",
    "from collections import Counter, defaultdict\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pylab\n",
    "import pandas as pd\n",
    "from scipy.stats import norm\n",
    "import nltk\n",
    "from fuel.datasets import H5PYDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# now create the fuel dataset\n",
    "DATASET_LOCATION = 'datasets/'\n",
    "DATASET_NAME = 'brown_pos_dataset.hdf5'\n",
    "DATASET_PATH = os.path.join(DATASET_LOCATION, DATASET_NAME)\n",
    "\n",
    "with open(os.path.join(DATASET_LOCATION, 'brown_pos_dataset.indices')) as indices_file:\n",
    "    corpus_indices = cPickle.load(indices_file)\n",
    "\n",
    "# in order to use Logistic Regression for POS tagging, we need some features for our words\n",
    "# so let's get them from an SVD \n",
    "# -- another option is to use a pre-trained index so that the input is the same for every model\n",
    "\n",
    "# for the NN examples, we'll either train our embeddings from scratch, or pre-initialize with Glove or W2V\n",
    "# build a sparse matrix for all of our instances in the train set\n",
    "\n",
    "# if a word in test or dev isn't in train, map it to u'_UNK_'\n",
    "# the training data dictates the words we know and don't know\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# ok lets load the brown corpus, and use the indexes to convert it to ints,\n",
    "# then build tfidf, then transpose to get w X d\n",
    "\n",
    "# TODO: convert the word vector representation into WxW cooccurrence over window size \n",
    "#  i.e. (top 499 words + _UNK_) * window\n",
    "\n",
    "UNKNOWN_TOKEN = u'_UNK_'\n",
    "\n",
    "# some params to filter the documents\n",
    "MIN_DOC_LEN = 50\n",
    "MAX_DOC_LEN = 500\n",
    "\n",
    "def map_to_index(tok, index):\n",
    "    if tok in index:\n",
    "        return index[tok]\n",
    "    else:\n",
    "        return index[UNKNOWN_TOKEN]\n",
    "\n",
    "brown_documents = [[w for p in d for w in p] for d in nltk.corpus.brown.paras()]\n",
    "brown_documents = [[map_to_index(w, corpus_indices['word2idx']) for w in d] \n",
    "                   for d in brown_documents if len(d) > MIN_DOC_LEN\n",
    "                  ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "114.18439292543022"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean([len(d) for d in brown_documents])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words in vocabulary: 9769\n",
      "(9769, 8368)\n"
     ]
    }
   ],
   "source": [
    "brown_vocab_size = len(corpus_indices['word2idx'].keys())\n",
    "print(\"Number of words in vocabulary: {}\".format(brown_vocab_size))\n",
    "\n",
    "# for each doc, fill in the word counts of that row\n",
    "# then take the transpose to get VxD\n",
    "# allocate np.array of 0s with dims DxV\n",
    "brown_doc_tf = np.zeros((len(brown_documents), brown_vocab_size), dtype='uint16')\n",
    "for doc_id, doc in enumerate(brown_documents):\n",
    "    counts = Counter(doc)\n",
    "    words, counts = zip(*counts.items())\n",
    "    brown_doc_tf[doc_id, words] = counts\n",
    "\n",
    "# Transpose doc X word into word X doc\n",
    "brown_word_tf = brown_doc_tf.T\n",
    "print(brown_word_tf.shape)\n",
    "\n",
    "# BINARIZE COUNTS\n",
    "# convert to sparse binary -- (don't consider counts)\n",
    "brown_binary_word_by_doc = np.zeros((brown_word_tf.shape))\n",
    "brown_binary_word_by_doc[brown_word_tf.nonzero()] = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 887, 1633, 1678, 1700, 2677, 3516, 3517, 3818, 4087, 4291, 4443,\n",
       "        5355, 5842]),)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "brown_word_tf[0].nonzero()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "brown_binary_word_by_doc[brown_binary_word_by_doc.nonzero()].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "float"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(float(brown_word_tf[1,2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# np.transpose(brown_binary_word_by_doc.nonzero())\n",
    "brown_sparse_word_by_doc = [[(j,float(brown_word_tf[i,j])) for j in row.nonzero()[0]] \n",
    "                            for i,row in enumerate(brown_word_tf)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# let's persist our word tf matrix and see how big it is -- with shape = (9769, 15667) it's 292Mb\n",
    "WORD_TF=\"brown.word-by-doc.binary.npy\"\n",
    "with open(os.path.join(DATASET_LOCATION, WORD_TF), \"wb\") as outfile:\n",
    "    np.save(outfile, brown_binary_word_by_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Remember that we can compare 2D projections for visual separation of the data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# ok let's train some vector spaces to use as features for our words\n",
    "from gensim import corpora, models, similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:gensim.models.lsimodel:no word id mapping provided; initializing from corpus, assuming identity\n",
      "WARNING:gensim.similarities.docsim:scanning corpus to determine the number of features (consider setting `num_features` explicitly)\n"
     ]
    }
   ],
   "source": [
    "NUM_TOPICS=100\n",
    "\n",
    "global_corpus = brown_sparse_word_by_doc\n",
    "\n",
    "# create a tfidf transformation from our corpus of counts\n",
    "global_tfidf_transformation = models.TfidfModel(global_corpus)\n",
    "global_corpus_tfidf = global_tfidf_transformation[global_corpus]\n",
    "\n",
    "# lsi = models.LsiModel(global_corpus_tfidf, id2word=global_dictionary, num_topics=50) # initialize an LSI transformation\n",
    "lsi = models.LsiModel(global_corpus_tfidf, num_topics=NUM_TOPICS) # initialize an LSI transformation\n",
    "\n",
    "lsi_index = similarities.MatrixSimilarity(lsi[global_corpus]) # transform corpus to LSI space and index it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'woman',\n",
       " u'she',\n",
       " u'her',\n",
       " u\"wasn't\",\n",
       " u'She',\n",
       " u'knew',\n",
       " u'herself',\n",
       " u\"didn't\",\n",
       " u'going',\n",
       " u'said',\n",
       " u\"wouldn't\",\n",
       " u'go',\n",
       " u'tired',\n",
       " u'had',\n",
       " u'out',\n",
       " u'strange',\n",
       " u'voice',\n",
       " u'just',\n",
       " u'told',\n",
       " u'me']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sanity check that the index models some distributional information\n",
    "TEST_WORD = 'woman'\n",
    "test_idx = corpus_indices['word2idx'][TEST_WORD]\n",
    "\n",
    "test_vec = lsi_index.index[test_idx]\n",
    "\n",
    "# do a little transposition dance to stop numpy from making a copy of\n",
    "        # self.index internally in numpy.dot (very slow).\n",
    "result = np.dot(lsi_index.index, test_vec.T).T  # return #queries x #index\n",
    "most_similar = np.argsort(result)[::-1]\n",
    "\n",
    "N = 20\n",
    "top_N = [corpus_indices['idx2word'][idx] for idx in most_similar[:N]]\n",
    "top_N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "131862\n",
      "43954\n",
      "43954\n"
     ]
    }
   ],
   "source": [
    "train_set = H5PYDataset(DATASET_PATH, which_sets=('train',))\n",
    "print(train_set.num_examples)\n",
    "\n",
    "test_set = H5PYDataset(DATASET_PATH, which_sets=('test',))\n",
    "print(test_set.num_examples)\n",
    "\n",
    "dev_set = H5PYDataset(DATASET_PATH, which_sets=('dev',))\n",
    "print(dev_set.num_examples)\n",
    "\n",
    "train_X, train_y = H5PYDataset(\n",
    "    DATASET_PATH, which_sets=('train',),\n",
    "    sources=['instances', 'targets'], load_in_memory=True).data_sources\n",
    "\n",
    "dev_X, dev_y = H5PYDataset(\n",
    "    DATASET_PATH, which_sets=('dev',),\n",
    "    sources=['instances', 'targets'], load_in_memory=True).data_sources\n",
    "\n",
    "test_X, test_y = H5PYDataset(\n",
    "    DATASET_PATH, which_sets=('test',),\n",
    "    sources=['instances', 'targets'], load_in_memory=True).data_sources\n",
    "\n",
    "# train_X, train_y = in_memory_train.data_sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# ok let's train an Sklearn Logistic Regression model \n",
    "working_index = lsi_index.index\n",
    "\n",
    "# sanity hack\n",
    "    #\n",
    "\n",
    "def windows_to_array(window_corpus, index, cutoff=None):\n",
    "    if cutoff is None:\n",
    "        cutoff = len(window_corpus)\n",
    "    return np.array([np.hstack([index[idx] for idx in window]) for window in window_corpus[:cutoff]])\n",
    "\n",
    "# convert the windows to hstacked vectors\n",
    "training_vectors = windows_to_array(train_X, working_index)\n",
    "dev_vectors = windows_to_array(dev_X, working_index)\n",
    "test_vectors = windows_to_array(test_X, working_index)\n",
    "\n",
    "train_y = train_y.ravel()\n",
    "test_y = test_y.ravel()\n",
    "dev_y = dev_y.ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(131862,)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, penalty='l2', random_state=None, tol=0.0001)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "model = LogisticRegression()\n",
    "model.fit(training_vectors, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "43954\n",
      "31851\n"
     ]
    }
   ],
   "source": [
    "# evaluate performance\n",
    "num_correct = sum([1 for t in model.predict(test_vectors) == test_y if t == True])\n",
    "print(len(test_y))\n",
    "print(num_correct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "364\n",
      "11730\n"
     ]
    }
   ],
   "source": [
    "# i want to know how many token instances in the test set are ambiguous\n",
    "a_toks = defaultdict(set)\n",
    "for w, t in zip(test_X, test_y):\n",
    "    # two is the middle of the window\n",
    "    a_toks[w[2]].update([t])\n",
    "    \n",
    "am_toks = []\n",
    "for w, ts in a_toks.items():\n",
    "    if len(ts) > 1:\n",
    "        am_toks.append((corpus_indices['idx2word'][w], [corpus_indices['idx2tag'][t] for t in ts]))\n",
    "print(len(am_toks))\n",
    "ambiguous = set([corpus_indices['word2idx'][k] for k,v in am_toks])\n",
    "a_instances = sum([1 for w in test_X if w[2] in ambiguous])\n",
    "print(a_instances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "model = SVC(class_weight='auto')\n",
    "model.fit(training_vectors, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# evaluate performance\n",
    "num_correct = sum([1 for t in model.predict(test_vectors) == test_y if t == True])\n",
    "print(len(test_y))\n",
    "print(num_correct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "model = RandomForestClassifier(n_estimators=50, n_jobs=8)\n",
    "model.fit(training_vectors, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# evaluate performance\n",
    "num_correct = sum([1 for t in model.predict(test_vectors) == test_y if t == True])\n",
    "print(len(test_y))\n",
    "print(num_correct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
