{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# use the stacked autoencoder for unsupervised pre-training, then fine tune parameters with a multilayer perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "# Put the relative or absolute path to your copy of the repo here\n",
    "DL4MT_PATH='../..'\n",
    "\n",
    "sys.path.append('../../')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import the autoencoder, logistic regression, and MLP classes saved in previous notebooks.\n",
    "\n",
    "Evaluate the quality of our POS window vectors using the multilayer perceptron after a supervised fine tuning step. \n",
    "\n",
    "compare the performance of the classifier trained with the stacked autoencoder, vs single layer autoencoder\n",
    "\n",
    "vs PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    " This tutorial introduces stacked denoising auto-encoders (SdA) using Theano.\n",
    "\n",
    " Denoising autoencoders are the building blocks for SdA.\n",
    " They are based on auto-encoders as the ones used in Bengio et al. 2007.\n",
    " An autoencoder takes an input x and first maps it to a hidden representation\n",
    " y = f_{\\theta}(x) = s(Wx+b), parameterized by \\theta={W,b}. The resulting\n",
    " latent representation y is then mapped back to a \"reconstructed\" vector\n",
    " z \\in [0,1]^d in input space z = g_{\\theta'}(y) = s(W'y + b').  The weight\n",
    " matrix W' can optionally be constrained such that W' = W^T, in which case\n",
    " the autoencoder is said to have tied weights. The network is trained such\n",
    " that to minimize the reconstruction error (the error between x and z).\n",
    "\n",
    " For the denosing autoencoder, during training, first x is corrupted into\n",
    " \\tilde{x}, where \\tilde{x} is a partially destroyed version of x by means\n",
    " of a stochastic mapping. Afterwards y is computed as before (using\n",
    " \\tilde{x}), y = s(W\\tilde{x} + b) and z as s(W'y + b'). The reconstruction\n",
    " error is now measured between z and the uncorrupted input x, which is\n",
    " computed as the cross-entropy :\n",
    "      - \\sum_{k=1}^d[ x_k \\log z_k + (1-x_k) \\log( 1-z_k)]\n",
    "\n",
    "\n",
    " References :\n",
    "   - P. Vincent, H. Larochelle, Y. Bengio, P.A. Manzagol: Extracting and\n",
    "   Composing Robust Features with Denoising Autoencoders, ICML'08, 1096-1103,\n",
    "   2008\n",
    "   - Y. Bengio, P. Lamblin, D. Popovici, H. Larochelle: Greedy Layer-Wise\n",
    "   Training of Deep Networks, Advances in Neural Information Processing\n",
    "   Systems 19, 2007\n",
    "\n",
    "\"\"\"\n",
    "import os\n",
    "import sys\n",
    "import timeit\n",
    "\n",
    "import numpy\n",
    "\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "from theano.tensor.shared_randomstreams import RandomStreams\n",
    "\n",
    "# import the classes that we created in previous notebooks\n",
    "# from logistic_sgd import LogisticRegression, load_data\n",
    "\n",
    "from dl4mt.logistic_regression import LogisticRegression\n",
    "from dl4mt.mlp import HiddenLayer\n",
    "from dl4mt.autoencoder_layer import dA\n",
    "\n",
    "\n",
    "class SdA(object):\n",
    "    \"\"\"Stacked denoising auto-encoder class (SdA)\n",
    "\n",
    "    A stacked denoising autoencoder model is obtained by stacking several\n",
    "    dAs. The hidden layer of the dA at layer `i` becomes the input of\n",
    "    the dA at layer `i+1`. The first layer dA gets as input the input of\n",
    "    the SdA, and the hidden layer of the last dA represents the output.\n",
    "    Note that after pretraining, the SdA is dealt with as a normal MLP,\n",
    "    the dAs are only used to initialize the weights.\n",
    "    \"\"\"\n",
    "    \n",
    "#     TODO: fix n_outs\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        numpy_rng,\n",
    "        theano_rng=None,\n",
    "        n_ins=784,\n",
    "        hidden_layers_sizes=[500, 500],\n",
    "        n_outs=10,\n",
    "        corruption_levels=[0.1, 0.1]\n",
    "    ):\n",
    "        \"\"\" This class is made to support a variable number of layers.\n",
    "\n",
    "        :type numpy_rng: numpy.random.RandomState\n",
    "        :param numpy_rng: numpy random number generator used to draw initial\n",
    "                    weights\n",
    "\n",
    "        :type theano_rng: theano.tensor.shared_randomstreams.RandomStreams\n",
    "        :param theano_rng: Theano random generator; if None is given one is\n",
    "                           generated based on a seed drawn from `rng`\n",
    "\n",
    "        :type n_ins: int\n",
    "        :param n_ins: dimension of the input to the sdA\n",
    "\n",
    "        :type n_layers_sizes: list of ints\n",
    "        :param n_layers_sizes: intermediate layers size, must contain\n",
    "                               at least one value\n",
    "\n",
    "        :type n_outs: int\n",
    "        :param n_outs: dimension of the output of the network\n",
    "\n",
    "        :type corruption_levels: list of float\n",
    "        :param corruption_levels: amount of corruption to use for each\n",
    "                                  layer\n",
    "        \"\"\"\n",
    "\n",
    "        self.sigmoid_layers = []\n",
    "        self.dA_layers = []\n",
    "        self.params = []\n",
    "        self.n_layers = len(hidden_layers_sizes)\n",
    "\n",
    "        assert self.n_layers > 0\n",
    "\n",
    "        if not theano_rng:\n",
    "            theano_rng = RandomStreams(numpy_rng.randint(2 ** 30))\n",
    "        # allocate symbolic variables for the data\n",
    "        self.x = T.matrix('x')  # the data is presented as rasterized images\n",
    "        self.y = T.ivector('y')  # the labels are presented as 1D vector of\n",
    "                                 # [int] labels\n",
    "\n",
    "        # The SdA is an MLP, for which all weights of intermediate layers\n",
    "        # are shared with a different denoising autoencoders\n",
    "        # We will first construct the SdA as a deep multilayer perceptron,\n",
    "        # and when constructing each sigmoidal layer we also construct a\n",
    "        # denoising autoencoder that shares weights with that layer\n",
    "        # During pretraining we will train these autoencoders (which will\n",
    "        # lead to chainging the weights of the MLP as well)\n",
    "        # During finetunining we will finish training the SdA by doing\n",
    "        # stochastich gradient descent on the MLP\n",
    "\n",
    "        for i in xrange(self.n_layers):\n",
    "            # construct the sigmoidal layer\n",
    "\n",
    "            # the size of the input is either the number of hidden units of\n",
    "            # the layer below or the input size if we are on the first layer\n",
    "            if i == 0:\n",
    "                input_size = n_ins\n",
    "            else:\n",
    "                input_size = hidden_layers_sizes[i - 1]\n",
    "\n",
    "            # the input to this layer is either the activation of the hidden\n",
    "            # layer below or the input of the SdA if you are on the first\n",
    "            # layer\n",
    "            if i == 0:\n",
    "                layer_input = self.x\n",
    "            else:\n",
    "                layer_input = self.sigmoid_layers[-1].output\n",
    "\n",
    "            sigmoid_layer = HiddenLayer(rng=numpy_rng,\n",
    "                                        input=layer_input,\n",
    "                                        n_in=input_size,\n",
    "                                        n_out=hidden_layers_sizes[i],\n",
    "                                        activation=T.nnet.sigmoid)\n",
    "\n",
    "            # add the layer to our list of layers\n",
    "            self.sigmoid_layers.append(sigmoid_layer)\n",
    "            # its arguably a philosophical question...\n",
    "            # but we are going to only declare that the parameters of the\n",
    "            # sigmoid_layers are parameters of the StackedDAA\n",
    "            # the visible biases in the dA are parameters of those\n",
    "            # dA, but not the SdA\n",
    "            self.params.extend(sigmoid_layer.params)\n",
    "\n",
    "            # Construct a denoising autoencoder that shared weights with this\n",
    "            # layer\n",
    "            dA_layer = dA(numpy_rng=numpy_rng,\n",
    "                          theano_rng=theano_rng,\n",
    "                          input=layer_input,\n",
    "                          n_visible=input_size,\n",
    "                          n_hidden=hidden_layers_sizes[i],\n",
    "                          W=sigmoid_layer.W, # this is the part that connects the MLP and Autoencoder\n",
    "                          bhid=sigmoid_layer.b)\n",
    "            self.dA_layers.append(dA_layer)\n",
    "\n",
    "            \n",
    "        # We now need to add a logistic layer on top of the MLP\n",
    "        self.logLayer = LogisticRegression(\n",
    "            input=self.sigmoid_layers[-1].output,\n",
    "            n_in=hidden_layers_sizes[-1],\n",
    "            n_out=n_outs\n",
    "        )\n",
    "\n",
    "        self.params.extend(self.logLayer.params)\n",
    "        # construct a function that implements one step of finetunining\n",
    "\n",
    "        # compute the cost for second phase of training, defined as the negative log likelihood\n",
    "        # in other words, use the \"MLP view\"\n",
    "        self.finetune_cost = self.logLayer.negative_log_likelihood(self.y)\n",
    "        \n",
    "        # compute the gradients with respect to the model parameters\n",
    "        # symbolic variable that points to the number of errors made on the\n",
    "        # minibatch given by self.x and self.y\n",
    "        self.errors = self.logLayer.errors(self.y)\n",
    "        \n",
    "        # TODO: make this into proper \"predict\" function (semantics for autoencoder vs MLP?)\n",
    "        # TODO: chain the predict functions\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \n",
    "        for layer in self.dA_layers:\n",
    "            X = layer.predict(X)\n",
    "        return X\n",
    "\n",
    "    def pretraining_functions(self, train_set_x, batch_size):\n",
    "        ''' Generates a list of functions, each of them implementing one\n",
    "        step in trainnig the dA corresponding to the layer with same index.\n",
    "        The function will require as input the minibatch index, and to train\n",
    "        a dA you just need to iterate, calling the corresponding function on\n",
    "        all minibatch indexes.\n",
    "\n",
    "        :type train_set_x: theano.tensor.TensorType\n",
    "        :param train_set_x: Shared variable that contains all datapoints used\n",
    "                            for training the dA\n",
    "\n",
    "        :type batch_size: int\n",
    "        :param batch_size: size of a [mini]batch\n",
    "\n",
    "        :type learning_rate: float\n",
    "        :param learning_rate: learning rate used during training for any of\n",
    "                              the dA layers\n",
    "        '''\n",
    "\n",
    "        # index to a [mini]batch\n",
    "        index = T.lscalar('index')  # index to a minibatch\n",
    "        corruption_level = T.scalar('corruption')  # % of corruption to use\n",
    "        learning_rate = T.scalar('lr')  # learning rate to use\n",
    "        # begining of a batch, given `index`\n",
    "        batch_begin = index * batch_size\n",
    "        # ending of a batch given `index`\n",
    "        batch_end = batch_begin + batch_size\n",
    "\n",
    "        pretrain_fns = []\n",
    "        for dA in self.dA_layers:\n",
    "            # get the cost and the updates list\n",
    "            cost, updates = dA.get_cost_updates(corruption_level,\n",
    "                                                learning_rate)\n",
    "            # compile the theano training function\n",
    "            fn = theano.function(\n",
    "                inputs=[\n",
    "                    index,\n",
    "                    theano.Param(corruption_level, default=0.2),\n",
    "                    theano.Param(learning_rate, default=0.1)\n",
    "                ],\n",
    "                outputs=cost,\n",
    "                updates=updates,\n",
    "                givens={\n",
    "                    self.x: train_set_x[batch_begin: batch_end]\n",
    "                }\n",
    "            )\n",
    "            # append `fn` to the list of functions\n",
    "            pretrain_fns.append(fn)\n",
    "\n",
    "        return pretrain_fns\n",
    "\n",
    "    def build_finetune_functions(self, datasets, batch_size, learning_rate):\n",
    "        '''Generates a function `train` that implements one step of\n",
    "        finetuning, a function `validate` that computes the error on\n",
    "        a batch from the validation set, and a function `test` that\n",
    "        computes the error on a batch from the testing set\n",
    "\n",
    "        :type datasets: list of pairs of theano.tensor.TensorType\n",
    "        :param datasets: It is a list that contain all the datasets;\n",
    "                         the has to contain three pairs, `train`,\n",
    "                         `valid`, `test` in this order, where each pair\n",
    "                         is formed of two Theano variables, one for the\n",
    "                         datapoints, the other for the labels\n",
    "\n",
    "        :type batch_size: int\n",
    "        :param batch_size: size of a minibatch\n",
    "\n",
    "        :type learning_rate: float\n",
    "        :param learning_rate: learning rate used during finetune stage\n",
    "        '''\n",
    "\n",
    "#         TODO: use our dataset loading funcs\n",
    "        (train_set_x, train_set_y) = datasets[0]\n",
    "        (valid_set_x, valid_set_y) = datasets[1]\n",
    "        (test_set_x, test_set_y) = datasets[2]\n",
    "\n",
    "        # compute number of minibatches for training, validation and testing\n",
    "        n_valid_batches = valid_set_x.get_value(borrow=True).shape[0]\n",
    "        n_valid_batches /= batch_size\n",
    "        n_test_batches = test_set_x.get_value(borrow=True).shape[0]\n",
    "        n_test_batches /= batch_size\n",
    "\n",
    "        index = T.lscalar('index')  # index to a [mini]batch\n",
    "\n",
    "        # compute the gradients with respect to the model parameters\n",
    "        gparams = T.grad(self.finetune_cost, self.params)\n",
    "\n",
    "        # compute list of fine-tuning updates\n",
    "        updates = [\n",
    "            (param, param - gparam * learning_rate)\n",
    "            for param, gparam in zip(self.params, gparams)\n",
    "        ]\n",
    "\n",
    "        train_fn = theano.function(\n",
    "            inputs=[index],\n",
    "            outputs=self.finetune_cost,\n",
    "            updates=updates,\n",
    "            givens={\n",
    "                self.x: train_set_x[\n",
    "                    index * batch_size: (index + 1) * batch_size\n",
    "                ],\n",
    "                self.y: train_set_y[\n",
    "                    index * batch_size: (index + 1) * batch_size\n",
    "                ]\n",
    "            },\n",
    "            name='train'\n",
    "        )\n",
    "\n",
    "        test_score_i = theano.function(\n",
    "            [index],\n",
    "            self.errors,\n",
    "            givens={\n",
    "                self.x: test_set_x[\n",
    "                    index * batch_size: (index + 1) * batch_size\n",
    "                ],\n",
    "                self.y: test_set_y[\n",
    "                    index * batch_size: (index + 1) * batch_size\n",
    "                ]\n",
    "            },\n",
    "            name='test'\n",
    "        )\n",
    "\n",
    "        valid_score_i = theano.function(\n",
    "            [index],\n",
    "            self.errors,\n",
    "            givens={\n",
    "                self.x: valid_set_x[\n",
    "                    index * batch_size: (index + 1) * batch_size\n",
    "                ],\n",
    "                self.y: valid_set_y[\n",
    "                    index * batch_size: (index + 1) * batch_size\n",
    "                ]\n",
    "            },\n",
    "            name='valid'\n",
    "        )\n",
    "\n",
    "        # Create a function that scans the entire validation set\n",
    "        def valid_score():\n",
    "            return [valid_score_i(i) for i in xrange(n_valid_batches)]\n",
    "\n",
    "        # Create a function that scans the entire test set\n",
    "        def test_score():\n",
    "            return [test_score_i(i) for i in xrange(n_test_batches)]\n",
    "\n",
    "        return train_fn, valid_score, test_score\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# working -- split into train and test funcs\n",
    "# TODO: separate out pretraining and finetuning so that we can evaluate our vectors at each time step\n",
    "\n",
    "def test_SdA(finetune_lr=0.1, pretraining_epochs=15,\n",
    "             pretrain_lr=0.001, training_epochs=1000,\n",
    "             dataset='mnist.pkl.gz', batch_size=1):\n",
    "    \"\"\"\n",
    "    Demonstrates how to train and test a stochastic denoising autoencoder.\n",
    "\n",
    "    :type learning_rate: float\n",
    "    :param learning_rate: learning rate used in the finetune stage\n",
    "    (factor for the stochastic gradient)\n",
    "\n",
    "    :type pretraining_epochs: int\n",
    "    :param pretraining_epochs: number of epoch to do pretraining\n",
    "\n",
    "    :type pretrain_lr: float\n",
    "    :param pretrain_lr: learning rate to be used during pre-training\n",
    "\n",
    "    :type n_iter: int\n",
    "    :param n_iter: maximal number of iterations ot run the optimizer\n",
    "\n",
    "    :type dataset: string\n",
    "    :param dataset: path the the pickled dataset\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    \n",
    "    #         TODO: use our dataset loading\n",
    "#     datasets = load_data(dataset)\n",
    "\n",
    "    train_set_x, train_set_y = datasets[0]\n",
    "    valid_set_x, valid_set_y = datasets[1]\n",
    "    test_set_x, test_set_y = datasets[2]\n",
    "\n",
    "    # compute number of minibatches for training, validation and testing\n",
    "    n_train_batches = train_set_x.get_value(borrow=True).shape[0]\n",
    "    n_train_batches /= batch_size\n",
    "\n",
    "    \n",
    "#     TODO: fix n-ins and n-outs\n",
    "    n_ins = train_set_x.get_value().shape[1]\n",
    "    n_outs = 12\n",
    "    \n",
    "    # numpy random generator\n",
    "    numpy_rng = numpy.random.RandomState(89677)\n",
    "    print '... building the model'\n",
    "    # construct the stacked denoising autoencoder class\n",
    "    # TODO: plot the hidden layer before and after training\n",
    "    sda = SdA(\n",
    "        numpy_rng=numpy_rng,\n",
    "        n_ins=n_ins,\n",
    "#         hidden_layers_sizes=[1000, 1000, 1000],\n",
    "        hidden_layers_sizes=[1000, 300, 100, 2],\n",
    "        n_outs=n_outs\n",
    "    )\n",
    "\n",
    "    #########################\n",
    "    # PRETRAINING THE MODEL #\n",
    "    #########################\n",
    "    print '... getting the pretraining functions'\n",
    "    pretraining_fns = sda.pretraining_functions(train_set_x=train_set_x,\n",
    "                                                batch_size=batch_size)\n",
    "\n",
    "    print '... pre-training the model'\n",
    "    start_time = timeit.default_timer()\n",
    "    ## Pre-train layer-wise\n",
    "    \n",
    "    # TODO: move this into function parameterization\n",
    "    corruption_levels = [.1, .2, .3, .3]\n",
    "    for i in xrange(sda.n_layers):\n",
    "        # go through pretraining epochs\n",
    "        for epoch in xrange(pretraining_epochs):\n",
    "            # go through the training set\n",
    "            c = []\n",
    "            for batch_index in xrange(n_train_batches):\n",
    "                c.append(pretraining_fns[i](index=batch_index,\n",
    "                         corruption=corruption_levels[i],\n",
    "                         lr=pretrain_lr))\n",
    "            print 'Pre-training layer %i, epoch %d, cost ' % (i, epoch),\n",
    "            print numpy.mean(c)\n",
    "\n",
    "    end_time = timeit.default_timer()\n",
    "\n",
    "#     print >> sys.stderr, ('The pretraining code for file ' +\n",
    "#                           os.path.split(__file__)[1] +\n",
    "#                           ' ran for %.2fm' % ((end_time - start_time) / 60.))\n",
    "    \n",
    "    ########################\n",
    "    # FINETUNING THE MODEL #\n",
    "    ########################\n",
    "\n",
    "    # get the training, validation and testing function for the model\n",
    "    print '... getting the finetuning functions'\n",
    "    train_fn, validate_model, test_model = sda.build_finetune_functions(\n",
    "        datasets=datasets,\n",
    "        batch_size=batch_size,\n",
    "        learning_rate=finetune_lr\n",
    "    )\n",
    "\n",
    "    print '... finetunning the model'\n",
    "    # early-stopping parameters\n",
    "    patience = 10 * n_train_batches  # look as this many examples regardless\n",
    "    patience_increase = 2.  # wait this much longer when a new best is\n",
    "                            # found\n",
    "    improvement_threshold = 0.995  # a relative improvement of this much is\n",
    "                                   # considered significant\n",
    "    validation_frequency = min(n_train_batches, patience / 2)\n",
    "                                  # go through this many\n",
    "                                  # minibatche before checking the network\n",
    "                                  # on the validation set; in this case we\n",
    "                                  # check every epoch\n",
    "\n",
    "    best_validation_loss = numpy.inf\n",
    "    test_score = 0.\n",
    "    start_time = timeit.default_timer()\n",
    "\n",
    "    done_looping = False\n",
    "    epoch = 0\n",
    "\n",
    "    while (epoch < training_epochs) and (not done_looping):\n",
    "        epoch = epoch + 1\n",
    "        for minibatch_index in xrange(n_train_batches):\n",
    "            minibatch_avg_cost = train_fn(minibatch_index)\n",
    "            iter = (epoch - 1) * n_train_batches + minibatch_index\n",
    "\n",
    "            if (iter + 1) % validation_frequency == 0:\n",
    "                validation_losses = validate_model()\n",
    "                this_validation_loss = numpy.mean(validation_losses)\n",
    "                print('epoch %i, minibatch %i/%i, validation error %f %%' %\n",
    "                      (epoch, minibatch_index + 1, n_train_batches,\n",
    "                       this_validation_loss * 100.))\n",
    "\n",
    "                # if we got the best validation score until now\n",
    "                if this_validation_loss < best_validation_loss:\n",
    "\n",
    "                    #improve patience if loss improvement is good enough\n",
    "                    if (\n",
    "                        this_validation_loss < best_validation_loss *\n",
    "                        improvement_threshold\n",
    "                    ):\n",
    "                        patience = max(patience, iter * patience_increase)\n",
    "\n",
    "                    # save best validation score and iteration number\n",
    "                    best_validation_loss = this_validation_loss\n",
    "                    best_iter = iter\n",
    "\n",
    "                    # test it on the test set\n",
    "                    test_losses = test_model()\n",
    "                    test_score = numpy.mean(test_losses)\n",
    "                    print(('     epoch %i, minibatch %i/%i, test error of '\n",
    "                           'best model %f %%') %\n",
    "                          (epoch, minibatch_index + 1, n_train_batches,\n",
    "                           test_score * 100.))\n",
    "\n",
    "            if patience <= iter:\n",
    "                done_looping = True\n",
    "                break\n",
    "\n",
    "    end_time = timeit.default_timer()\n",
    "    print(\n",
    "        (\n",
    "            'Optimization complete with best validation score of %f %%, '\n",
    "            'on iteration %i, '\n",
    "            'with test performance %f %%'\n",
    "        )\n",
    "        % (best_validation_loss * 100., best_iter + 1, test_score * 100.)\n",
    "    )\n",
    "\n",
    "    return sda\n",
    "#     print >> sys.stderr, ('The training code for file ' +\n",
    "#                           os.path.split(__file__)[1] +\n",
    "#                           ' ran for %.2fm' % ((end_time - start_time) / 60.))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... building the model\n",
      "... getting the pretraining functions\n",
      "... pre-training the model\n",
      "Pre-training layer 0, epoch 0, cost  156.707\n",
      "Pre-training layer 0, epoch 1, cost  112.171\n",
      "Pre-training layer 1, epoch 0, cost  622.997\n",
      "Pre-training layer 1, epoch 1, cost  568.85\n",
      "Pre-training layer 2, epoch 0, cost  156.961\n",
      "Pre-training layer 2, epoch 1, cost  127.46\n",
      "Pre-training layer 3, epoch 0, cost  68.0368\n",
      "Pre-training layer 3, epoch 1, cost  63.1253\n",
      "... getting the finetuning functions\n",
      "... finetunning the model\n",
      "epoch 1, minibatch 500/500, validation error 73.910000 %\n",
      "     epoch 1, minibatch 500/500, test error of best model 67.350000 %\n",
      "epoch 2, minibatch 500/500, validation error 67.220000 %\n",
      "     epoch 2, minibatch 500/500, test error of best model 59.840000 %\n",
      "Optimization complete with best validation score of 67.220000 %, on iteration 1000, with test performance 59.840000 %\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "from dl4mt.datasets import prep_dataset\n",
    "from dl4mt.training import train_model\n",
    "\n",
    "# location of our datasets\n",
    "DATASET_LOCATION = '../../datasets/'\n",
    "\n",
    "# the pos dataset consists of windows around words\n",
    "POS_DATASET_NAME = 'brown_pos_dataset.hdf5'\n",
    "POS_DATASET_PATH = os.path.join(DATASET_LOCATION, POS_DATASET_NAME)\n",
    "WORD_BY_WORD_MATRIX = 'brown.word-by-word.normalized.npy'\n",
    "VECTOR_INDEX_PATH = os.path.join(DATASET_LOCATION, WORD_BY_WORD_MATRIX)\n",
    "\n",
    "# the cutoff for the maximum number of training and dev examples\n",
    "CUTOFF = 10000\n",
    "  \n",
    "train_dataset = prep_dataset(POS_DATASET_PATH, VECTOR_INDEX_PATH, which_sets=['train'], cutoff=CUTOFF)\n",
    "dev_dataset = prep_dataset(POS_DATASET_PATH, VECTOR_INDEX_PATH, which_sets=['dev'], cutoff=CUTOFF)\n",
    "test_dataset = prep_dataset(POS_DATASET_PATH, VECTOR_INDEX_PATH, which_sets=['test'], cutoff=CUTOFF)\n",
    "\n",
    "# TODO: remove hack\n",
    "datasets = (train_dataset, dev_dataset, test_dataset)\n",
    "\n",
    "# get the functions and params that we need for our models\n",
    "# initialization_data = initialize_logistic_regression(train_dataset, dev_dataset,\n",
    "#                                                      learning_rate=0.1, batch_size=50)\n",
    "\n",
    "# classifier, train_model_func, validate_model_func, n_train_batches, n_valid_batches = initialization_data\n",
    "    \n",
    "# load the training function and train the LR model \n",
    "# TODO: -- train_model should return the validation error as a list, then we can plot it\n",
    "# in general, train_model should return any useful information about the training process\n",
    "# train_model(train_model_func,  n_train_batches, validate_model=validate_model_func,\n",
    "#             n_valid_batches=n_valid_batches, training_epochs=100)\n",
    "\n",
    "\n",
    "# def test_SdA(finetune_lr=0.1, pretraining_epochs=15,\n",
    "#              pretrain_lr=0.001, training_epochs=1000,\n",
    "#              dataset='mnist.pkl.gz', batch_size=1):\n",
    "    \n",
    "classifier = test_SdA(dataset=datasets, pretraining_epochs=15, training_epochs=10, batch_size=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TODO: make the dev and test data loadable in the same way as the training data below\n",
    "\n",
    "# make a theano function to get predictions from a trained model\n",
    "training_data = theano.tensor.matrix('training_X')\n",
    "# predictions = classifier.get_encoding(training_data)\n",
    "predictions = classifier.predict(training_data)\n",
    "\n",
    "get_predictions = theano.function([training_data], predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# get predictions and evaluate\n",
    "p = get_predictions(train_dataset[0].get_value())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib qt\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "\n",
    "# get train_y without the cast\n",
    "train_y = prep_dataset(POS_DATASET_PATH, VECTOR_INDEX_PATH, which_sets=['train'],\n",
    "                       cast_y=False, cutoff=CUTOFF)[1].get_value().astype('int32')\n",
    "\n",
    "# TODO: Set this at the top of the notebook -- this is the visualization cutoff, not the training cutoff\n",
    "CUTOFF_BEGIN=0\n",
    "CUTOFF_END=1000\n",
    "\n",
    "# TODO: this method of visualizing the labels loses the class tags -- we want to know which color is which\n",
    "y_vals = train_y[CUTOFF_BEGIN:CUTOFF_END]\n",
    "norm_y_vals = y_vals / float(np.amax(y_vals))\n",
    "\n",
    "jitter1 = np.random.normal(loc=0.0, scale=0.05, size=CUTOFF_END-CUTOFF_BEGIN)\n",
    "jitter2 = np.random.normal(loc=0.0, scale=0.05, size=CUTOFF_END-CUTOFF_BEGIN)\n",
    "x1 = p[CUTOFF_BEGIN:CUTOFF_END,0] + jitter1\n",
    "x2 = p[CUTOFF_BEGIN:CUTOFF_END,1] + jitter2\n",
    "\n",
    "\n",
    "plt.scatter(x1, x2, c=norm_y_vals, s=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
