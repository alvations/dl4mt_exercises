{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The purpose of this tutorial is to review logistic regression, autoencoders, and the Multilayer Perceptron (MLP)\n",
    "\n",
    "- create a one-layer autoencoder to encode word windows into 100-dim vectors\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import codecs\n",
    "import re\n",
    "import json\n",
    "import random\n",
    "import math\n",
    "import os\n",
    "import gzip\n",
    "import cPickle\n",
    "import sys\n",
    "from collections import Counter, defaultdict\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pylab\n",
    "import pandas as pd\n",
    "from scipy.stats import norm\n",
    "import nltk\n",
    "\n",
    "pylab.rcParams['figure.figsize'] = (10.0, 8.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from fuel.datasets import H5PYDataset\n",
    "\n",
    "DATASET_LOCATION = 'datasets/'\n",
    "\n",
    "# the pos dataset consists of windows around words\n",
    "POS_DATASET_NAME = 'brown_pos_dataset.hdf5'\n",
    "POS_DATASET_PATH = os.path.join(DATASET_LOCATION, POS_DATASET_NAME)\n",
    "\n",
    "CORPUS_INDICES = 'brown_pos_dataset.indices'\n",
    "WORD_BY_DOC_MATRIX = 'brown.word-by-doc.binary.npy'\n",
    "WORD_BY_WORD_MATRIX = 'brown.word-by-word.normalized.npy'\n",
    "\n",
    "# Indexes for mapping words <--> ints\n",
    "with open(os.path.join(DATASET_LOCATION, CORPUS_INDICES)) as indices_file:\n",
    "    corpus_indices = cPickle.load(indices_file)\n",
    "    \n",
    "# Word X Doc binary matrix\n",
    "word_df = np.load(os.path.join(DATASET_LOCATION, WORD_BY_DOC_MATRIX))\n",
    "\n",
    "train_X, train_y = H5PYDataset(\n",
    "    POS_DATASET_PATH, which_sets=('train',),\n",
    "    sources=['instances', 'targets'], load_in_memory=True).data_sources\n",
    "\n",
    "dev_X, dev_y = H5PYDataset(\n",
    "    POS_DATASET_PATH, which_sets=('dev',),\n",
    "    sources=['instances', 'targets'], load_in_memory=True).data_sources\n",
    "\n",
    "test_X, test_y = H5PYDataset(\n",
    "    POS_DATASET_PATH, which_sets=('test',),\n",
    "    sources=['instances', 'targets'], load_in_memory=True).data_sources\n",
    "\n",
    "# make sure the dl4mt module is on the path\n",
    "sys.path.append('../..')\n",
    "sys.path.append('/home/chris/projects/dl4mt_labs/dcu_deep_learning/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_df[word_df.nonzero()].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ../../dl4mt/datasets.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ../../dl4mt/datasets.py\n",
    "# Uncomment to save this cell to file in order to import it in later notebooks\n",
    "\n",
    "# TODO: this cell should be written to file in this notebook or an earlier one\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "import numpy as np\n",
    "from fuel.datasets import H5PYDataset\n",
    "\n",
    "def create_hstacked_vectors(index_path, seqs):\n",
    "    # load the index, map the seqs (word indices) through the index, hstack, then cast the results to np.array\n",
    "    index = np.load(index_path)\n",
    "    \n",
    "    working_index = np.load(index_path)\n",
    "    return np.array([np.hstack([index[idx] for idx in seq]) for seq in seqs])\n",
    "\n",
    "# load a portion of a fuel dataset\n",
    "def load_fuel_dataset(filename, which_sets=None):\n",
    "#     TODO: if which_sets is not None return all sets\n",
    "\n",
    "    X, y = H5PYDataset(\n",
    "        filename, which_sets=which_sets,\n",
    "        sources=['instances', 'targets'], load_in_memory=True).data_sources\n",
    "    \n",
    "    # make sure y is 0-dimensional\n",
    "    # TODO: put this hack in another function\n",
    "    y = y.ravel()\n",
    "    \n",
    "    # TODO: return X and Y here\n",
    "    return (X,y)\n",
    "    \n",
    "    \n",
    "def shared_dataset(X, y, borrow=True):\n",
    "    \"\"\" Function that loads the dataset into shared variables\n",
    "\n",
    "    The reason we store our dataset in shared variables is to allow\n",
    "    Theano to copy it into the GPU memory (when code is run on GPU).\n",
    "    Since copying data into the GPU is slow, copying a minibatch everytime\n",
    "    is needed (the default behaviour if the data is not in a shared\n",
    "    variable) would lead to a large decrease in performance.\n",
    "    \"\"\"\n",
    "\n",
    "    shared_x = theano.shared(np.asarray(X,\n",
    "                                           dtype=theano.config.floatX), borrow=borrow)\n",
    "    shared_y = theano.shared(np.asarray(y,\n",
    "                                           dtype=theano.config.floatX), borrow=borrow)\n",
    "    # When storing data on the GPU it has to be stored as floats\n",
    "    # therefore we will store the labels as ``floatX`` as well\n",
    "    # (``shared_y`` does exactly that). But during our computations\n",
    "    # we need them as ints (we use labels as index, and if they are\n",
    "    # floats it doesn't make sense) therefore instead of returning\n",
    "    # ``shared_y`` we will have to cast it to int. This little hack\n",
    "    # lets us get around this issue\n",
    "    return shared_x, T.cast(shared_y, 'int32')\n",
    "\n",
    "def prep_dataset(path, index_path, which_sets=[], cutoff=None):\n",
    "    X, y = load_fuel_dataset(path, which_sets=which_sets)\n",
    "    if cutoff is not None:\n",
    "        X = X[:cutoff]\n",
    "        y = y[:cutoff]\n",
    "\n",
    "    # extract windows\n",
    "    X = create_hstacked_vectors(index_path, X)\n",
    "\n",
    "    # make shared dataset\n",
    "    return shared_dataset(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# %%writefile ../../dl4mt/autoencoder_layer.py\n",
    "# Uncomment to save this cell to file in order to import it in later notebooks\n",
    "\n",
    "\"\"\"\n",
    " Most of this class was taken from the lisa-lab deeplearning tutorials \n",
    " -- https://github.com/lisa-lab/DeepLearningTutorials\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import timeit\n",
    "\n",
    "import numpy\n",
    "\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "from theano.tensor.shared_randomstreams import RandomStreams\n",
    "\n",
    "class dA(object):\n",
    "    \"\"\"Denoising Auto-Encoder class (dA)\n",
    "\n",
    "    A denoising autoencoders tries to reconstruct the input from a corrupted\n",
    "    version of it by projecting it first in a latent space and reprojecting\n",
    "    it afterwards back in the input space. Please refer to Vincent et al.,2008\n",
    "    for more details. If x is the input then equation (1) computes a partially\n",
    "    destroyed version of x by means of a stochastic mapping q_D. Equation (2)\n",
    "    computes the projection of the input into the latent space. Equation (3)\n",
    "    computes the reconstruction of the input, while equation (4) computes the\n",
    "    reconstruction error.\n",
    "\n",
    "    .. math::\n",
    "\n",
    "        \\tilde{x} ~ q_D(\\tilde{x}|x)                                     (1)\n",
    "\n",
    "        y = s(W \\tilde{x} + b)                                           (2)\n",
    "\n",
    "        x = s(W' y  + b')                                                (3)\n",
    "\n",
    "        L(x,z) = -sum_{k=1}^d [x_k \\log z_k + (1-x_k) \\log( 1-z_k)]      (4)\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        numpy_rng,\n",
    "        theano_rng=None,\n",
    "        input=None,\n",
    "        n_visible=784,\n",
    "        n_hidden=500,\n",
    "        W=None,\n",
    "        W_prime=None,\n",
    "        bhid=None,\n",
    "        bvis=None\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initialize the dA class by specifying the number of visible units (the\n",
    "        dimension d of the input ), the number of hidden units ( the dimension\n",
    "        d' of the latent or hidden space ) and the corruption level. The\n",
    "        constructor also receives symbolic variables for the input, weights and\n",
    "        bias. Such a symbolic variables are useful when, for example the input\n",
    "        is the result of some computations, or when weights are shared between\n",
    "        the dA and an MLP layer. When dealing with SdAs this always happens,\n",
    "        the dA on layer 2 gets as input the output of the dA on layer 1,\n",
    "        and the weights of the dA are used in the second stage of training\n",
    "        to construct an MLP.\n",
    "\n",
    "        :type numpy_rng: numpy.random.RandomState\n",
    "        :param numpy_rng: number random generator used to generate weights\n",
    "\n",
    "        :type theano_rng: theano.tensor.shared_randomstreams.RandomStreams\n",
    "        :param theano_rng: Theano random generator; if None is given one is\n",
    "                     generated based on a seed drawn from `rng`\n",
    "\n",
    "        :type input: theano.tensor.TensorType\n",
    "        :param input: a symbolic description of the input or None for\n",
    "                      standalone dA\n",
    "\n",
    "        :type n_visible: int\n",
    "        :param n_visible: number of visible units\n",
    "\n",
    "        :type n_hidden: int\n",
    "        :param n_hidden:  number of hidden units\n",
    "\n",
    "        :type W: theano.tensor.TensorType\n",
    "        :param W: Theano variable pointing to a set of weights that should be\n",
    "                  shared belong the dA and another architecture; if dA should\n",
    "                  be standalone set this to None\n",
    "                  \n",
    "        :type W_prime: theano.tensor.TensorType\n",
    "        :param W_prime: Theano variable pointing to a set of weights that should be\n",
    "                  shared belong the dA and another architecture; if dA should\n",
    "                  be standalone set this to None\n",
    "\n",
    "        :type bhid: theano.tensor.TensorType\n",
    "        :param bhid: Theano variable pointing to a set of biases values (for\n",
    "                     hidden units) that should be shared belong dA and another\n",
    "                     architecture; if dA should be standalone set this to None\n",
    "\n",
    "        :type bvis: theano.tensor.TensorType\n",
    "        :param bvis: Theano variable pointing to a set of biases values (for\n",
    "                     visible units) that should be shared belong dA and another\n",
    "                     architecture; if dA should be standalone set this to None\n",
    "\n",
    "\n",
    "        \"\"\"\n",
    "        self.n_visible = n_visible\n",
    "        self.n_hidden = n_hidden\n",
    "\n",
    "        # create a Theano random generator that gives symbolic random values\n",
    "        if not theano_rng:\n",
    "            theano_rng = RandomStreams(numpy_rng.randint(2 ** 30))\n",
    "\n",
    "        # note : W' was written as `W_prime` and b' as `b_prime`\n",
    "        if not W:\n",
    "            # W is initialized with `initial_W` which is uniformly sampled\n",
    "            # from -4*sqrt(6./(n_visible+n_hidden)) and\n",
    "            # 4*sqrt(6./(n_hidden+n_visible))the output of uniform if\n",
    "            # converted using asarray to dtype\n",
    "            # theano.config.floatX so that the code is runable on GPU\n",
    "            initial_W = numpy.asarray(\n",
    "                numpy_rng.uniform(\n",
    "                    low=-4 * numpy.sqrt(6. / (n_hidden + n_visible)),\n",
    "                    high=4 * numpy.sqrt(6. / (n_hidden + n_visible)),\n",
    "                    size=(n_visible, n_hidden)\n",
    "                ),\n",
    "                dtype=theano.config.floatX\n",
    "            )\n",
    "            W = theano.shared(value=initial_W, name='W', borrow=True)\n",
    "\n",
    "        if not W_prime:\n",
    "            # W is initialized with `initial_W` which is uniformly sampled\n",
    "            # from -4*sqrt(6./(n_visible+n_hidden)) and\n",
    "            # 4*sqrt(6./(n_hidden+n_visible))the output of uniform if\n",
    "            # converted using asarray to dtype\n",
    "            # theano.config.floatX so that the code is runable on GPU\n",
    "            initial_W_prime = numpy.asarray(\n",
    "                numpy_rng.uniform(\n",
    "                    low=-4 * numpy.sqrt(6. / (n_hidden + n_visible)),\n",
    "                    high=4 * numpy.sqrt(6. / (n_hidden + n_visible)),\n",
    "                    size=(n_hidden, n_visible)\n",
    "                ),\n",
    "                dtype=theano.config.floatX\n",
    "            )\n",
    "            W_prime = theano.shared(value=initial_W_prime, name='W', borrow=True)\n",
    "\n",
    "            \n",
    "        if not bvis:\n",
    "            bvis = theano.shared(\n",
    "                value=numpy.zeros(\n",
    "                    n_visible,\n",
    "                    dtype=theano.config.floatX\n",
    "                ),\n",
    "                borrow=True\n",
    "            )\n",
    "\n",
    "        if not bhid:\n",
    "            bhid = theano.shared(\n",
    "                value=numpy.zeros(\n",
    "                    n_hidden,\n",
    "                    dtype=theano.config.floatX\n",
    "                ),\n",
    "                name='b',\n",
    "                borrow=True\n",
    "            )\n",
    "\n",
    "        self.W = W\n",
    "        # b corresponds to the bias of the hidden\n",
    "        self.b = bhid\n",
    "        # b_prime corresponds to the bias of the visible\n",
    "        self.b_prime = bvis\n",
    "        \n",
    "        # TODO -- try untying the weights\n",
    "        # Autoencoder with tied weights, therefore W_prime is W transpose\n",
    "        self.W_prime = self.W.T\n",
    "        self.W_prime = W_prime\n",
    "        \n",
    "        self.theano_rng = theano_rng\n",
    "        # if no input is given, generate a variable representing the input\n",
    "        if input is None:\n",
    "            # we use a matrix because we expect a minibatch of several\n",
    "            # examples, each example being a row\n",
    "            self.x = T.dmatrix(name='input')\n",
    "        else:\n",
    "            self.x = input\n",
    "\n",
    "        self.params = [self.W, self.b, self.b_prime]\n",
    "\n",
    "    # TODO: user should implement this function -- to start, just return identity\n",
    "    def get_corrupted_input(self, input, corruption_level):\n",
    "        \"\"\"This function keeps ``1-corruption_level`` entries of the inputs the\n",
    "        same and zero-out randomly selected subset of size ``coruption_level``\n",
    "        Note : first argument of theano.rng.binomial is the shape(size) of\n",
    "               random numbers that it should produce\n",
    "               second argument is the number of trials\n",
    "               third argument is the probability of success of any trial\n",
    "\n",
    "                this will produce an array of 0s and 1s where 1 has a\n",
    "                probability of 1 - ``corruption_level`` and 0 with\n",
    "                ``corruption_level``\n",
    "\n",
    "                The binomial function return int64 data type by\n",
    "                default.  int64 multiplicated by the input\n",
    "                type(floatX) always return float64.  To keep all data\n",
    "                in floatX when floatX is float32, we set the dtype of\n",
    "                the binomial to floatX. As in our case the value of\n",
    "                the binomial is always 0 or 1, this don't change the\n",
    "                result. This is needed to allow the gpu to work\n",
    "                correctly as it only support float32 for now.\n",
    "\n",
    "        \"\"\"\n",
    "        return self.theano_rng.binomial(size=input.shape, n=1,\n",
    "                                        p=1 - corruption_level,\n",
    "                                        dtype=theano.config.floatX) * input\n",
    " \n",
    "    def get_hidden_values(self, input):\n",
    "        \"\"\" Computes the values of the hidden layer \"\"\"\n",
    "        return T.nnet.sigmoid(T.dot(input, self.W) + self.b)\n",
    "    \n",
    "    def predict(self, input):\n",
    "        \"\"\" Alias to get_hidden_values for consistent API \"\"\"\n",
    "        return self.get_hidden_values(input)\n",
    "\n",
    "    def get_reconstructed_input(self, hidden):\n",
    "        \"\"\"Computes the reconstructed input given the values of the\n",
    "        hidden layer\n",
    "\n",
    "        \"\"\"\n",
    "        return T.nnet.sigmoid(T.dot(hidden, self.W_prime) + self.b_prime)\n",
    "\n",
    "    def get_cost_updates(self, corruption_level, learning_rate):\n",
    "        \"\"\" This function computes the cost and the updates for one trainng\n",
    "        step of the dA \"\"\"\n",
    "\n",
    "        tilde_x = self.get_corrupted_input(self.x, corruption_level)\n",
    "        y = self.get_hidden_values(tilde_x)\n",
    "        z = self.get_reconstructed_input(y)\n",
    "        \n",
    "        # sum over the columns of each training instance\n",
    "        L = - T.sum(self.x * T.log(z) + (1 - self.x) * T.log(1 - z), axis=1)\n",
    "        \n",
    "        # note : L is now a vector, where each element is the\n",
    "        #        cross-entropy cost of the reconstruction of the\n",
    "        #        corresponding example of the minibatch. We need to\n",
    "        #        compute the average of all these to get the cost of\n",
    "        #        the minibatch\n",
    "        cost = T.mean(L)\n",
    "\n",
    "        # compute the gradients of the cost of the `dA` with respect\n",
    "        # to its parameters\n",
    "        gparams = T.grad(cost, self.params)\n",
    "        # generate the list of updates\n",
    "        updates = [\n",
    "            (param, param - learning_rate * gparam)\n",
    "            for param, gparam in zip(self.params, gparams)\n",
    "        ]\n",
    "\n",
    "        return (cost, updates)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TODO: modify this function to always return a trained model, \n",
    "# with a consistent interface to the functionality of the network\n",
    "# trained models need to be able to predict and ideally persist their params for loading later\n",
    "# the interface to persistence and loading trained models should also be consistent\n",
    "import numpy\n",
    "from theano.tensor.shared_randomstreams import RandomStreams\n",
    "\n",
    "def train_dA(train_X, train_y, learning_rate=0.1, training_epochs=25,\n",
    "            corruption_level=0.0, batch_size=10):\n",
    "\n",
    "    \"\"\"\n",
    "    :type learning_rate: float\n",
    "    :param learning_rate: learning rate used for training the Autoencoder\n",
    "\n",
    "    :type training_epochs: int\n",
    "    :param training_epochs: number of epochs used for training\n",
    "\n",
    "    :type dataset: string\n",
    "    :param dataset: path to the picked dataset\n",
    "\n",
    "    \"\"\"\n",
    "        \n",
    "    # compute number of minibatches for training, validation and testing\n",
    "    n_train_batches = train_X.get_value(borrow=True).shape[0] / batch_size\n",
    "\n",
    "    # allocate symbolic variables for the data\n",
    "    index = T.lscalar()    # index to a [mini]batch\n",
    "    x = T.matrix('x')  # the data will be concatenated word vector windows\n",
    "\n",
    "    # Train the model with corruption\n",
    "\n",
    "    rng = numpy.random.RandomState(123)\n",
    "    theano_rng = RandomStreams(rng.randint(2 ** 30))\n",
    "\n",
    "    # TODO: move \"n_hidden\" out to a more obvious spot\n",
    "    da = dA(\n",
    "        numpy_rng=rng,\n",
    "        theano_rng=theano_rng,\n",
    "        input=x,\n",
    "        n_visible=train_X.get_value().shape[1],\n",
    "        n_hidden=2\n",
    "    )\n",
    "\n",
    "    cost, updates = da.get_cost_updates(\n",
    "        corruption_level=corruption_level,\n",
    "        learning_rate=learning_rate\n",
    "    )\n",
    "\n",
    "    train_da = theano.function(\n",
    "        [index],\n",
    "        cost,\n",
    "        updates=updates,\n",
    "        givens={\n",
    "            x: train_X[index * batch_size: (index + 1) * batch_size]\n",
    "        }\n",
    "    )\n",
    "\n",
    "    start_time = timeit.default_timer()\n",
    "    \n",
    "    # TODO: the epoch code below is common to all training loops\n",
    "    # TODO: there should be a function called \"train\" which takes in the training function and any other args\n",
    "    # then it transparently runs the loop below -- or one of the more complex ones from later notebooks\n",
    "    # the advantage here is that we can modify the training loop in one place and it will work anywhere\n",
    "\n",
    "    ############\n",
    "    # TRAINING #\n",
    "    ############\n",
    "\n",
    "    # go through training epochs applying dropout each time\n",
    "    for epoch in xrange(training_epochs):\n",
    "        # go through training set\n",
    "        c = []\n",
    "        for batch_index in xrange(n_train_batches):\n",
    "            c.append(train_da(batch_index))\n",
    "\n",
    "        print 'Training epoch %d, cost ' % epoch, numpy.mean(c)\n",
    "\n",
    "    end_time = timeit.default_timer()\n",
    "\n",
    "    training_time = (end_time - start_time)\n",
    "# % ((training_time) / 60.)\n",
    "    print('The training code for file ran for %.2fm' % ((training_time) / 60.))\n",
    "\n",
    "    return da\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['', '/home/chris/programs/anaconda/lib/python2.7/site-packages/PyBrain-0.3.1-py2.7.egg', '/home/chris/programs/anaconda/lib/python2.7/site-packages/python_twitter-2.0-py2.7.egg', '/home/chris/projects/GroundHog', '/home/chris/programs/anaconda/lib/python2.7/site-packages/glove-0.0.1-py2.7-linux-x86_64.egg', '/home/chris/programs/anaconda/lib/python2.7/site-packages/progressbar2-2.7.3-py2.7.egg', '/home/chris/programs/anaconda/lib/python2.7/site-packages/gensim-0.11.1_1-py2.7-linux-x86_64.egg', '/home/chris/programs/anaconda/lib/python2.7/site-packages/smart_open-1.2.1-py2.7.egg', '/home/chris/programs/anaconda/lib/python2.7/site-packages/bz2file-0.98-py2.7.egg', '/home/chris/programs/anaconda/lib/python2.7/site-packages/boto-2.38.0-py2.7.egg', '/home/chris/programs/theano/Theano', '/home/chris/projects/machine_learning/neural_networks', '/home/chris/projects/lxmls_2015/lxmls-toolkit', '/home/chris/projects/post_editing_rules', '/home/chris/projects/ipythonblocks/ipythonblocks', '/home/chris/projects/neural_qe', '/home/chris/codebase/ngram/python', '/home/chris/projects/optimizing-autocompleter', '/home/chris/projects', '/home/chris/projects/conll_discourse_parsing', '/home/chris/projects/Metrics/Python', '/home/chris/projects/angular/editor_components/microservices', '/home/chris/projects/distributed_representations/multilingual_w2v', '/home/chris/projects/fuel', '/home/chris/projects/blocks', '/home/chris/projects/dl4mt_labs/dcu_deep_learning/notebooks/day1', '/home/chris/projects/quality_estimation/word-level-error-classification', '/home/chris/projects/quality_estimation/word-level-error-classification/alignment/monolingual/monolingual-word-aligner', '/home/chris/projects/quality_estimation/marmot', '/home/chris/projects/semeval2015/proto', '/home/chris/projects/translate_toolkit', '/home/chris/programs/amagama-tm-server/bin', '/home/chris/programs/amagama-tm-server', '/home/chris/projects/quest/learning/src', '/home/chris/projects/twitter/twitter_interface/tweet_retriever/twitter_backend', '/home/chris/projects/twitter', '/home/chris/projects/post_editing_rules/python_azure_translate_api', '/home/chris/programs/anaconda/lib/python27.zip', '/home/chris/programs/anaconda/lib/python2.7', '/home/chris/programs/anaconda/lib/python2.7/plat-linux2', '/home/chris/programs/anaconda/lib/python2.7/lib-tk', '/home/chris/programs/anaconda/lib/python2.7/lib-old', '/home/chris/programs/anaconda/lib/python2.7/lib-dynload', '/home/chris/.local/lib/python2.7/site-packages', '/home/chris/programs/anaconda/lib/python2.7/site-packages', '/home/chris/programs/anaconda/lib/python2.7/site-packages/PIL', '/home/chris/programs/anaconda/lib/python2.7/site-packages/setuptools-18.3.2-py2.7.egg', '/home/chris/programs/anaconda/lib/python2.7/site-packages/IPython/extensions', '/home/chris/.ipython', '../..', '/home/chris/projects/dl4mt_labs/dcu_deep_learning/']\n"
     ]
    }
   ],
   "source": [
    "print(sys.path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'/home/chris/projects/dl4mt_labs/dcu_deep_learning/notebooks/day1'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "global name 'VECTOR_INDEX_PATH' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-16-1e95e1b7a895>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[0mCUTOFF\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1000\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m \u001b[0mdataset_X\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdataset_y\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mprep_dataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mPOS_DATASET_PATH\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwhich_sets\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'train'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcutoff\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mCUTOFF\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m \u001b[1;31m# train the model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m trained_model = train_dA(dataset_X, dataset_y, learning_rate=0.001,\n",
      "\u001b[1;32m/home/chris/projects/dl4mt_labs/dcu_deep_learning/dl4mt/datasets.py\u001b[0m in \u001b[0;36mprep_dataset\u001b[1;34m(path, which_sets, cutoff)\u001b[0m\n\u001b[0;32m     60\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     61\u001b[0m     \u001b[1;31m# extract windows\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 62\u001b[1;33m     \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcreate_hstacked_vectors\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mVECTOR_INDEX_PATH\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     63\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     64\u001b[0m     \u001b[1;31m# make shared dataset\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: global name 'VECTOR_INDEX_PATH' is not defined"
     ]
    }
   ],
   "source": [
    "# TODO: prep all data here, leveraging data prep functions from earlier notebooks (or this notebook)\n",
    "from dl4mt.datasets import prep_dataset\n",
    "\n",
    "# load the training data\n",
    "\n",
    "# VECTOR_INDEX_PATH = os.path.join(DATASET_LOCATION, WORD_BY_DOC_MATRIX)\n",
    "VECTOR_INDEX_PATH = os.path.join(DATASET_LOCATION, WORD_BY_WORD_MATRIX)\n",
    "CUTOFF = 1000\n",
    "  \n",
    "dataset_X, dataset_y = prep_dataset(POS_DATASET_PATH, VECTOR_INDEX_PATH, which_sets=['train'], cutoff=CUTOFF)\n",
    "# train the model\n",
    "trained_model = train_dA(dataset_X, dataset_y, learning_rate=0.001,\n",
    "                         corruption_level=0.3, training_epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TODO: make the dev and test data loadable in the same way as the training data below\n",
    "\n",
    "# make a theano function to get predictions from a trained model\n",
    "training_data = theano.tensor.matrix('training_X')\n",
    "predictions = trained_model.predict(training_data)\n",
    "get_predictions = theano.function([training_data], predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get predictions and evaluate\n",
    "p = get_predictions(dataset_X.get_value())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib qt\n",
    "\n",
    "# import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "\n",
    "# TODO: Set this at the top of the notebook -- this is the visualization cutoff, not the training cutoff\n",
    "CUTOFF_BEGIN=0\n",
    "CUTOFF_END=1000\n",
    "\n",
    "# TODO: this method of visualizing the labels loses the class tags -- we want to know which color is which\n",
    "y_vals = np.array([y[0] for y in train_y[CUTOFF_BEGIN:CUTOFF_END]])\n",
    "norm_y_vals = y_vals / float(np.amax(y_vals))\n",
    "\n",
    "jitter1 = np.random.normal(loc=0.0, scale=0.05, size=CUTOFF_END-CUTOFF_BEGIN)\n",
    "jitter2 = np.random.normal(loc=0.0, scale=0.05, size=CUTOFF_END-CUTOFF_BEGIN)\n",
    "x1 = p[CUTOFF_BEGIN:CUTOFF_END,0] + jitter1\n",
    "x2 = p[CUTOFF_BEGIN:CUTOFF_END,1] + jitter2\n",
    "\n",
    "\n",
    "plt.scatter(x1, x2, c=norm_y_vals, s=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({0: 17,\n",
       "         1: 319,\n",
       "         2: 118,\n",
       "         3: 19,\n",
       "         4: 133,\n",
       "         5: 125,\n",
       "         6: 12,\n",
       "         7: 152,\n",
       "         9: 18,\n",
       "         10: 31,\n",
       "         11: 56})"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_counts = Counter(y_vals)\n",
    "y_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# WORKING: how to visualize the filters in an autoencoder for text?\n",
    "# Idea: pass the training data through and store the top N instances which activate this node\n",
    "# at the end, output the top 5 instances for each node (convert window instance back to text)\n",
    "\n",
    "# WORKING: visualize 2d autoencoder output\n",
    "\n",
    "# WORKING: vizualize 2d stacked vs 2d \"tied weights\" autoencoders\n",
    "\n",
    "# WORKING: Documents don't really give us any information about POS -- we need a better way of representing words\n",
    "# idea: replicate most common 1000 words 4x\n",
    "# encode each word with a 4000-dimensional vector indicating words which occur in POS-2, -1, +1, +2\n",
    "\n",
    "# A stacked autoencoder can be used as part of an MLP, where the layers are \"fine-tuned\" for the task\n",
    "\n",
    "# SIMPLIFICATION IDEA -- ONLY SELECT A SUBSET OF CLASSES (I.e. just noun vs. adj), THIS SHOULD MAKE COMPARISON EASIER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load fuel dataset (POS windows) DONE\n",
    "# Load one-hot Word X Document numpy matrix DONE\n",
    "\n",
    "# train autoencoder, stop when reconstruction performance doesn't improve any more, or after a fixed number of epochs\n",
    "\n",
    "# CHALLENGE: implement denoising/dropout -- i.e. implement the corruption\n",
    "\n",
    "# save the parameters of the first part of the network (the encoder)\n",
    "# use the encoder parameters to build the prediction transformation\n",
    "\n",
    "# evaluate the performance of the autoencoder features vs. hstacked LSI and W2V features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'DET'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset_X.get_value()[1].nonzero()[0])\n",
    "len(dataset_X.get_value()[1])\n",
    "dataset_X.get_value().shape\n",
    "[corpus_indices['idx2word'][w] for w in train_X[63624]]\n",
    "corpus_indices['idx2tag'][train_y[63624][0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Working Notes\n",
    "# In a separate notebook, compare 2d autoencoder vs 2d LSI -- visualize with scatter plots\n",
    "# cite Hinton etc for this visualization methodology "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
