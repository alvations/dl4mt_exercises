{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "\n",
    "# making sure the dl4mt module is on the path \n",
    "# -- this path depends upon the location where the notebook is running\n",
    "# here it is assumed to be the day1/ directory, change it to point to the location of the dl4mt/ dir if necessary \n",
    "sys.path.append('../..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ../../dl4mt/logistic_regression.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ../../dl4mt/logistic_regression.py\n",
    "# Uncomment to save this cell to file in order to import it in later notebooks\n",
    "\n",
    "import cPickle\n",
    "import gzip\n",
    "import os\n",
    "import sys\n",
    "import timeit\n",
    "\n",
    "import numpy\n",
    "\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "\n",
    "\n",
    "class LogisticRegression(object):\n",
    "    \"\"\"Multi-class Logistic Regression Class\n",
    "\n",
    "    The logistic regression is fully described by a weight matrix :math:`W`\n",
    "    and bias vector :math:`b`. Classification is done by projecting data\n",
    "    points onto a set of hyperplanes, the distance to which is used to\n",
    "    determine a class membership probability.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input, n_in, n_out):\n",
    "        \"\"\" Initialize the parameters of the logistic regression\n",
    "\n",
    "        :type input: theano.tensor.TensorType\n",
    "        :param input: symbolic variable that describes the input of the\n",
    "                      architecture (one minibatch)\n",
    "\n",
    "        :type n_in: int\n",
    "        :param n_in: number of input units, the dimension of the space in\n",
    "                     which the datapoints lie\n",
    "\n",
    "        :type n_out: int\n",
    "        :param n_out: number of output units, the dimension of the space in\n",
    "                      which the labels lie\n",
    "\n",
    "        \"\"\"\n",
    "  \n",
    "        # initialize with 0 the weights W as a matrix of shape (n_in, n_out)\n",
    "        self.W = theano.shared(\n",
    "            value=numpy.zeros(\n",
    "                (n_in, n_out),\n",
    "                dtype=theano.config.floatX\n",
    "            ),\n",
    "            name='W',\n",
    "            borrow=True\n",
    "        )\n",
    "        # initialize the biases b as a vector of n_out 0s\n",
    "        self.b = theano.shared(\n",
    "            value=numpy.zeros(\n",
    "                (n_out,),\n",
    "                dtype=theano.config.floatX\n",
    "            ),\n",
    "            name='b',\n",
    "            borrow=True\n",
    "        )\n",
    "\n",
    "        # symbolic expression for computing the matrix of class-membership\n",
    "        # probabilities\n",
    "        # Where:\n",
    "        # W is a matrix where column-k represent the separation hyperplane for\n",
    "        # class-k\n",
    "        # x is a matrix where row-j  represents input training sample-j\n",
    "        # b is a vector where element-k represent the free parameter of\n",
    "        # hyperplane-k\n",
    "        self.p_y_given_x = T.nnet.softmax(T.dot(input, self.W) + self.b)\n",
    "\n",
    "        # symbolic description of how to map p_y_given_x to the class with the maximum probability\n",
    "        self.y_pred = T.argmax(self.p_y_given_x, axis=1)\n",
    "    \n",
    "        # parameters of the model\n",
    "        self.params = [self.W, self.b]\n",
    "\n",
    "        # keep track of model input -- this will be useful for creating theano functions\n",
    "        self.input = input\n",
    "\n",
    "    # TODO: put math in a separate cell with LaTeX + markdown -- \n",
    "    # TODO: implement this as a challenge -- add tests in the following cell to see if it's correct\n",
    "    def negative_log_likelihood(self, y):\n",
    "        \"\"\"Return the mean of the negative log-likelihood of the prediction\n",
    "        of this model under a given target distribution.\n",
    "\n",
    "        .. math::\n",
    "\n",
    "            \\frac{1}{|\\mathcal{D}|} \\mathcal{L} (\\theta=\\{W,b\\}, \\mathcal{D}) =\n",
    "            \\frac{1}{|\\mathcal{D}|} \\sum_{i=0}^{|\\mathcal{D}|}\n",
    "                \\log(P(Y=y^{(i)}|x^{(i)}, W,b)) \\\\\n",
    "            \\ell (\\theta=\\{W,b\\}, \\mathcal{D})\n",
    "\n",
    "        :type y: theano.tensor.TensorType\n",
    "        :param y: corresponds to a vector that gives for each example the\n",
    "                  correct label\n",
    "\n",
    "        Note: we use the mean instead of the sum so that\n",
    "              the learning rate is less dependent on the batch size\n",
    "        \"\"\"\n",
    "\n",
    "        # y.shape[0] is (symbolically) the number of rows in y, i.e.,\n",
    "        # number of examples (call it n) in the minibatch\n",
    "        # T.arange(y.shape[0]) is a symbolic vector which will contain\n",
    "        # [0,1,2,... n-1] T.log(self.p_y_given_x) is a matrix of\n",
    "        # Log-Probabilities (call it LP) with one row per example and\n",
    "        # one column per class LP[T.arange(y.shape[0]),y] is a vector\n",
    "        # v containing [LP[0,y[0]], LP[1,y[1]], LP[2,y[2]], ...,\n",
    "        # LP[n-1,y[n-1]]] and T.mean(LP[T.arange(y.shape[0]),y]) is\n",
    "        # the mean (across minibatch examples) of the elements in v,\n",
    "        # i.e., the mean log-likelihood across the minibatch.\n",
    "        return -T.mean(T.log(self.p_y_given_x)[T.arange(y.shape[0]), y])\n",
    "\n",
    "    def errors(self, y):\n",
    "        \"\"\"Return a float representing the number of errors in the minibatch\n",
    "        over the total number of examples of the minibatch ; zero one\n",
    "        loss over the size of the minibatch\n",
    "\n",
    "        :type y: theano.tensor.TensorType\n",
    "        :param y: corresponds to a vector that gives for each example the\n",
    "                  correct label\n",
    "        \"\"\"\n",
    "\n",
    "        # check if y has same dimension of y_pred\n",
    "        if y.ndim != self.y_pred.ndim:\n",
    "            raise TypeError(\n",
    "                'y should have the same shape as self.y_pred',\n",
    "                ('y', y.type, 'y_pred', self.y_pred.type)\n",
    "            )\n",
    "        # check if y is of the correct datatype\n",
    "        if y.dtype.startswith('int'):\n",
    "            # the T.neq operator returns a vector of 0s and 1s, where 1\n",
    "            # represents a mistake in prediction\n",
    "            return T.mean(T.neq(self.y_pred, y))\n",
    "        else:\n",
    "            raise NotImplementedError()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from dl4mt.logistic_regression import LogisticRegression\n",
    "\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "\n",
    "# TODO: switch to google code doc style\n",
    "\n",
    "# this function is unique to this model, and initializes the functions and parameters we'll need for training\n",
    "def initialize_logistic_regression(train_dataset, dev_dataset, learning_rate=0.1, batch_size=10):\n",
    "    \n",
    "    \"\"\"Initializes the functions and parameters for a Logistic Regression model\n",
    "\n",
    "    :type learning_rate: float\n",
    "    :param learning_rate: learning rate used (factor for the stochastic\n",
    "                          gradient)\n",
    "\n",
    "    :type n_epochs: int\n",
    "    :param n_epochs: maximal number of epochs to run the optimizer\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    train_set_x, train_set_y = train_dataset\n",
    "    valid_set_x, valid_set_y = dev_dataset\n",
    "\n",
    "    # compute number of minibatches for training, validation and testing\n",
    "    n_train_batches = train_set_x.get_value(borrow=True).shape[0] / batch_size\n",
    "    n_valid_batches = valid_set_x.get_value(borrow=True).shape[0] / batch_size\n",
    "\n",
    "    # create the logistic regression model\n",
    "    # allocate symbolic variables for the data\n",
    "    index = T.lscalar()  # index to a [mini]batch\n",
    "\n",
    "    # generate symbolic variables for input (x and y represent a minibatch)\n",
    "    x = T.matrix('x')  # data\n",
    "    y = T.ivector('y')  # labels, presented as 1D vector of [int] labels\n",
    "\n",
    "    # TODO: parameterize  n_out outside the training function -- try to get directly from data\n",
    "    n_in = train_set_x.get_value().shape[1]\n",
    "    n_out = 12\n",
    "    \n",
    "    classifier = LogisticRegression(input=x, n_in=n_in, n_out=n_out)\n",
    "\n",
    "    # the cost we minimize during training is the negative log likelihood of the model\n",
    "    cost = classifier.negative_log_likelihood(y)\n",
    "\n",
    "    validate_model_func = theano.function(\n",
    "        inputs=[index],\n",
    "        outputs=classifier.errors(y),\n",
    "        givens={\n",
    "            x: valid_set_x[index * batch_size: (index + 1) * batch_size],\n",
    "            y: valid_set_y[index * batch_size: (index + 1) * batch_size]\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # compute the gradient of cost with respect to theta = (W,b)\n",
    "    g_W = T.grad(cost=cost, wrt=classifier.W)\n",
    "    g_b = T.grad(cost=cost, wrt=classifier.b)\n",
    "\n",
    "    # specify how to update the parameters of the model as a list of\n",
    "    # (variable, update expression) pairs.\n",
    "    updates = [(classifier.W, classifier.W - learning_rate * g_W),\n",
    "               (classifier.b, classifier.b - learning_rate * g_b)]\n",
    "\n",
    "    # compiling a Theano function `train_model` that returns the cost, but in\n",
    "    # the same time updates the parameter of the model based on the rules\n",
    "    # defined in `updates`\n",
    "    train_model_func = theano.function(\n",
    "        inputs=[index],\n",
    "        outputs=cost,\n",
    "        updates=updates,\n",
    "        givens={\n",
    "            x: train_set_x[index * batch_size: (index + 1) * batch_size],\n",
    "            y: train_set_y[index * batch_size: (index + 1) * batch_size]\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # return the classifier, the training and validation functions, and n_train_batches, n_valid_batches\n",
    "    return (classifier, train_model_func, validate_model_func, n_train_batches, n_valid_batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ../../dl4mt/training.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ../../dl4mt/training.py\n",
    "\n",
    "# this function is shared by all day1 models which are trained with SGD\n",
    "import timeit\n",
    "import numpy\n",
    "\n",
    "def train_model(train_model, n_train_batches, validate_model=None, n_valid_batches=0, training_epochs=25):\n",
    "    \n",
    "    # early-stopping parameters\n",
    "    patience = 5000  # look at this many examples regardless\n",
    "    patience_increase = 2  # wait this much longer when a new best is\n",
    "                                  # found\n",
    "    improvement_threshold = 0.995  # a relative improvement of this much is\n",
    "                                  # considered significant\n",
    "    validation_frequency = min(n_train_batches, patience / 2)\n",
    "                                  # go through this many\n",
    "                                  # minibatches before checking the network\n",
    "                                  # on the validation set; in this case we\n",
    "                                  # check every epoch\n",
    "\n",
    "    best_validation_loss = numpy.inf\n",
    "    test_score = 0.\n",
    "    start_time = timeit.default_timer()\n",
    "\n",
    "    done_looping = False\n",
    "    epoch = 0\n",
    "    while (epoch < training_epochs) and (not done_looping):\n",
    "        epoch = epoch + 1\n",
    "        mini_batch_costs = []\n",
    "        for minibatch_index in xrange(n_train_batches):\n",
    "            \n",
    "            # iteration number\n",
    "            iter = (epoch - 1) * n_train_batches + minibatch_index\n",
    "            \n",
    "            minibatch_avg_cost = train_model(minibatch_index)\n",
    "            mini_batch_costs.append(minibatch_avg_cost)\n",
    "            \n",
    "            if validate_model is not None:\n",
    "                if (iter + 1) % validation_frequency == 0:\n",
    "                    # compute zero-one loss on validation set\n",
    "                    validation_losses = [validate_model(i)\n",
    "                                         for i in xrange(n_valid_batches)]\n",
    "                    this_validation_loss = numpy.mean(validation_losses)\n",
    "\n",
    "                    print(\n",
    "                        'epoch %i, minibatch %i/%i, validation error %f %%' %\n",
    "                        (\n",
    "                            epoch,\n",
    "                            minibatch_index + 1,\n",
    "                            n_train_batches,\n",
    "                            this_validation_loss * 100.\n",
    "                        )\n",
    "                    )\n",
    "\n",
    "                    # if we got the best validation score until now\n",
    "                    if this_validation_loss < best_validation_loss:\n",
    "                        #improve patience if loss improvement is good enough\n",
    "                        if this_validation_loss < best_validation_loss *  \\\n",
    "                           improvement_threshold:\n",
    "                            patience = max(patience, iter * patience_increase)\n",
    "\n",
    "                        best_validation_loss = this_validation_loss\n",
    "\n",
    "\n",
    "                if patience <= iter:\n",
    "                    done_looping = True\n",
    "                    break\n",
    "         \n",
    "        print(\n",
    "                'epoch %i, average training cost %0.2f' %\n",
    "                (\n",
    "                    epoch,\n",
    "                    numpy.mean(mini_batch_costs)\n",
    "                )\n",
    "        )\n",
    "                \n",
    "\n",
    "    end_time = timeit.default_timer()\n",
    "    print('Optimization complete!')\n",
    "    if best_validation_loss < numpy.inf:\n",
    "        print(\"Best validation score: %f %%\") % (best_validation_loss * 100.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ../../dl4mt/datasets.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ../../dl4mt/datasets.py\n",
    "# Uncomment to save this cell to file in order to import it in later notebooks\n",
    "\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "import numpy as np\n",
    "from fuel.datasets import H5PYDataset\n",
    "\n",
    "def create_hstacked_vectors(index_path, seqs):\n",
    "    # load the index, map the seqs (word indices) through the index, hstack, then cast the results to np.array\n",
    "    index = np.load(index_path)\n",
    "    \n",
    "    working_index = np.load(index_path)\n",
    "    return np.array([np.hstack([index[idx] for idx in seq]) for seq in seqs])\n",
    "\n",
    "# load a portion of a fuel dataset\n",
    "def load_fuel_dataset(filename, which_sets=None):\n",
    "#     TODO: if which_sets is not None return all sets\n",
    "\n",
    "    X, y = H5PYDataset(\n",
    "        filename, which_sets=which_sets,\n",
    "        sources=['instances', 'targets'], load_in_memory=True).data_sources\n",
    "    \n",
    "    # make sure y is 0-dimensional\n",
    "    y = y.ravel()\n",
    "    \n",
    "    return (X,y)\n",
    "    \n",
    "    \n",
    "def shared_dataset(X, y, borrow=True, cast_y=True):\n",
    "    \"\"\" Function that loads the dataset into shared variables\n",
    "\n",
    "    The reason we store our dataset in shared variables is to allow\n",
    "    Theano to copy it into the GPU memory (when code is run on GPU).\n",
    "    Since copying data into the GPU is slow, copying a minibatch everytime\n",
    "    is needed (the default behaviour if the data is not in a shared\n",
    "    variable) would lead to a large decrease in performance.\n",
    "    \"\"\"\n",
    "\n",
    "    shared_x = theano.shared(np.asarray(X,\n",
    "                                           dtype=theano.config.floatX), borrow=borrow)\n",
    "    shared_y = theano.shared(np.asarray(y,\n",
    "                                           dtype=theano.config.floatX), borrow=borrow)\n",
    " \n",
    "    # note that shared_y is cast to int because it is used as a vector of indices\n",
    "    if cast_y:\n",
    "        return shared_x, T.cast(shared_y, 'int32')\n",
    "    return shared_x, shared_y\n",
    "\n",
    "def prep_dataset(path, index_path, which_sets=[], cutoff=None, cast_y=True):\n",
    "    X, y = load_fuel_dataset(path, which_sets=which_sets)\n",
    "    if cutoff is not None:\n",
    "        X = X[:cutoff]\n",
    "        y = y[:cutoff]\n",
    "\n",
    "    # extract windows\n",
    "    X = create_hstacked_vectors(index_path, X)\n",
    "\n",
    "    # make shared dataset\n",
    "    return shared_dataset(X, y, cast_y=cast_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, minibatch 200/200, validation error 24.580000 %\n",
      "epoch 1, training cost 1.00\n",
      "epoch 2, minibatch 200/200, validation error 21.930000 %\n",
      "epoch 2, training cost 0.58\n",
      "epoch 3, minibatch 200/200, validation error 20.820000 %\n",
      "epoch 3, training cost 0.48\n",
      "epoch 4, minibatch 200/200, validation error 20.260000 %\n",
      "epoch 4, training cost 0.43\n",
      "epoch 5, minibatch 200/200, validation error 19.930000 %\n",
      "epoch 5, training cost 0.40\n",
      "epoch 6, minibatch 200/200, validation error 19.640000 %\n",
      "epoch 6, training cost 0.38\n",
      "epoch 7, minibatch 200/200, validation error 19.410000 %\n",
      "epoch 7, training cost 0.36\n",
      "epoch 8, minibatch 200/200, validation error 19.210000 %\n",
      "epoch 8, training cost 0.34\n",
      "epoch 9, minibatch 200/200, validation error 19.240000 %\n",
      "epoch 9, training cost 0.33\n",
      "epoch 10, minibatch 200/200, validation error 18.990000 %\n",
      "epoch 10, training cost 0.32\n",
      "epoch 11, minibatch 200/200, validation error 18.810000 %\n",
      "epoch 11, training cost 0.31\n",
      "epoch 12, minibatch 200/200, validation error 18.780000 %\n",
      "epoch 12, training cost 0.30\n",
      "epoch 13, minibatch 200/200, validation error 18.650000 %\n",
      "epoch 13, training cost 0.29\n",
      "epoch 14, minibatch 200/200, validation error 18.570000 %\n",
      "epoch 14, training cost 0.29\n",
      "epoch 15, minibatch 200/200, validation error 18.470000 %\n",
      "epoch 15, training cost 0.28\n",
      "epoch 16, minibatch 200/200, validation error 18.300000 %\n",
      "epoch 16, training cost 0.27\n",
      "epoch 17, minibatch 200/200, validation error 18.230000 %\n",
      "epoch 17, training cost 0.27\n",
      "epoch 18, minibatch 200/200, validation error 18.150000 %\n",
      "epoch 18, training cost 0.26\n",
      "epoch 19, minibatch 200/200, validation error 18.150000 %\n",
      "epoch 19, training cost 0.26\n",
      "epoch 20, minibatch 200/200, validation error 18.110000 %\n",
      "epoch 20, training cost 0.26\n",
      "epoch 21, minibatch 200/200, validation error 18.080000 %\n",
      "epoch 21, training cost 0.25\n",
      "epoch 22, minibatch 200/200, validation error 18.020000 %\n",
      "epoch 22, training cost 0.25\n",
      "epoch 23, minibatch 200/200, validation error 17.990000 %\n",
      "epoch 23, training cost 0.25\n",
      "epoch 24, minibatch 200/200, validation error 17.930000 %\n",
      "epoch 24, training cost 0.24\n",
      "epoch 25, minibatch 200/200, validation error 17.890000 %\n",
      "epoch 25, training cost 0.24\n",
      "epoch 26, minibatch 200/200, validation error 17.940000 %\n",
      "epoch 26, training cost 0.24\n",
      "epoch 27, minibatch 200/200, validation error 17.940000 %\n",
      "epoch 27, training cost 0.24\n",
      "epoch 28, minibatch 200/200, validation error 17.900000 %\n",
      "epoch 28, training cost 0.23\n",
      "epoch 29, minibatch 200/200, validation error 17.910000 %\n",
      "epoch 29, training cost 0.23\n",
      "epoch 30, minibatch 200/200, validation error 17.890000 %\n",
      "epoch 30, training cost 0.23\n",
      "epoch 31, minibatch 200/200, validation error 17.850000 %\n",
      "epoch 31, training cost 0.23\n",
      "epoch 32, training cost 0.22\n",
      "Optimization complete with best validation score of 17.850000 %\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "from dl4mt.datasets import prep_dataset\n",
    "from dl4mt.training import train_model\n",
    "\n",
    "# location of our datasets\n",
    "DATASET_LOCATION = '../../datasets/'\n",
    "\n",
    "# the pos dataset consists of windows around words\n",
    "POS_DATASET_NAME = 'brown_pos_dataset.hdf5'\n",
    "POS_DATASET_PATH = os.path.join(DATASET_LOCATION, POS_DATASET_NAME)\n",
    "WORD_BY_WORD_MATRIX = 'brown.word-by-word.normalized.npy'\n",
    "VECTOR_INDEX_PATH = os.path.join(DATASET_LOCATION, WORD_BY_WORD_MATRIX)\n",
    "\n",
    "# the cutoff for the maximum number of training and dev examples\n",
    "CUTOFF = 10000\n",
    "  \n",
    "train_dataset = prep_dataset(POS_DATASET_PATH, VECTOR_INDEX_PATH, which_sets=['train'], cutoff=CUTOFF)\n",
    "dev_dataset = prep_dataset(POS_DATASET_PATH, VECTOR_INDEX_PATH, which_sets=['dev'], cutoff=CUTOFF)\n",
    "\n",
    "# get the functions and params that we need for our models\n",
    "initialization_data = initialize_logistic_regression(train_dataset, dev_dataset,\n",
    "                                                     learning_rate=0.1, batch_size=50)\n",
    "\n",
    "classifier, train_model_func, validate_model_func, n_train_batches, n_valid_batches = initialization_data\n",
    "    \n",
    "# load the training function and train the LR model \n",
    "# TODO: -- train_model should return the validation error as a list, then we can plot it\n",
    "# in general, train_model should return any useful information about the training process\n",
    "train_model(train_model_func,  n_train_batches, validate_model=validate_model_func,\n",
    "            n_valid_batches=n_valid_batches, training_epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing ../../dl4mt/predict.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ../../dl4mt/predict.py\n",
    "\n",
    "import theano\n",
    "\n",
    "# TODO: how to do multiple types in docstring?\n",
    "# TODO -- we also want a prediction function for the test data -- should also be common to all models\n",
    "# TODO: model evaluation and visualization should also be shared across notebooks\n",
    "\n",
    "def predict(classifier, test_X):\n",
    "    \"\"\"s\n",
    "    classifier is expected to be either: (1) a theano graph with `input` and `y_pred`,\n",
    "    or a path to a pickled model\n",
    "    \"\"\"\n",
    "\n",
    "    if type(classifier) is str:\n",
    "        # load the saved model at the path specified by 'classifier'\n",
    "        classifier = cPickle.load(open(classifier))\n",
    "\n",
    "    # compile a predictor function\n",
    "    predict_model = theano.function(\n",
    "        inputs=[classifier.input],\n",
    "        outputs=classifier.y_pred)\n",
    "\n",
    "    y_hat = predict_model(test_X)\n",
    "    \n",
    "    return y_hat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO: allow user to load another distributed representation for the same data, such as PCA, or autoencoder vecs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 1.0),\n",
       " (0, 1.0),\n",
       " (0, 2.0),\n",
       " (0, 1.0),\n",
       " (0, 7.0),\n",
       " (0, 6.0),\n",
       " (0, 7.0),\n",
       " (0, 2.0),\n",
       " (0, 4.0),\n",
       " (0, 1.0),\n",
       " (0, 2.0),\n",
       " (0, 7.0),\n",
       " (0, 11.0),\n",
       " (0, 1.0),\n",
       " (0, 5.0),\n",
       " (0, 3.0),\n",
       " (0, 7.0),\n",
       " (0, 4.0),\n",
       " (0, 1.0),\n",
       " (0, 11.0),\n",
       " (0, 11.0),\n",
       " (0, 1.0),\n",
       " (0, 6.0),\n",
       " (0, 7.0),\n",
       " (0, 4.0),\n",
       " (0, 5.0),\n",
       " (0, 11.0),\n",
       " (0, 1.0),\n",
       " (0, 5.0),\n",
       " (0, 5.0),\n",
       " (0, 10.0),\n",
       " (0, 1.0),\n",
       " (0, 1.0),\n",
       " (0, 1.0),\n",
       " (0, 1.0),\n",
       " (0, 1.0),\n",
       " (0, 2.0),\n",
       " (0, 1.0),\n",
       " (0, 7.0),\n",
       " (0, 0.0),\n",
       " (0, 5.0),\n",
       " (0, 10.0),\n",
       " (0, 3.0),\n",
       " (0, 7.0),\n",
       " (0, 7.0),\n",
       " (0, 4.0),\n",
       " (0, 1.0),\n",
       " (0, 5.0),\n",
       " (0, 5.0),\n",
       " (0, 3.0),\n",
       " (0, 7.0),\n",
       " (0, 0.0),\n",
       " (0, 7.0),\n",
       " (0, 0.0),\n",
       " (0, 11.0),\n",
       " (0, 5.0),\n",
       " (0, 5.0),\n",
       " (0, 3.0),\n",
       " (0, 7.0),\n",
       " (0, 5.0),\n",
       " (0, 3.0),\n",
       " (0, 7.0),\n",
       " (0, 7.0),\n",
       " (0, 2.0),\n",
       " (0, 1.0),\n",
       " (0, 0.0),\n",
       " (0, 3.0),\n",
       " (0, 7.0),\n",
       " (0, 7.0),\n",
       " (0, 6.0),\n",
       " (0, 7.0),\n",
       " (0, 11.0),\n",
       " (0, 1.0),\n",
       " (0, 2.0),\n",
       " (0, 11.0),\n",
       " (0, 1.0),\n",
       " (0, 5.0),\n",
       " (0, 0.0),\n",
       " (0, 5.0),\n",
       " (0, 6.0),\n",
       " (0, 7.0),\n",
       " (0, 11.0),\n",
       " (0, 1.0),\n",
       " (0, 5.0),\n",
       " (0, 1.0),\n",
       " (0, 7.0),\n",
       " (0, 4.0),\n",
       " (0, 7.0),\n",
       " (0, 1.0),\n",
       " (0, 5.0),\n",
       " (0, 5.0),\n",
       " (0, 2.0),\n",
       " (0, 4.0),\n",
       " (0, 1.0),\n",
       " (0, 7.0),\n",
       " (0, 3.0),\n",
       " (0, 2.0),\n",
       " (0, 7.0),\n",
       " (0, 5.0),\n",
       " (0, 7.0),\n",
       " (0, 7.0),\n",
       " (0, 3.0),\n",
       " (0, 5.0),\n",
       " (0, 7.0),\n",
       " (0, 3.0),\n",
       " (0, 7.0),\n",
       " (0, 4.0),\n",
       " (0, 1.0),\n",
       " (0, 1.0),\n",
       " (0, 5.0),\n",
       " (0, 5.0),\n",
       " (0, 7.0),\n",
       " (0, 4.0),\n",
       " (0, 11.0),\n",
       " (0, 1.0),\n",
       " (0, 5.0),\n",
       " (0, 4.0),\n",
       " (0, 1.0),\n",
       " (0, 7.0),\n",
       " (0, 1.0),\n",
       " (0, 5.0),\n",
       " (0, 2.0),\n",
       " (0, 4.0),\n",
       " (0, 9.0),\n",
       " (0, 0.0),\n",
       " (0, 7.0),\n",
       " (0, 7.0),\n",
       " (0, 1.0),\n",
       " (0, 5.0),\n",
       " (0, 6.0),\n",
       " (0, 7.0),\n",
       " (0, 4.0),\n",
       " (0, 7.0),\n",
       " (0, 0.0),\n",
       " (0, 4.0),\n",
       " (0, 1.0),\n",
       " (0, 7.0),\n",
       " (0, 7.0),\n",
       " (0, 5.0),\n",
       " (0, 1.0),\n",
       " (0, 11.0),\n",
       " (0, 1.0),\n",
       " (0, 1.0),\n",
       " (0, 10.0),\n",
       " (0, 1.0),\n",
       " (0, 9.0),\n",
       " (0, 1.0),\n",
       " (0, 2.0),\n",
       " (0, 1.0),\n",
       " (0, 1.0),\n",
       " (0, 7.0),\n",
       " (0, 7.0),\n",
       " (0, 2.0),\n",
       " (0, 1.0),\n",
       " (0, 1.0),\n",
       " (0, 5.0),\n",
       " (0, 4.0),\n",
       " (0, 1.0),\n",
       " (0, 7.0),\n",
       " (0, 2.0),\n",
       " (0, 2.0),\n",
       " (0, 11.0),\n",
       " (0, 1.0),\n",
       " (0, 4.0),\n",
       " (0, 1.0),\n",
       " (0, 7.0),\n",
       " (0, 2.0),\n",
       " (0, 6.0),\n",
       " (0, 4.0),\n",
       " (0, 11.0),\n",
       " (0, 9.0),\n",
       " (0, 5.0),\n",
       " (0, 0.0),\n",
       " (0, 0.0),\n",
       " (0, 7.0),\n",
       " (0, 3.0),\n",
       " (0, 7.0),\n",
       " (0, 4.0),\n",
       " (0, 1.0),\n",
       " (0, 2.0),\n",
       " (0, 1.0),\n",
       " (0, 1.0),\n",
       " (0, 5.0),\n",
       " (0, 5.0),\n",
       " (0, 3.0),\n",
       " (0, 7.0),\n",
       " (0, 7.0),\n",
       " (0, 4.0),\n",
       " (0, 1.0),\n",
       " (0, 2.0),\n",
       " (0, 3.0),\n",
       " (0, 2.0),\n",
       " (0, 0.0),\n",
       " (0, 4.0),\n",
       " (0, 1.0),\n",
       " (0, 5.0),\n",
       " (0, 10.0),\n",
       " (0, 11.0),\n",
       " (0, 1.0),\n",
       " (0, 7.0),\n",
       " (0, 4.0),\n",
       " (0, 11.0),\n",
       " (0, 1.0),\n",
       " (0, 2.0),\n",
       " (0, 4.0),\n",
       " (0, 1.0),\n",
       " (0, 1.0),\n",
       " (0, 7.0),\n",
       " (0, 7.0),\n",
       " (0, 2.0),\n",
       " (0, 4.0),\n",
       " (0, 11.0),\n",
       " (0, 1.0),\n",
       " (0, 5.0),\n",
       " (0, 4.0),\n",
       " (0, 1.0),\n",
       " (0, 2.0),\n",
       " (0, 11.0),\n",
       " (0, 7.0),\n",
       " (0, 1.0),\n",
       " (0, 5.0),\n",
       " (0, 4.0),\n",
       " (0, 1.0),\n",
       " (0, 2.0),\n",
       " (0, 1.0),\n",
       " (0, 2.0),\n",
       " (0, 4.0),\n",
       " (0, 1.0),\n",
       " (0, 7.0),\n",
       " (0, 0.0),\n",
       " (0, 7.0),\n",
       " (0, 5.0),\n",
       " (0, 6.0),\n",
       " (0, 7.0),\n",
       " (0, 4.0),\n",
       " (0, 1.0),\n",
       " (0, 2.0),\n",
       " (0, 2.0),\n",
       " (0, 4.0),\n",
       " (0, 5.0),\n",
       " (0, 4.0),\n",
       " (0, 1.0),\n",
       " (0, 7.0),\n",
       " (0, 0.0),\n",
       " (0, 7.0),\n",
       " (0, 4.0),\n",
       " (0, 11.0),\n",
       " (0, 1.0),\n",
       " (0, 5.0),\n",
       " (0, 3.0),\n",
       " (0, 7.0),\n",
       " (0, 7.0),\n",
       " (0, 4.0),\n",
       " (0, 1.0),\n",
       " (0, 2.0),\n",
       " (0, 1.0),\n",
       " (0, 5.0),\n",
       " (0, 2.0),\n",
       " (0, 3.0),\n",
       " (0, 7.0),\n",
       " (0, 0.0),\n",
       " (0, 7.0),\n",
       " (0, 4.0),\n",
       " (0, 1.0),\n",
       " (0, 2.0),\n",
       " (0, 4.0),\n",
       " (0, 1.0),\n",
       " (0, 7.0),\n",
       " (0, 7.0),\n",
       " (0, 11.0),\n",
       " (0, 1.0),\n",
       " (0, 5.0),\n",
       " (0, 10.0),\n",
       " (0, 0.0),\n",
       " (0, 7.0),\n",
       " (0, 6.0),\n",
       " (0, 4.0),\n",
       " (0, 1.0),\n",
       " (0, 2.0),\n",
       " (0, 4.0),\n",
       " (0, 1.0),\n",
       " (0, 2.0),\n",
       " (0, 1.0),\n",
       " (0, 5.0),\n",
       " (0, 3.0),\n",
       " (0, 7.0),\n",
       " (0, 7.0),\n",
       " (0, 4.0),\n",
       " (0, 1.0),\n",
       " (0, 2.0),\n",
       " (0, 4.0),\n",
       " (0, 1.0),\n",
       " (0, 5.0),\n",
       " (0, 10.0),\n",
       " (0, 4.0),\n",
       " (0, 1.0),\n",
       " (0, 7.0),\n",
       " (0, 7.0),\n",
       " (0, 7.0),\n",
       " (0, 4.0),\n",
       " (0, 1.0),\n",
       " (0, 6.0),\n",
       " (0, 7.0),\n",
       " (0, 4.0),\n",
       " (0, 1.0),\n",
       " (0, 2.0),\n",
       " (0, 7.0),\n",
       " (0, 6.0),\n",
       " (0, 1.0),\n",
       " (0, 2.0),\n",
       " (0, 4.0),\n",
       " (0, 1.0),\n",
       " (0, 5.0),\n",
       " (0, 11.0),\n",
       " (0, 11.0),\n",
       " (0, 11.0),\n",
       " (0, 1.0),\n",
       " (0, 1.0),\n",
       " (0, 7.0),\n",
       " (0, 1.0),\n",
       " (0, 7.0),\n",
       " (0, 0.0),\n",
       " (0, 4.0),\n",
       " (0, 5.0),\n",
       " (0, 11.0),\n",
       " (0, 1.0),\n",
       " (0, 5.0),\n",
       " (0, 5.0),\n",
       " (0, 6.0),\n",
       " (0, 0.0),\n",
       " (0, 2.0),\n",
       " (0, 1.0),\n",
       " (0, 1.0),\n",
       " (0, 5.0),\n",
       " (0, 4.0),\n",
       " (0, 7.0),\n",
       " (0, 4.0),\n",
       " (0, 0.0),\n",
       " (0, 11.0),\n",
       " (0, 1.0),\n",
       " (0, 2.0),\n",
       " (0, 4.0),\n",
       " (0, 11.0),\n",
       " (0, 1.0),\n",
       " (0, 2.0),\n",
       " (0, 11.0),\n",
       " (0, 1.0),\n",
       " (0, 5.0),\n",
       " (0, 1.0),\n",
       " (0, 7.0),\n",
       " (0, 2.0),\n",
       " (0, 1.0),\n",
       " (0, 7.0),\n",
       " (0, 4.0),\n",
       " (0, 1.0),\n",
       " (0, 7.0),\n",
       " (0, 1.0),\n",
       " (0, 1.0),\n",
       " (0, 0.0),\n",
       " (0, 0.0),\n",
       " (0, 5.0),\n",
       " (0, 6.0),\n",
       " (0, 0.0),\n",
       " (0, 11.0),\n",
       " (0, 6.0),\n",
       " (0, 7.0),\n",
       " (0, 4.0),\n",
       " (0, 1.0),\n",
       " (0, 2.0),\n",
       " (0, 1.0),\n",
       " (0, 5.0),\n",
       " (0, 10.0),\n",
       " (0, 7.0),\n",
       " (0, 6.0),\n",
       " (0, 4.0),\n",
       " (0, 1.0),\n",
       " (0, 5.0),\n",
       " (0, 1.0),\n",
       " (0, 2.0),\n",
       " (0, 4.0),\n",
       " (0, 1.0),\n",
       " (0, 1.0),\n",
       " (0, 1.0),\n",
       " (0, 5.0),\n",
       " (0, 5.0),\n",
       " (0, 7.0),\n",
       " (0, 4.0),\n",
       " (0, 11.0),\n",
       " (0, 1.0),\n",
       " (0, 5.0),\n",
       " (0, 4.0),\n",
       " (0, 1.0),\n",
       " (0, 3.0),\n",
       " (0, 7.0),\n",
       " (0, 4.0),\n",
       " (0, 1.0),\n",
       " (0, 1.0),\n",
       " (0, 7.0),\n",
       " (0, 7.0),\n",
       " (0, 4.0),\n",
       " (0, 1.0),\n",
       " (0, 5.0),\n",
       " (0, 5.0),\n",
       " (0, 4.0),\n",
       " (0, 11.0),\n",
       " (0, 1.0),\n",
       " (0, 7.0),\n",
       " (0, 1.0),\n",
       " (0, 7.0),\n",
       " (0, 4.0),\n",
       " (0, 9.0),\n",
       " (0, 1.0),\n",
       " (0, 2.0),\n",
       " (0, 4.0),\n",
       " (0, 11.0),\n",
       " (0, 1.0),\n",
       " (0, 5.0),\n",
       " (0, 0.0),\n",
       " (0, 11.0),\n",
       " (0, 1.0),\n",
       " (0, 7.0),\n",
       " (0, 11.0),\n",
       " (0, 2.0),\n",
       " (0, 4.0),\n",
       " (0, 1.0),\n",
       " (0, 7.0),\n",
       " (0, 5.0),\n",
       " (0, 6.0),\n",
       " (0, 7.0),\n",
       " (0, 6.0),\n",
       " (0, 7.0),\n",
       " (0, 6.0),\n",
       " (0, 4.0),\n",
       " (0, 1.0),\n",
       " (0, 7.0),\n",
       " (0, 5.0),\n",
       " (0, 4.0),\n",
       " (0, 1.0),\n",
       " (0, 1.0),\n",
       " (0, 1.0),\n",
       " (0, 5.0),\n",
       " (0, 1.0),\n",
       " (0, 9.0),\n",
       " (0, 5.0),\n",
       " (0, 7.0),\n",
       " (0, 5.0),\n",
       " (0, 11.0),\n",
       " (0, 1.0),\n",
       " (0, 7.0),\n",
       " (0, 4.0),\n",
       " (0, 1.0),\n",
       " (0, 1.0),\n",
       " (0, 1.0),\n",
       " (0, 5.0),\n",
       " (0, 7.0),\n",
       " (0, 2.0),\n",
       " (0, 4.0),\n",
       " (0, 1.0),\n",
       " (0, 3.0),\n",
       " (0, 7.0),\n",
       " (0, 1.0),\n",
       " (0, 11.0),\n",
       " (0, 2.0),\n",
       " (0, 3.0),\n",
       " (0, 5.0),\n",
       " (0, 4.0),\n",
       " (0, 7.0),\n",
       " (0, 1.0),\n",
       " (0, 5.0),\n",
       " (0, 3.0),\n",
       " (0, 7.0),\n",
       " (0, 2.0),\n",
       " (0, 3.0),\n",
       " (0, 5.0),\n",
       " (0, 7.0),\n",
       " (0, 0.0),\n",
       " (0, 7.0),\n",
       " (0, 10.0),\n",
       " (0, 7.0),\n",
       " (0, 11.0),\n",
       " (0, 2.0),\n",
       " (0, 4.0),\n",
       " (0, 0.0),\n",
       " (0, 11.0),\n",
       " (0, 1.0),\n",
       " (0, 2.0),\n",
       " (0, 5.0),\n",
       " (0, 10.0),\n",
       " (0, 1.0),\n",
       " (0, 2.0),\n",
       " (0, 5.0),\n",
       " (0, 11.0),\n",
       " (0, 1.0),\n",
       " (0, 5.0),\n",
       " (0, 11.0),\n",
       " (0, 2.0),\n",
       " (0, 6.0),\n",
       " (0, 6.0),\n",
       " (0, 7.0),\n",
       " (0, 11.0),\n",
       " (0, 1.0),\n",
       " (0, 2.0),\n",
       " (0, 11.0),\n",
       " (0, 1.0),\n",
       " (0, 0.0),\n",
       " (0, 7.0),\n",
       " (0, 2.0),\n",
       " (0, 4.0),\n",
       " (0, 1.0),\n",
       " (0, 11.0),\n",
       " (0, 2.0),\n",
       " (0, 4.0),\n",
       " (0, 4.0),\n",
       " (0, 1.0),\n",
       " (0, 7.0),\n",
       " (0, 5.0),\n",
       " (0, 4.0),\n",
       " (0, 1.0),\n",
       " (0, 7.0),\n",
       " (0, 2.0),\n",
       " (0, 11.0),\n",
       " (0, 1.0),\n",
       " (0, 2.0),\n",
       " (0, 1.0),\n",
       " (0, 2.0),\n",
       " (0, 11.0),\n",
       " (0, 1.0),\n",
       " (0, 2.0),\n",
       " (0, 4.0),\n",
       " (0, 1.0),\n",
       " (0, 5.0),\n",
       " (0, 4.0),\n",
       " (0, 1.0),\n",
       " (0, 2.0),\n",
       " (0, 4.0),\n",
       " (0, 1.0),\n",
       " (0, 7.0),\n",
       " (0, 7.0),\n",
       " (0, 7.0),\n",
       " (0, 2.0),\n",
       " (0, 4.0),\n",
       " (0, 1.0),\n",
       " (0, 6.0),\n",
       " (0, 7.0),\n",
       " (0, 5.0),\n",
       " (0, 4.0),\n",
       " (0, 7.0),\n",
       " (0, 7.0),\n",
       " (0, 6.0),\n",
       " (0, 7.0),\n",
       " (0, 2.0),\n",
       " (0, 3.0),\n",
       " (0, 7.0),\n",
       " (0, 7.0),\n",
       " (0, 10.0),\n",
       " (0, 5.0),\n",
       " (0, 6.0),\n",
       " (0, 7.0),\n",
       " (0, 4.0),\n",
       " (0, 1.0),\n",
       " (0, 2.0),\n",
       " (0, 1.0),\n",
       " (0, 5.0),\n",
       " (0, 10.0),\n",
       " (0, 7.0),\n",
       " (0, 3.0),\n",
       " (0, 5.0),\n",
       " (0, 7.0),\n",
       " (0, 6.0),\n",
       " (0, 11.0),\n",
       " (0, 1.0),\n",
       " (0, 2.0),\n",
       " (0, 1.0),\n",
       " (0, 2.0),\n",
       " (0, 0.0),\n",
       " (0, 4.0),\n",
       " (0, 11.0),\n",
       " (0, 7.0),\n",
       " (0, 7.0),\n",
       " (0, 6.0),\n",
       " (0, 7.0),\n",
       " (0, 1.0),\n",
       " (0, 10.0),\n",
       " (0, 7.0),\n",
       " (0, 2.0),\n",
       " (0, 4.0),\n",
       " (0, 11.0),\n",
       " (0, 1.0),\n",
       " (0, 5.0),\n",
       " (0, 5.0),\n",
       " (0, 4.0),\n",
       " (0, 3.0),\n",
       " (0, 7.0),\n",
       " (0, 7.0),\n",
       " (0, 6.0),\n",
       " (0, 7.0),\n",
       " (0, 7.0),\n",
       " (0, 7.0),\n",
       " (0, 10.0),\n",
       " (0, 7.0),\n",
       " (0, 4.0),\n",
       " (0, 1.0),\n",
       " (0, 2.0),\n",
       " (0, 11.0),\n",
       " (0, 1.0),\n",
       " (0, 10.0),\n",
       " (0, 11.0),\n",
       " (0, 1.0),\n",
       " (0, 11.0),\n",
       " (0, 1.0),\n",
       " (0, 10.0),\n",
       " (0, 11.0),\n",
       " (0, 10.0),\n",
       " (0, 11.0),\n",
       " (0, 1.0),\n",
       " (0, 5.0),\n",
       " (0, 3.0),\n",
       " (0, 7.0),\n",
       " (0, 2.0),\n",
       " (0, 1.0),\n",
       " (0, 0.0),\n",
       " (0, 11.0),\n",
       " (0, 2.0),\n",
       " (0, 4.0),\n",
       " (0, 1.0),\n",
       " (0, 2.0),\n",
       " (0, 11.0),\n",
       " (0, 11.0),\n",
       " (0, 1.0),\n",
       " (0, 6.0),\n",
       " (0, 7.0),\n",
       " (0, 7.0),\n",
       " (0, 11.0),\n",
       " (0, 1.0),\n",
       " (0, 5.0),\n",
       " (0, 1.0),\n",
       " (0, 5.0),\n",
       " (0, 7.0),\n",
       " (0, 7.0),\n",
       " (0, 0.0),\n",
       " (0, 6.0),\n",
       " (0, 1.0),\n",
       " (0, 2.0),\n",
       " (0, 4.0),\n",
       " (0, 11.0),\n",
       " (0, 1.0),\n",
       " (0, 2.0),\n",
       " (0, 4.0),\n",
       " (0, 11.0),\n",
       " (0, 1.0),\n",
       " (0, 1.0),\n",
       " (0, 5.0),\n",
       " (0, 4.0),\n",
       " (0, 1.0),\n",
       " (0, 2.0),\n",
       " (0, 7.0),\n",
       " (0, 11.0),\n",
       " (0, 1.0),\n",
       " (0, 1.0),\n",
       " (0, 7.0),\n",
       " (0, 4.0),\n",
       " (0, 1.0),\n",
       " (0, 5.0),\n",
       " (0, 10.0),\n",
       " (0, 4.0),\n",
       " (0, 11.0),\n",
       " (0, 1.0),\n",
       " (0, 7.0),\n",
       " (0, 5.0),\n",
       " (0, 2.0),\n",
       " (0, 4.0),\n",
       " (0, 1.0),\n",
       " (0, 5.0),\n",
       " (0, 0.0),\n",
       " (0, 11.0),\n",
       " (0, 5.0),\n",
       " (0, 4.0),\n",
       " (0, 9.0),\n",
       " (0, 1.0),\n",
       " (0, 1.0),\n",
       " (0, 2.0),\n",
       " (0, 11.0),\n",
       " (0, 1.0),\n",
       " (0, 7.0),\n",
       " (0, 4.0),\n",
       " (0, 1.0),\n",
       " (0, 2.0),\n",
       " (0, 4.0),\n",
       " (0, 1.0),\n",
       " (0, 2.0),\n",
       " (0, 1.0),\n",
       " (0, 2.0),\n",
       " (0, 9.0),\n",
       " (0, 9.0),\n",
       " (0, 1.0),\n",
       " (0, 5.0),\n",
       " (0, 2.0),\n",
       " (0, 4.0),\n",
       " (0, 11.0),\n",
       " (0, 1.0),\n",
       " (0, 2.0),\n",
       " (0, 9.0),\n",
       " (0, 1.0),\n",
       " (0, 1.0),\n",
       " (0, 5.0),\n",
       " (0, 3.0),\n",
       " (0, 7.0),\n",
       " (0, 11.0),\n",
       " (0, 6.0),\n",
       " (0, 7.0),\n",
       " (0, 2.0),\n",
       " (0, 4.0),\n",
       " (0, 1.0),\n",
       " (0, 7.0),\n",
       " (0, 0.0),\n",
       " (0, 3.0),\n",
       " (0, 7.0),\n",
       " (0, 4.0),\n",
       " (0, 1.0),\n",
       " (0, 7.0),\n",
       " (0, 5.0),\n",
       " (0, 7.0),\n",
       " (0, 6.0),\n",
       " (0, 7.0),\n",
       " (0, 4.0),\n",
       " (0, 11.0),\n",
       " (0, 11.0),\n",
       " (0, 1.0),\n",
       " (0, 5.0),\n",
       " (0, 2.0),\n",
       " (0, 4.0),\n",
       " (0, 11.0),\n",
       " (0, 1.0),\n",
       " (0, 6.0),\n",
       " (0, 7.0),\n",
       " (0, 4.0),\n",
       " (0, 1.0),\n",
       " (0, 5.0),\n",
       " (0, 2.0),\n",
       " (0, 1.0),\n",
       " (0, 6.0),\n",
       " (0, 7.0),\n",
       " (0, 11.0),\n",
       " (0, 1.0),\n",
       " (0, 3.0),\n",
       " (0, 7.0),\n",
       " (0, 11.0),\n",
       " (0, 2.0),\n",
       " (0, 4.0),\n",
       " (0, 1.0),\n",
       " (0, 3.0),\n",
       " (0, 7.0),\n",
       " (0, 6.0),\n",
       " (0, 4.0),\n",
       " (0, 0.0),\n",
       " (0, 11.0),\n",
       " (0, 6.0),\n",
       " (0, 7.0),\n",
       " (0, 2.0),\n",
       " (0, 4.0),\n",
       " (0, 11.0),\n",
       " (0, 1.0),\n",
       " (0, 4.0),\n",
       " (0, 11.0),\n",
       " (0, 1.0),\n",
       " (0, 2.0),\n",
       " (0, 4.0),\n",
       " (0, 1.0),\n",
       " (0, 1.0),\n",
       " (0, 5.0),\n",
       " (0, 2.0),\n",
       " (0, 4.0),\n",
       " (0, 1.0),\n",
       " (0, 1.0),\n",
       " (0, 7.0),\n",
       " (0, 2.0),\n",
       " (0, 0.0),\n",
       " (0, 7.0),\n",
       " (0, 4.0),\n",
       " (0, 11.0),\n",
       " (0, 10.0),\n",
       " (0, 11.0),\n",
       " (0, 1.0),\n",
       " (0, 11.0),\n",
       " (0, 1.0),\n",
       " (0, 7.0),\n",
       " (0, 5.0),\n",
       " (0, 1.0),\n",
       " (0, 1.0),\n",
       " (0, 1.0),\n",
       " (0, 5.0),\n",
       " (0, 11.0),\n",
       " (0, 1.0),\n",
       " (0, 2.0),\n",
       " (0, 4.0),\n",
       " (0, 11.0),\n",
       " (0, 1.0),\n",
       " (0, 1.0),\n",
       " (0, 7.0),\n",
       " (0, 10.0),\n",
       " (0, 1.0),\n",
       " (0, 1.0),\n",
       " (0, 5.0),\n",
       " (0, 2.0),\n",
       " (0, 4.0),\n",
       " (0, 1.0),\n",
       " (0, 2.0),\n",
       " (0, 4.0),\n",
       " (0, 1.0),\n",
       " (0, 1.0),\n",
       " (0, 2.0),\n",
       " (0, 11.0),\n",
       " (0, 1.0),\n",
       " (0, 2.0),\n",
       " (0, 1.0),\n",
       " (0, 2.0),\n",
       " (0, 1.0),\n",
       " (0, 9.0),\n",
       " (0, 5.0),\n",
       " (0, 9.0),\n",
       " (0, 5.0),\n",
       " (0, 7.0),\n",
       " (0, 2.0),\n",
       " (0, 4.0),\n",
       " (0, 1.0),\n",
       " (0, 2.0),\n",
       " (0, 7.0),\n",
       " (0, 6.0),\n",
       " (0, 7.0),\n",
       " (0, 1.0),\n",
       " (0, 0.0),\n",
       " (0, 6.0),\n",
       " (0, 7.0),\n",
       " (0, 4.0),\n",
       " (0, 1.0),\n",
       " (0, 1.0),\n",
       " (0, 10.0),\n",
       " (0, 6.0),\n",
       " (0, 7.0),\n",
       " (0, 11.0),\n",
       " (0, 1.0),\n",
       " (0, 11.0),\n",
       " (0, 2.0),\n",
       " (0, 1.0),\n",
       " (0, 0.0),\n",
       " (0, 3.0),\n",
       " (0, 7.0),\n",
       " (0, 5.0),\n",
       " (0, 5.0),\n",
       " (0, 2.0),\n",
       " (0, 4.0),\n",
       " (0, 7.0),\n",
       " (0, 4.0),\n",
       " (0, 11.0),\n",
       " (0, 1.0),\n",
       " (0, 2.0),\n",
       " (0, 4.0),\n",
       " (0, 1.0),\n",
       " (0, 1.0),\n",
       " (0, 10.0),\n",
       " (0, 2.0),\n",
       " (0, 4.0),\n",
       " (0, 7.0),\n",
       " (0, 4.0),\n",
       " (0, 11.0),\n",
       " (0, 1.0),\n",
       " (0, 5.0),\n",
       " (0, 1.0),\n",
       " (0, 2.0),\n",
       " (0, 4.0),\n",
       " (0, 7.0),\n",
       " (0, 7.0),\n",
       " (0, 11.0),\n",
       " (0, 6.0),\n",
       " (0, 7.0),\n",
       " (0, 5.0),\n",
       " (0, 5.0),\n",
       " (0, 3.0),\n",
       " (0, 7.0),\n",
       " (0, 6.0),\n",
       " (0, 6.0),\n",
       " (0, 7.0),\n",
       " (0, 5.0),\n",
       " (0, 5.0),\n",
       " (0, 2.0),\n",
       " (0, 4.0),\n",
       " (0, 11.0),\n",
       " (0, 1.0),\n",
       " (0, 5.0),\n",
       " (0, 1.0),\n",
       " (0, 2.0),\n",
       " (0, 8.0),\n",
       " (0, 7.0),\n",
       " (0, 0.0),\n",
       " (0, 7.0),\n",
       " (0, 2.0),\n",
       " (0, 4.0),\n",
       " (0, 1.0),\n",
       " (0, 2.0),\n",
       " (0, 1.0),\n",
       " (0, 2.0),\n",
       " (0, 7.0),\n",
       " (0, 1.0),\n",
       " (0, 5.0),\n",
       " (0, 3.0),\n",
       " (0, 7.0),\n",
       " (0, 0.0),\n",
       " (0, 2.0),\n",
       " (0, 1.0),\n",
       " (0, 2.0),\n",
       " (0, 4.0),\n",
       " (0, 1.0),\n",
       " (0, 2.0),\n",
       " (0, 7.0),\n",
       " (0, 7.0),\n",
       " (0, 11.0),\n",
       " (0, 1.0),\n",
       " (0, 4.0),\n",
       " (0, 1.0),\n",
       " (0, 0.0),\n",
       " (0, 11.0),\n",
       " (0, 4.0),\n",
       " (0, 1.0),\n",
       " (0, 3.0),\n",
       " (0, 7.0),\n",
       " (0, 7.0),\n",
       " (0, 5.0),\n",
       " (0, 5.0),\n",
       " (0, 2.0),\n",
       " (0, 1.0),\n",
       " (0, 1.0),\n",
       " (0, 7.0),\n",
       " (0, 4.0),\n",
       " (0, 7.0),\n",
       " (0, 1.0),\n",
       " (0, 2.0),\n",
       " (0, 1.0),\n",
       " (0, 1.0),\n",
       " (0, 2.0),\n",
       " (0, 1.0),\n",
       " (0, 2.0),\n",
       " (0, 4.0),\n",
       " (0, 11.0),\n",
       " (0, 1.0),\n",
       " (0, 2.0),\n",
       " (0, 4.0),\n",
       " (0, 1.0),\n",
       " (0, 5.0),\n",
       " (0, 4.0),\n",
       " (0, 1.0),\n",
       " (0, 2.0),\n",
       " (0, 4.0),\n",
       " (0, 1.0),\n",
       " (0, 1.0),\n",
       " (0, 7.0),\n",
       " (0, 0.0),\n",
       " (0, 7.0),\n",
       " (0, 6.0),\n",
       " (0, 7.0),\n",
       " (0, 7.0),\n",
       " (0, 0.0),\n",
       " (0, 2.0),\n",
       " (0, 7.0),\n",
       " (0, 1.0),\n",
       " (0, 6.0),\n",
       " (0, 7.0),\n",
       " (0, 1.0),\n",
       " (0, 2.0),\n",
       " (0, 11.0),\n",
       " (0, 1.0),\n",
       " (0, 5.0),\n",
       " (0, 3.0),\n",
       " (0, 2.0),\n",
       " (0, 11.0),\n",
       " (0, 1.0),\n",
       " (0, 7.0),\n",
       " (0, 4.0),\n",
       " (0, 1.0),\n",
       " (0, 2.0),\n",
       " (0, 9.0),\n",
       " (0, 7.0),\n",
       " (0, 1.0),\n",
       " (0, 5.0),\n",
       " (0, 9.0),\n",
       " (0, 0.0),\n",
       " (0, 7.0),\n",
       " (0, 11.0),\n",
       " (0, 1.0),\n",
       " (0, 2.0),\n",
       " (0, 4.0),\n",
       " (0, 1.0),\n",
       " (0, 5.0),\n",
       " (0, 10.0),\n",
       " (0, 4.0),\n",
       " (0, 11.0),\n",
       " (0, 7.0),\n",
       " (0, 4.0),\n",
       " (0, 1.0),\n",
       " (0, 11.0),\n",
       " (0, 1.0),\n",
       " ...]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dl4mt.predict import predict\n",
    "\n",
    "test_dataset = prep_dataset(POS_DATASET_PATH, VECTOR_INDEX_PATH, which_sets=['test'], cutoff=CUTOFF, cast_y=False)\n",
    "\n",
    "test_X, test_y = test_dataset\n",
    "predictions = predict(classifier, test_X.get_value())\n",
    "predictions\n",
    "zip(predictions, list(test_y.get_value()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.052145424762251445"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum([y==p for y,p in zip(predictions, list(test_y.get_value()))]) / float(len(predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO: visualize the error with different training hyperparams\n",
    "# TODO: functions for mapping back to the original dataset to check what we got wrong and why\n",
    "# TODO: add a theano graph visualization since this model is relatively simple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "                    if save_model_path is not None:\n",
    "                        # save the best model\n",
    "                        with open(save_model + '.pkl', 'w') as f:\n",
    "                            cPickle.dump(classifier, f)\n",
    "                        print(save)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "#     n_test_batches = test_set_x.get_value(borrow=True).shape[0] / batch_size\n",
    "\n",
    "# def predict():\n",
    "#     \"\"\"\n",
    "#     An example of how to load a trained model and use it\n",
    "#     to predict labels.\n",
    "#     \"\"\"\n",
    "\n",
    "#     # load the saved model\n",
    "#     classifier = cPickle.load(open('best_model.pkl'))\n",
    "\n",
    "#     # compile a predictor function\n",
    "#     predict_model = theano.function(\n",
    "#         inputs=[classifier.input],\n",
    "#         outputs=classifier.y_pred)\n",
    "\n",
    "#     # We can test it on some examples from test test\n",
    "#     dataset='mnist.pkl.gz'\n",
    "#     datasets = load_data(dataset)\n",
    "#     test_set_x, test_set_y = datasets[2]\n",
    "#     test_set_x = test_set_x.get_value()\n",
    "\n",
    "#     predicted_values = predict_model(test_set_x[:10])\n",
    "#     print (\"Predicted values for the first 10 examples in test set:\")\n",
    "#     print predicted_values\n",
    "\n",
    "\n",
    "# test it on the test set -- TODO: testing should be done outside of training -- move this\n",
    "#                     test_losses = [test_model(i)\n",
    "#                                    for i in xrange(n_test_batches)]\n",
    "#                     test_score = numpy.mean(test_losses)\n",
    "\n",
    "#                     print(\n",
    "#                         (\n",
    "#                             '     epoch %i, minibatch %i/%i, test error of'\n",
    "#                             ' best model %f %%'\n",
    "#                         ) %\n",
    "#                         (\n",
    "#                             epoch,\n",
    "#                             minibatch_index + 1,\n",
    "#                             n_train_batches,\n",
    "#                             test_score * 100.\n",
    "#                         )\n",
    "#                     )\n",
    "\n",
    "#     print 'The code run for %d epochs, with %f epochs/sec' % (\n",
    "#         epoch, 1. * epoch / (end_time - start_time))\n",
    "#     print >> sys.stderr, ('The code for file ' +\n",
    "#                           os.path.split(__file__)[1] +\n",
    "#                           ' ran for %.1fs' % ((end_time - start_time)))\n",
    "\n",
    "    # compiling a Theano function that computes the mistakes that are made by\n",
    "    # the model on a minibatch\n",
    "#     test_model = theano.function(\n",
    "#         inputs=[index],\n",
    "#         outputs=classifier.errors(y),\n",
    "#         givens={\n",
    "#             x: test_set_x[index * batch_size: (index + 1) * batch_size],\n",
    "#             y: test_set_y[index * batch_size: (index + 1) * batch_size]\n",
    "#         }\n",
    "#     )\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "This tutorial introduces logistic regression using Theano and stochastic\n",
    "gradient descent.\n",
    "\n",
    "Logistic regression is a probabilistic, linear classifier. It is parametrized\n",
    "by a weight matrix :math:`W` and a bias vector :math:`b`. Classification is\n",
    "done by projecting data points onto a set of hyperplanes, the distance to\n",
    "which is used to determine a class membership probability.\n",
    "\n",
    "Mathematically, this can be written as:\n",
    "\n",
    ".. math::\n",
    "  P(Y=i|x, W,b) &= softmax_i(W x + b) \\\\\n",
    "                &= \\frac {e^{W_i x + b_i}} {\\sum_j e^{W_j x + b_j}}\n",
    "\n",
    "\n",
    "The output of the model or prediction is then done by taking the argmax of\n",
    "the vector whose i'th element is P(Y=i|x).\n",
    "\n",
    ".. math::\n",
    "\n",
    "  y_{pred} = argmax_i P(Y=i|x,W,b)\n",
    "\n",
    "\n",
    "This tutorial presents a stochastic gradient descent optimization method\n",
    "suitable for large datasets.\n",
    "\n",
    "\n",
    "References:\n",
    "\n",
    "    - textbooks: \"Pattern Recognition and Machine Learning\" -\n",
    "                 Christopher M. Bishop, section 4.3.2\n",
    "\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
