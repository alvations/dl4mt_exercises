{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import pylab\n",
    "import sys\n",
    "\n",
    "pylab.rcParams['figure.figsize'] = (10.0, 8.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function, division\n",
    "import logging\n",
    "import pprint\n",
    "import math\n",
    "import numpy\n",
    "import numpy as np\n",
    "import os\n",
    "import operator\n",
    "import theano\n",
    "\n",
    "from theano import tensor\n",
    "\n",
    "from collections import OrderedDict\n",
    "\n",
    "from theano import function\n",
    "from fuel.datasets import IndexableDataset\n",
    "from fuel.transformers import Mapping, Batch, Padding, Filter\n",
    "from fuel.schemes import (ConstantScheme, ShuffledScheme,\n",
    "                          ShuffledExampleScheme, SequentialExampleScheme,\n",
    "                          BatchSizeScheme)\n",
    "from fuel.transformers import Flatten\n",
    "from fuel.streams import DataStream\n",
    "from blocks.config import config\n",
    "from blocks.bricks import Tanh, Initializable, Logistic, Identity\n",
    "from blocks.bricks.base import application\n",
    "from blocks.bricks import Linear, Rectifier, Softmax\n",
    "from blocks.graph import ComputationGraph\n",
    "from blocks.bricks.lookup import LookupTable\n",
    "from blocks.bricks.recurrent import SimpleRecurrent, GatedRecurrent, LSTM, Bidirectional\n",
    "from blocks.bricks.parallel import Fork\n",
    "from blocks.bricks.sequence_generators import (\n",
    "    SequenceGenerator, Readout, SoftmaxEmitter, LookupFeedback)\n",
    "from blocks.algorithms import (GradientDescent, Scale,\n",
    "                               StepClipping, CompositeRule, AdaDelta, Adam)\n",
    "from blocks.initialization import Orthogonal, IsotropicGaussian, Constant\n",
    "from blocks.bricks.cost import CategoricalCrossEntropy\n",
    "from blocks.bricks.cost import SquaredError\n",
    "from blocks.serialization import load_parameter_values\n",
    "from blocks.model import Model\n",
    "from blocks.monitoring import aggregation\n",
    "from blocks.extensions import FinishAfter, Printing, Timing\n",
    "from blocks.extensions.saveload import Checkpoint\n",
    "from blocks.extensions.monitoring import TrainingDataMonitoring\n",
    "from blocks.main_loop import MainLoop\n",
    "from blocks.bricks import WEIGHT\n",
    "from blocks.roles import INPUT\n",
    "from blocks.filter import VariableFilter\n",
    "from blocks.graph import apply_dropout\n",
    "from blocks.utils import named_copy, dict_union, shared_floatx_nans, shared_floatx_zeros, shared_floatx\n",
    "from blocks.utils import dict_union, shared_floatx_nans, shared_floatx_zeros, shared_floatx\n",
    "from blocks.bricks.recurrent import BaseRecurrent\n",
    "from blocks.bricks.wrappers import As2D\n",
    "from blocks.bricks.base import lazy\n",
    "from blocks.bricks.recurrent import recurrent\n",
    "from blocks.roles import add_role, INITIAL_STATE\n",
    "from blocks.extensions import SimpleExtension\n",
    "from blocks.bricks import MLP\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# SOME USEFUL STUFF FOR THEANO DEBUGGING\n",
    "\n",
    "config.recursion_limit = 100000\n",
    "floatX = theano.config.floatX\n",
    "logger = logging.getLogger(__name__)\n",
    "# this is to let the log print in the notebook\n",
    "logger.setLevel(logging.DEBUG)\n",
    "\n",
    "# theano debugging stuff\n",
    "theano.config.optimizer='fast_compile'\n",
    "# theano.config.optimizer='None'\n",
    "theano.config.exception_verbosity='high'\n",
    "\n",
    "# compute_test_value is 'off' by default, meaning this feature is inactive\n",
    "# theano.config.compute_test_value = 'off' # Use 'warn' to activate this feature\n",
    "# theano.config.compute_test_value = 'warn' # Use 'warn' to activate this feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# some filter and mapping functions for fuel to use\n",
    "def _transpose(data):\n",
    "    return tuple(array.T for array in data)\n",
    "\n",
    "# swap the (batch, time) axes to make the shape (time, batch, ...)\n",
    "def _swapaxes(data):\n",
    "    return tuple(array.swapaxes(0,1) for array in data)\n",
    "\n",
    "def _filter_long(data):\n",
    "    return len(data[0]) <= 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# this is how you could load a bigger slice of the corpus\n",
    "# from nltk.corpus import brown\n",
    "# words_by_line, tags_by_line = zip(*[zip(*sen) for sen in list(brown.tagged_sents())[:10000]])\n",
    "# for these examples, we need to make sure every seq is the same length to avoid padding/mask issues\n",
    "\n",
    "# a tiny toy dataset for learning the POS of the ambiguous word \"calls\"\n",
    "words_by_line = [['She', 'calls', 'me', 'every', 'day', '.'],  \n",
    "                 ['I', 'received', 'two', 'calls', 'yesterday', '.']] *10\n",
    "\n",
    "\n",
    "tags_by_line = [[u'PPS', u'VBZ', u'PPO', u'AT', u'NN', u'.'],\n",
    "                [u'PPSS', u'VBN', u'CD', u'NNS', u'NR', u'.']] *10\n",
    "\n",
    "idx2word = dict(enumerate(set([w for l in words_by_line for w in l])))\n",
    "word2idx = {v:k for k,v in idx2word.items()}\n",
    "\n",
    "idx2tag = dict(enumerate(set([t for l in tags_by_line for t in l])))\n",
    "tag2idx = {v:k for k,v in idx2tag.items()}\n",
    "\n",
    "iwords = [[word2idx[w] for w in l] for l in words_by_line]\n",
    "itags =  [[tag2idx[t] for t in l] for l in tags_by_line]\n",
    "\n",
    "# now create the fuel dataset\n",
    "qe_dataset = IndexableDataset(\n",
    "    indexables=OrderedDict([('words', iwords), ('tags', itags)]))\n",
    "\n",
    "# now we're going to progressively wrap data streams with other streams that transform the stream somehow\n",
    "qe_dataset.example_iteration_scheme = ShuffledExampleScheme(qe_dataset.num_examples)\n",
    "data_stream = qe_dataset.get_example_stream()\n",
    "data_stream = Batch(data_stream, iteration_scheme=ConstantScheme(1))\n",
    "\n",
    "# add padding and masks to the dataset\n",
    "# data_stream = Padding(data_stream, mask_sources=('words','tags'))\n",
    "data_stream = Padding(data_stream, mask_sources=('words'))\n",
    "data_stream = Mapping(data_stream, _swapaxes)\n",
    "\n",
    "# Example of how the iterator works\n",
    "# for batch in list(data_stream.get_epoch_iterator()):\n",
    "#     print([source.shape for source in batch])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tagset_size=len(tag2idx.keys())\n",
    "vocab_size=len(word2idx.keys())\n",
    "dimension=5\n",
    "\n",
    "class LookupRecurrent(BaseRecurrent, Initializable):\n",
    "    \"\"\"The recurrent transition with lookup and feedback \n",
    "\n",
    "    The most well-known recurrent transition: a matrix multiplication,\n",
    "    optionally followed by a non-linearity.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    dim : int\n",
    "        The dimension of the hidden state\n",
    "    activation : :class:`.Brick`\n",
    "        The brick to apply as activation.\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    See :class:`.Initializable` for initialization parameters.\n",
    "\n",
    "    \"\"\"\n",
    "    @lazy(allocation=['dim'])\n",
    "    def __init__(self, dim, activation, **kwargs):\n",
    "        super(LookupRecurrent, self).__init__(**kwargs)\n",
    "        self.dim = dim\n",
    "        \n",
    "        word_lookup = LookupTable(vocab_size, dimension)\n",
    "        word_lookup.weights_init = IsotropicGaussian(0.01)\n",
    "        word_lookup.initialize()\n",
    "        self.word_lookup = word_lookup\n",
    "\n",
    "        # There will be a Softmax on top of this layer\n",
    "        state_to_output = Linear(name='state_to_output', input_dim=dimension, output_dim=tagset_size)\n",
    "        state_to_output.weights_init = IsotropicGaussian(0.01)\n",
    "        state_to_output.biases_init = Constant(0.0)\n",
    "        state_to_output.initialize()\n",
    "        self.state_to_output = state_to_output\n",
    "\n",
    "        # note - As2D won't work with masks\n",
    "        nonlinearity = Softmax()\n",
    "        wrapper_2D = As2D(nonlinearity.apply)\n",
    "        wrapper_2D.initialize()\n",
    "        self.wrapper_2D = wrapper_2D\n",
    "\n",
    "        # the \"lookup\" brick -- aka the word embeddings\n",
    "        lookup = LookupFeedback(num_outputs=tagset_size, feedback_dim=dimension)\n",
    "        lookup.weights_init = IsotropicGaussian(0.1)\n",
    "        lookup.biases_init = Constant(0.0)\n",
    "        lookup.initialize()\n",
    "        self.lookup = lookup\n",
    "        \n",
    "        # a non-linear activation (i.e. Sigmoid, Tanh, ReLU, ...)\n",
    "        self.activation = activation\n",
    "        \n",
    "        self.children = [activation, state_to_output, nonlinearity, \n",
    "                         wrapper_2D, word_lookup, lookup]\n",
    "\n",
    "    @property\n",
    "    def W(self):\n",
    "        return self.parameters[0]\n",
    "\n",
    "    def get_dim(self, name):\n",
    "        if name == 'mask':\n",
    "            return 0\n",
    "        if name in (LookupRecurrent.apply.sequences +\n",
    "                    LookupRecurrent.apply.states):\n",
    "            return self.dim\n",
    "        return super(LookupRecurrent, self).get_dim(name)\n",
    "\n",
    "    # the initial state is the 'original' lookup+feedback\n",
    "    # the initial state is combined with the first input to produce the first output\n",
    "    def _allocate(self):\n",
    "        self.parameters.append(shared_floatx_nans((self.dim, self.dim),\n",
    "                                                  name=\"W\"))\n",
    "        add_role(self.parameters[0], WEIGHT)\n",
    "        \n",
    "        self.parameters.append(shared_floatx(np.random.random(self.dim,), name=\"initial_state\"))\n",
    "        add_role(self.parameters[1], INITIAL_STATE)\n",
    "       \n",
    "    def _initialize(self):\n",
    "        self.weights_init.initialize(self.W, self.rng)\n",
    "    \n",
    "    def get_predictions(self, inputs):\n",
    "        linear_mapping = self.state_to_output.apply(inputs)\n",
    "        readouts = self.wrapper_2D.apply(linear_mapping)\n",
    "        return readouts\n",
    "    \n",
    "    # TODO: change inputs-->states or something more clear\n",
    "    def get_feedback(self, inputs):\n",
    "        linear_mapping = self.state_to_output.apply(inputs)\n",
    "        readouts = self.wrapper_2D.apply(linear_mapping)\n",
    "        predictions = readouts.argmax(axis=1)\n",
    "        return self.lookup.feedback(predictions)\n",
    "    \n",
    "    @recurrent(sequences=['inputs', 'mask'], states=['states'], outputs=['states'], contexts=[])\n",
    "    def apply(self, inputs=None, states=None, mask=None):\n",
    "        \"\"\"Apply the transition.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        inputs : :class:`~tensor.TensorVariable`\n",
    "            The 2D inputs, in the shape (batch, features).\n",
    "        states : :class:`~tensor.TensorVariable`\n",
    "            The 2D states, in the shape (batch, features).\n",
    "        mask : :class:`~tensor.TensorVariable`\n",
    "            A 1D binary array in the shape (batch,) which is 1 if\n",
    "            there is data available, 0 if not. Assumed to be 1-s\n",
    "            only if not given.\n",
    "\n",
    "        \"\"\"\n",
    "        # first compute the current representation (_not_ state) via the standard recurrent transition\n",
    "        current_representation = self.word_lookup.apply(inputs) + tensor.dot(states, self.W)\n",
    "        \n",
    "        # another nonlinearity?\n",
    "        next_states = self.children[0].apply(current_representation)\n",
    "        \n",
    "        if mask:\n",
    "            next_states = (mask[:, None] * next_states +\n",
    "                           (1 - mask[:, None]) * states)\n",
    "        # Try to return multiple things\n",
    "        return next_states\n",
    "\n",
    "    # trainable initial state\n",
    "    @application(outputs=apply.states)\n",
    "    def initial_states(self, batch_size, *args, **kwargs):\n",
    "        return tensor.repeat(self.parameters[1][None, :], batch_size, 0)\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# test applying our transition to a batch, and see what we get back\n",
    "transition = LookupRecurrent(dim=dimension, activation=Tanh())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# this is the cost function that we'll use to train our model\n",
    "def get_cost(words,words_mask,targets):\n",
    "\n",
    "#     comment this out if you are using the GatedRecurrent transition\n",
    "    states = transition.apply(\n",
    "        **dict_union(inputs=words, mask=words_mask, return_initial_states=True))\n",
    "    \n",
    "    output = states[1:]\n",
    "    output_shape = output.shape\n",
    "\n",
    "    dim1 = output_shape[0] * output_shape[1]\n",
    "    dim2 = output_shape[2]\n",
    "  \n",
    "    y_hat = Softmax().apply(\n",
    "        transition.state_to_output.apply(\n",
    "        output.reshape((dim1, dim2))))\n",
    "    \n",
    "    # try the blocks crossentropy\n",
    "    y = targets.flatten()\n",
    "    costs = theano.tensor.nnet.categorical_crossentropy(y_hat,y)\n",
    "    \n",
    "    final_cost = costs.mean()\n",
    "    \n",
    "#     return final_cost\n",
    "    return (final_cost, y_hat, y, costs, final_cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_prediction(words, words_mask):\n",
    "    \n",
    "    states = transition.apply(\n",
    "        **dict_union(inputs=words, mask=words_mask, return_initial_states=True))\n",
    "    \n",
    "    # we only care about the RNN states, which are the first and only output\n",
    "    output = states[1:]\n",
    "    output_shape = output.shape\n",
    "    dim1 = output_shape[0] * output_shape[1]\n",
    "    dim2 = output_shape[2]\n",
    "    \n",
    "    y_hat = Softmax().apply(\n",
    "        transition.state_to_output.apply(\n",
    "            output.reshape((dim1, dim2))))\n",
    "\n",
    "    predictions = y_hat\n",
    "\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chris/programs/anaconda/lib/python2.7/site-packages/theano/scan_module/scan_perform_ext.py:135: RuntimeWarning: numpy.ndarray size changed, may indicate binary incompatibility\n",
      "  from scan_perform.scan_perform import *\n"
     ]
    }
   ],
   "source": [
    "words=tensor.lmatrix(\"words\")\n",
    "words_mask=tensor.matrix(\"words_mask\")\n",
    "targets=tensor.lmatrix(\"tags\")\n",
    "# targets_mask=tensor.matrix(\"tags_mask\")\n",
    "\n",
    "# let's get some feedback from the cost function so we can monitor it\n",
    "cost, yhat, y, raw_costs, true_costs = get_cost(words, words_mask, targets)\n",
    "\n",
    "yhat.name = 'yyhat'\n",
    "y.name = 'y_inside'\n",
    "raw_costs.name = 'raw_costs'\n",
    "true_costs.name = 'true_costs'\n",
    "# mlp_cost.name = 'mlp_cost'\n",
    "\n",
    " \n",
    "# can we just get ther computation graph directly here? (without Model)\n",
    "cost_cg = ComputationGraph(cost)\n",
    "weights = VariableFilter(roles=[WEIGHT])(cost_cg.variables)\n",
    "\n",
    "cost.name = \"sequence_log_likelihood_cost_regularized\"\n",
    "prediction_model = Model(get_prediction(words, words_mask)).get_theano_function()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Parameters:\n",
      "[('/lookuprecurrent/state_to_output.b', (11,)),\n",
      " ('/lookuprecurrent.initial_state', (5,)),\n",
      " ('/lookuprecurrent/lookuptable.W', (10, 5)),\n",
      " ('/lookuprecurrent.W', (5, 5)),\n",
      " ('/lookuprecurrent/state_to_output.W', (5, 11))]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-------------------------------------------------------------------------------\n",
      "BEFORE FIRST EPOCH\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 0\n",
      "\t iterations_done: 0\n",
      "\t received_first_batch: False\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 0:\n",
      "\t time_initialization: 0.422866821289\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 0\n",
      "\t iterations_done: 10\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 10:\n",
      "\t sequence_log_likelihood_cost_regularized: 2.40128183365\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 0\n",
      "\t iterations_done: 20\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 20:\n",
      "\t sequence_log_likelihood_cost_regularized: 2.32834124565\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 1\n",
      "\t iterations_done: 30\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 30:\n",
      "\t sequence_log_likelihood_cost_regularized: 2.22969889641\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 1\n",
      "\t iterations_done: 40\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 40:\n",
      "\t sequence_log_likelihood_cost_regularized: 2.0998737812\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 2\n",
      "\t iterations_done: 50\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 50:\n",
      "\t sequence_log_likelihood_cost_regularized: 1.76355731487\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 2\n",
      "\t iterations_done: 60\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 60:\n",
      "\t sequence_log_likelihood_cost_regularized: 1.57644081116\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 3\n",
      "\t iterations_done: 70\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 70:\n",
      "\t sequence_log_likelihood_cost_regularized: 1.31076157093\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 3\n",
      "\t iterations_done: 80\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 80:\n",
      "\t sequence_log_likelihood_cost_regularized: 1.18034923077\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 4\n",
      "\t iterations_done: 90\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 90:\n",
      "\t sequence_log_likelihood_cost_regularized: 0.884596347809\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 4\n",
      "\t iterations_done: 100\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 100:\n",
      "\t sequence_log_likelihood_cost_regularized: 0.946525514126\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 5\n",
      "\t iterations_done: 110\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 110:\n",
      "\t sequence_log_likelihood_cost_regularized: 0.794351518154\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 5\n",
      "\t iterations_done: 120\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 120:\n",
      "\t sequence_log_likelihood_cost_regularized: 0.771010220051\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 6\n",
      "\t iterations_done: 130\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 130:\n",
      "\t sequence_log_likelihood_cost_regularized: 0.511634707451\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 6\n",
      "\t iterations_done: 140\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 140:\n",
      "\t sequence_log_likelihood_cost_regularized: 0.504561424255\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 7\n",
      "\t iterations_done: 150\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 150:\n",
      "\t sequence_log_likelihood_cost_regularized: 0.49036321044\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 7\n",
      "\t iterations_done: 160\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 160:\n",
      "\t sequence_log_likelihood_cost_regularized: 0.396243780851\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 8\n",
      "\t iterations_done: 170\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 170:\n",
      "\t sequence_log_likelihood_cost_regularized: 0.362914890051\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 8\n",
      "\t iterations_done: 180\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 180:\n",
      "\t sequence_log_likelihood_cost_regularized: 0.345722287893\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 9\n",
      "\t iterations_done: 190\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 190:\n",
      "\t sequence_log_likelihood_cost_regularized: 0.336530297995\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 9\n",
      "\t iterations_done: 200\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 200:\n",
      "\t sequence_log_likelihood_cost_regularized: 0.379827827215\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 10\n",
      "\t iterations_done: 210\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 210:\n",
      "\t sequence_log_likelihood_cost_regularized: 0.351217746735\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 10\n",
      "\t iterations_done: 220\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 220:\n",
      "\t sequence_log_likelihood_cost_regularized: 0.325640022755\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 11\n",
      "\t iterations_done: 230\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 230:\n",
      "\t sequence_log_likelihood_cost_regularized: 0.274875253439\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 11\n",
      "\t iterations_done: 240\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 240:\n",
      "\t sequence_log_likelihood_cost_regularized: 0.311262458563\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 12\n",
      "\t iterations_done: 250\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 250:\n",
      "\t sequence_log_likelihood_cost_regularized: 0.203902676702\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 12\n",
      "\t iterations_done: 260\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 260:\n",
      "\t sequence_log_likelihood_cost_regularized: 0.265695929527\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 13\n",
      "\t iterations_done: 270\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 270:\n",
      "\t sequence_log_likelihood_cost_regularized: 0.239446997643\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 13\n",
      "\t iterations_done: 280\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 280:\n",
      "\t sequence_log_likelihood_cost_regularized: 0.259189337492\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 14\n",
      "\t iterations_done: 290\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 290:\n",
      "\t sequence_log_likelihood_cost_regularized: 0.201801836491\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 14\n",
      "\t iterations_done: 300\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 300:\n",
      "\t sequence_log_likelihood_cost_regularized: 0.221340581775\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 15\n",
      "\t iterations_done: 310\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 310:\n",
      "\t sequence_log_likelihood_cost_regularized: 0.256435155869\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 15\n",
      "\t iterations_done: 320\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 320:\n",
      "\t sequence_log_likelihood_cost_regularized: 0.224238753319\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 16\n",
      "\t iterations_done: 330\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 330:\n",
      "\t sequence_log_likelihood_cost_regularized: 0.203753814101\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 16\n",
      "\t iterations_done: 340\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 340:\n",
      "\t sequence_log_likelihood_cost_regularized: 0.204461112618\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 17\n",
      "\t iterations_done: 350\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 350:\n",
      "\t sequence_log_likelihood_cost_regularized: 0.213911652565\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 17\n",
      "\t iterations_done: 360\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 360:\n",
      "\t sequence_log_likelihood_cost_regularized: 0.18956874311\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 18\n",
      "\t iterations_done: 370\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 370:\n",
      "\t sequence_log_likelihood_cost_regularized: 0.248442232609\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 18\n",
      "\t iterations_done: 380\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 380:\n",
      "\t sequence_log_likelihood_cost_regularized: 0.200267896056\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 19\n",
      "\t iterations_done: 390\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 390:\n",
      "\t sequence_log_likelihood_cost_regularized: 0.208173498511\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 19\n",
      "\t iterations_done: 400\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 400:\n",
      "\t sequence_log_likelihood_cost_regularized: 0.199688389897\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 20\n",
      "\t iterations_done: 410\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 410:\n",
      "\t sequence_log_likelihood_cost_regularized: 0.19158475101\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 20\n",
      "\t iterations_done: 420\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 420:\n",
      "\t sequence_log_likelihood_cost_regularized: 0.189088121057\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 21\n",
      "\t iterations_done: 430\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 430:\n",
      "\t sequence_log_likelihood_cost_regularized: 0.195689156651\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 21\n",
      "\t iterations_done: 440\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 440:\n",
      "\t sequence_log_likelihood_cost_regularized: 0.190213307738\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 22\n",
      "\t iterations_done: 450\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 450:\n",
      "\t sequence_log_likelihood_cost_regularized: 0.181784585118\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 22\n",
      "\t iterations_done: 460\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 460:\n",
      "\t sequence_log_likelihood_cost_regularized: 0.165978685021\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 23\n",
      "\t iterations_done: 470\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 470:\n",
      "\t sequence_log_likelihood_cost_regularized: 0.202049434185\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 23\n",
      "\t iterations_done: 480\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 480:\n",
      "\t sequence_log_likelihood_cost_regularized: 0.168178483844\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 24\n",
      "\t iterations_done: 490\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 490:\n",
      "\t sequence_log_likelihood_cost_regularized: 0.153016418219\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 24\n",
      "\t iterations_done: 500\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 500:\n",
      "\t sequence_log_likelihood_cost_regularized: 0.16180999577\n",
      "\t training_finish_requested: True\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "TRAINING HAS BEEN FINISHED:\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 24\n",
      "\t iterations_done: 500\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 500:\n",
      "\t sequence_log_likelihood_cost_regularized: 0.16180999577\n",
      "\t training_finish_requested: True\n",
      "\t training_finished: True\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Construct the main loop and start training!\n",
    "# hyperparams from user input\n",
    "# what is the right number of training batches?\n",
    "\n",
    "transition.weights_init = IsotropicGaussian(0.1)\n",
    "transition.biases_init = Constant(0.)\n",
    "transition.initialize()\n",
    "\n",
    "num_batches=500\n",
    "\n",
    "from blocks.monitoring import aggregation\n",
    "\n",
    "batch_cost = cost\n",
    "final_cost = aggregation.mean(batch_cost, 1)\n",
    "final_cost.name = 'final_cost'\n",
    "test_model = Model(final_cost)\n",
    "\n",
    "cg = ComputationGraph(final_cost)\n",
    "\n",
    "# note that you must explicitly provide the cost function `cost=...`\n",
    "algorithm = GradientDescent(\n",
    "    cost=final_cost, parameters=cg.parameters,\n",
    "    step_rule=AdaDelta()) \n",
    "\n",
    "#     CompositeRule([StepClipping(10.0), Scale(0.01)]))   \n",
    "\n",
    "#     CompositeRule([StepClipping(10.0), Adam()])   \n",
    "#     step_rule=AdaDelta())\n",
    "# step_rule=Scale(learning_rate=1e-3)\n",
    "#     step_rule=AdaDelta())    \n",
    "#     step_rule=CompositeRule([StepClipping(10.0), Scale(0.01)]))\n",
    "\n",
    "parameters = test_model.get_parameter_dict()\n",
    "logger.info(\"Parameters:\\n\" +\n",
    "                pprint.pformat(\n",
    "                    [(key, value.get_value().shape) for key, value in parameters.items()],\n",
    "                    width=120))\n",
    "\n",
    "observables = [cost]\n",
    "#     algorithm.total_step_norm, algorithm.total_gradient_norm]\n",
    "\n",
    "# for name, parameter in parameters.items():\n",
    "#     observables.append(named_copy(\n",
    "#         parameter.norm(2), name + \"_norm\"))\n",
    "#     observables.append(named_copy(\n",
    "#         algorithm.gradients[parameter].norm(2), name + \"_grad_norm\"))\n",
    "\n",
    "# this will be the prefix of the saved model and log\n",
    "save_path='/home/chris/projects/neural_qe/proto/test_output/tmp/test-lookup-recurrent-model'\n",
    "\n",
    "average_monitoring = TrainingDataMonitoring(\n",
    "    observables, prefix=\"average\", every_n_batches=1000)\n",
    "\n",
    "main_loop = MainLoop(\n",
    "    model=test_model,\n",
    "    data_stream=data_stream,\n",
    "    algorithm=algorithm,\n",
    "    extensions=[\n",
    "        Timing(),\n",
    "        TrainingDataMonitoring(observables, after_batch=True),\n",
    "        average_monitoring,\n",
    "        FinishAfter(after_n_batches=num_batches),\n",
    "        # This shows a way to handle NaN emerging during\n",
    "        # training: simply finish it.\n",
    "#         .add_condition([\"after_batch\"], _is_nan),\n",
    "        # Saving the model and the log separately is convenient,\n",
    "        # because loading the whole pickle takes quite some time.\n",
    "#         Checkpoint(save_path, every_n_batches=1000,\n",
    "#                    save_separately=[\"model\", \"log\"]),\n",
    "        Printing(every_n_batches=10, after_epoch=False)])\n",
    "main_loop.run()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'PPS', u'VBZ', u'PPO', u'NNS']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use this to test some examples to get an idea what your model learned\n",
    "new_ex = [['She', 'calls', 'two', 'calls']]\n",
    "# new_ex = [['She', 'calls', 'me', 'every', 'day']]\n",
    "new_ex_int = [[word2idx[w] for w in l] for l in new_ex]\n",
    "example = np.array(new_ex_int).swapaxes(0,1)\n",
    "o = prediction_model(example, np.ones(example.shape).astype(theano.config.floatX))[0]\n",
    "predictions = [idx2tag[i] for i in o.argmax(axis=1)]\n",
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
