{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# from __future__ import division, print_function\n",
    "import codecs\n",
    "import re\n",
    "import json\n",
    "import random\n",
    "import math\n",
    "import os\n",
    "import gzip\n",
    "import cPickle\n",
    "from collections import Counter, defaultdict\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pylab\n",
    "import pandas as pd\n",
    "from scipy.stats import norm\n",
    "import nltk\n",
    "from fuel.datasets import H5PYDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load fuel dataset (POS windows) DONE\n",
    "# Load one-hot Word X Document numpy matrix DONE\n",
    "\n",
    "# train autoencoder, stop when reconstruction performance doesn't improve any more, or after a fixed number of epochs\n",
    "\n",
    "# CHALLENGE: replace denoising with dropout\n",
    "\n",
    "# save the parameters of the first part of the network (the encoder)\n",
    "# use the encoder parameters to build the prediction transformation\n",
    "\n",
    "# evaluate the performance of the autoencoder features vs. hstacked LSI and W2V features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# now create the fuel dataset\n",
    "DATASET_LOCATION = 'datasets/'\n",
    "\n",
    "# the pos dataset consists of windows around words\n",
    "POS_DATASET_NAME = 'brown_pos_dataset.hdf5'\n",
    "POS_DATASET_PATH = os.path.join(DATASET_LOCATION, POS_DATASET_NAME)\n",
    "\n",
    "CORPUS_INDICES = 'brown_pos_dataset.indices'\n",
    "WORD_BY_DOC_MATRIX = \"brown.word-by-doc.binary.npy\"\n",
    "\n",
    "# Indexes for mapping words <--> ints\n",
    "with open(os.path.join(DATASET_LOCATION, CORPUS_INDICES)) as indices_file:\n",
    "    corpus_indices = cPickle.load(indices_file)\n",
    "    \n",
    "# Word X Doc binary matrix\n",
    "word_df = np.load(os.path.join(DATASET_LOCATION, WORD_BY_DOC_MATRIX))\n",
    "\n",
    "\n",
    "train_X, train_y = H5PYDataset(\n",
    "    POS_DATASET_PATH, which_sets=('train',),\n",
    "    sources=['instances', 'targets'], load_in_memory=True).data_sources\n",
    "\n",
    "dev_X, dev_y = H5PYDataset(\n",
    "    POS_DATASET_PATH, which_sets=('dev',),\n",
    "    sources=['instances', 'targets'], load_in_memory=True).data_sources\n",
    "\n",
    "test_X, test_y = H5PYDataset(\n",
    "    POS_DATASET_PATH, which_sets=('test',),\n",
    "    sources=['instances', 'targets'], load_in_memory=True).data_sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_df[word_df.nonzero()].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# let's persist our word tf matrix and see how big it is -- with shape = (9769, 15667) its 292Mb\n",
    "# WORD_TF=\"brown.word-tf.npy\"\n",
    "# with open(os.path.join(DATASET_LOCATION, WORD_TF), \"wb\") as outfile:\n",
    "#     np.save(outfile, brown_word_tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create a one-layer autoencoder to encode word windows into 100-dim vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ \\tilde{x} ~ q_D(\\tilde{x}|x) $\n",
    "\n",
    "$ y = s(W \\tilde{x} + b) $\n",
    "\n",
    "$ x = s(W' y  + b') $\n",
    "\n",
    "$ L(x,z) = -sum_{k=1}^d [x_k \\log z_k + (1-x_k) \\log( 1-z_k)] $\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_hstacked_vectors(index_path, seqs, cutoff=None):\n",
    "    # load the index, map the seqs (word indices) through the index, hstack, then cast the results to np.array\n",
    "    index = np.load(index_path)\n",
    "    \n",
    "    if cutoff is None:\n",
    "        cutoff = len(seqs)\n",
    "    \n",
    "    working_index = np.load(index_path)\n",
    "    return np.array([np.hstack([index[idx] for idx in seq]) for seq in seqs[:cutoff]])\n",
    "\n",
    "\n",
    "# use the raw word-df matrix to construct the hstacked vectors \n",
    "# Note that this function could return a HUGE object\n",
    "# def windows_to_array(window_corpus, index, cutoff=None):\n",
    "\n",
    "# convert the windows to hstacked vectors\n",
    "# training_vectors = windows_to_array(train_X, working_index)\n",
    "# dev_vectors = windows_to_array(dev_X, working_index)\n",
    "# test_vectors = windows_to_array(test_X, working_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load a portion of a fuel dataset\n",
    "\n",
    "def load_fuel_dataset(filename, which_sets=None):\n",
    "#     if which_sets is not None return all sets\n",
    "\n",
    "#     the problem here is that we're getting the int ids but we actually need the hstacked vectors for each instance\n",
    "    X, y = H5PYDataset(\n",
    "        filename, which_sets=which_sets,\n",
    "        sources=['instances', 'targets'], load_in_memory=True).data_sources\n",
    "    \n",
    "    # make sure y is 0-dimensional\n",
    "    y = y.ravel()\n",
    "    \n",
    "    #train_set, valid_set, test_set format: tuple(input, target)\n",
    "    #input is an numpy.ndarray of 2 dimensions (a matrix)\n",
    "    #where rows correspond to an example. target is a\n",
    "    #numpy.ndarray of 1 dimensions (vector)) that have the same length as\n",
    "    #the number of rows in the input. It should give the target\n",
    "    #target to the example with the same index in the input.\n",
    "\n",
    "    def shared_dataset(X, y, borrow=True):\n",
    "        \"\"\" Function that loads the dataset into shared variables\n",
    "\n",
    "        The reason we store our dataset in shared variables is to allow\n",
    "        Theano to copy it into the GPU memory (when code is run on GPU).\n",
    "        Since copying data into the GPU is slow, copying a minibatch everytime\n",
    "        is needed (the default behaviour if the data is not in a shared\n",
    "        variable) would lead to a large decrease in performance.\n",
    "        \"\"\"\n",
    "        \n",
    "        shared_x = theano.shared(numpy.asarray(X,\n",
    "                                               dtype=theano.config.floatX), borrow=borrow)\n",
    "        shared_y = theano.shared(numpy.asarray(y,\n",
    "                                               dtype=theano.config.floatX), borrow=borrow)\n",
    "        # When storing data on the GPU it has to be stored as floats\n",
    "        # therefore we will store the labels as ``floatX`` as well\n",
    "        # (``shared_y`` does exactly that). But during our computations\n",
    "        # we need them as ints (we use labels as index, and if they are\n",
    "        # floats it doesn't make sense) therefore instead of returning\n",
    "        # ``shared_y`` we will have to cast it to int. This little hack\n",
    "        # lets us get around this issue\n",
    "        return shared_x, T.cast(shared_y, 'int32')\n",
    "    \n",
    "    \n",
    "    # HACK - map dataset_X through word X doc index, then create the shared datasets\n",
    "    VECTOR_INDEX_PATH = os.path.join(DATASET_LOCATION, WORD_BY_DOC_MATRIX)\n",
    "    CUTOFF = 1000\n",
    "    X = create_hstacked_vectors(VECTOR_INDEX_PATH, X, cutoff=CUTOFF)\n",
    "    \n",
    "    dataset_X, dataset_y = shared_dataset(X, y)\n",
    "    \n",
    "    return (dataset_X, dataset_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    " This tutorial introduces denoising auto-encoders (dA) using Theano.\n",
    "\n",
    " Denoising autoencoders are the building blocks for SdA.\n",
    " They are based on auto-encoders as the ones used in Bengio et al. 2007.\n",
    " An autoencoder takes an input x and first maps it to a hidden representation\n",
    " y = f_{\\theta}(x) = s(Wx+b), parameterized by \\theta={W,b}. The resulting\n",
    " latent representation y is then mapped back to a \"reconstructed\" vector\n",
    " z \\in [0,1]^d in input space z = g_{\\theta'}(y) = s(W'y + b').  The weight\n",
    " matrix W' can optionally be constrained such that W' = W^T, in which case\n",
    " the autoencoder is said to have tied weights. The network is trained such\n",
    " that to minimize the reconstruction error (the error between x and z).\n",
    "\n",
    " For the denosing autoencoder, during training, first x is corrupted into\n",
    " \\tilde{x}, where \\tilde{x} is a partially destroyed version of x by means\n",
    " of a stochastic mapping. Afterwards y is computed as before (using\n",
    " \\tilde{x}), y = s(W\\tilde{x} + b) and z as s(W'y + b'). The reconstruction\n",
    " error is now measured between z and the uncorrupted input x, which is\n",
    " computed as the cross-entropy :\n",
    "      - \\sum_{k=1}^d[ x_k \\log z_k + (1-x_k) \\log( 1-z_k)]\n",
    "\n",
    "\n",
    " References :\n",
    "   - P. Vincent, H. Larochelle, Y. Bengio, P.A. Manzagol: Extracting and\n",
    "   Composing Robust Features with Denoising Autoencoders, ICML'08, 1096-1103,\n",
    "   2008\n",
    "   - Y. Bengio, P. Lamblin, D. Popovici, H. Larochelle: Greedy Layer-Wise\n",
    "   Training of Deep Networks, Advances in Neural Information Processing\n",
    "   Systems 19, 2007\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import timeit\n",
    "\n",
    "import numpy\n",
    "\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "from theano.tensor.shared_randomstreams import RandomStreams\n",
    "\n",
    "# from logistic_sgd import load_data\n",
    "from utils import tile_raster_images\n",
    "\n",
    "try:\n",
    "    import PIL.Image as Image\n",
    "except ImportError:\n",
    "    import Image\n",
    "\n",
    "\n",
    "class dA(object):\n",
    "    \"\"\"Denoising Auto-Encoder class (dA)\n",
    "\n",
    "    A denoising autoencoders tries to reconstruct the input from a corrupted\n",
    "    version of it by projecting it first in a latent space and reprojecting\n",
    "    it afterwards back in the input space. Please refer to Vincent et al.,2008\n",
    "    for more details. If x is the input then equation (1) computes a partially\n",
    "    destroyed version of x by means of a stochastic mapping q_D. Equation (2)\n",
    "    computes the projection of the input into the latent space. Equation (3)\n",
    "    computes the reconstruction of the input, while equation (4) computes the\n",
    "    reconstruction error.\n",
    "\n",
    "    .. math::\n",
    "\n",
    "        \\tilde{x} ~ q_D(\\tilde{x}|x)                                     (1)\n",
    "\n",
    "        y = s(W \\tilde{x} + b)                                           (2)\n",
    "\n",
    "        x = s(W' y  + b')                                                (3)\n",
    "\n",
    "        L(x,z) = -sum_{k=1}^d [x_k \\log z_k + (1-x_k) \\log( 1-z_k)]      (4)\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        numpy_rng,\n",
    "        theano_rng=None,\n",
    "        input=None,\n",
    "        n_visible=784,\n",
    "        n_hidden=500,\n",
    "        W=None,\n",
    "        bhid=None,\n",
    "        bvis=None\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initialize the dA class by specifying the number of visible units (the\n",
    "        dimension d of the input ), the number of hidden units ( the dimension\n",
    "        d' of the latent or hidden space ) and the corruption level. The\n",
    "        constructor also receives symbolic variables for the input, weights and\n",
    "        bias. Such a symbolic variables are useful when, for example the input\n",
    "        is the result of some computations, or when weights are shared between\n",
    "        the dA and an MLP layer. When dealing with SdAs this always happens,\n",
    "        the dA on layer 2 gets as input the output of the dA on layer 1,\n",
    "        and the weights of the dA are used in the second stage of training\n",
    "        to construct an MLP.\n",
    "\n",
    "        :type numpy_rng: numpy.random.RandomState\n",
    "        :param numpy_rng: number random generator used to generate weights\n",
    "\n",
    "        :type theano_rng: theano.tensor.shared_randomstreams.RandomStreams\n",
    "        :param theano_rng: Theano random generator; if None is given one is\n",
    "                     generated based on a seed drawn from `rng`\n",
    "\n",
    "        :type input: theano.tensor.TensorType\n",
    "        :param input: a symbolic description of the input or None for\n",
    "                      standalone dA\n",
    "\n",
    "        :type n_visible: int\n",
    "        :param n_visible: number of visible units\n",
    "\n",
    "        :type n_hidden: int\n",
    "        :param n_hidden:  number of hidden units\n",
    "\n",
    "        :type W: theano.tensor.TensorType\n",
    "        :param W: Theano variable pointing to a set of weights that should be\n",
    "                  shared belong the dA and another architecture; if dA should\n",
    "                  be standalone set this to None\n",
    "\n",
    "        :type bhid: theano.tensor.TensorType\n",
    "        :param bhid: Theano variable pointing to a set of biases values (for\n",
    "                     hidden units) that should be shared belong dA and another\n",
    "                     architecture; if dA should be standalone set this to None\n",
    "\n",
    "        :type bvis: theano.tensor.TensorType\n",
    "        :param bvis: Theano variable pointing to a set of biases values (for\n",
    "                     visible units) that should be shared belong dA and another\n",
    "                     architecture; if dA should be standalone set this to None\n",
    "\n",
    "\n",
    "        \"\"\"\n",
    "        self.n_visible = n_visible\n",
    "        self.n_hidden = n_hidden\n",
    "\n",
    "        # create a Theano random generator that gives symbolic random values\n",
    "        if not theano_rng:\n",
    "            theano_rng = RandomStreams(numpy_rng.randint(2 ** 30))\n",
    "\n",
    "        # note : W' was written as `W_prime` and b' as `b_prime`\n",
    "        if not W:\n",
    "            # W is initialized with `initial_W` which is uniformly sampled\n",
    "            # from -4*sqrt(6./(n_visible+n_hidden)) and\n",
    "            # 4*sqrt(6./(n_hidden+n_visible))the output of uniform if\n",
    "            # converted using asarray to dtype\n",
    "            # theano.config.floatX so that the code is runable on GPU\n",
    "            initial_W = numpy.asarray(\n",
    "                numpy_rng.uniform(\n",
    "                    low=-4 * numpy.sqrt(6. / (n_hidden + n_visible)),\n",
    "                    high=4 * numpy.sqrt(6. / (n_hidden + n_visible)),\n",
    "                    size=(n_visible, n_hidden)\n",
    "                ),\n",
    "                dtype=theano.config.floatX\n",
    "            )\n",
    "            W = theano.shared(value=initial_W, name='W', borrow=True)\n",
    "\n",
    "        if not bvis:\n",
    "            bvis = theano.shared(\n",
    "                value=numpy.zeros(\n",
    "                    n_visible,\n",
    "                    dtype=theano.config.floatX\n",
    "                ),\n",
    "                borrow=True\n",
    "            )\n",
    "\n",
    "        if not bhid:\n",
    "            bhid = theano.shared(\n",
    "                value=numpy.zeros(\n",
    "                    n_hidden,\n",
    "                    dtype=theano.config.floatX\n",
    "                ),\n",
    "                name='b',\n",
    "                borrow=True\n",
    "            )\n",
    "\n",
    "        self.W = W\n",
    "        # b corresponds to the bias of the hidden\n",
    "        self.b = bhid\n",
    "        # b_prime corresponds to the bias of the visible\n",
    "        self.b_prime = bvis\n",
    "        # Autoencoder with tied weights, therefore W_prime is W transpose\n",
    "        self.W_prime = self.W.T\n",
    "        self.theano_rng = theano_rng\n",
    "        # if no input is given, generate a variable representing the input\n",
    "        if input is None:\n",
    "            # we use a matrix because we expect a minibatch of several\n",
    "            # examples, each example being a row\n",
    "            self.x = T.dmatrix(name='input')\n",
    "        else:\n",
    "            self.x = input\n",
    "\n",
    "        self.params = [self.W, self.b, self.b_prime]\n",
    "\n",
    "    def get_corrupted_input(self, input, corruption_level):\n",
    "        \"\"\"This function keeps ``1-corruption_level`` entries of the inputs the\n",
    "        same and zero-out randomly selected subset of size ``coruption_level``\n",
    "        Note : first argument of theano.rng.binomial is the shape(size) of\n",
    "               random numbers that it should produce\n",
    "               second argument is the number of trials\n",
    "               third argument is the probability of success of any trial\n",
    "\n",
    "                this will produce an array of 0s and 1s where 1 has a\n",
    "                probability of 1 - ``corruption_level`` and 0 with\n",
    "                ``corruption_level``\n",
    "\n",
    "                The binomial function return int64 data type by\n",
    "                default.  int64 multiplicated by the input\n",
    "                type(floatX) always return float64.  To keep all data\n",
    "                in floatX when floatX is float32, we set the dtype of\n",
    "                the binomial to floatX. As in our case the value of\n",
    "                the binomial is always 0 or 1, this don't change the\n",
    "                result. This is needed to allow the gpu to work\n",
    "                correctly as it only support float32 for now.\n",
    "\n",
    "        \"\"\"\n",
    "        return self.theano_rng.binomial(size=input.shape, n=1,\n",
    "                                        p=1 - corruption_level,\n",
    "                                        dtype=theano.config.floatX) * input\n",
    "\n",
    "    def get_hidden_values(self, input):\n",
    "        \"\"\" Computes the values of the hidden layer \"\"\"\n",
    "        return T.nnet.sigmoid(T.dot(input, self.W) + self.b)\n",
    "\n",
    "    def get_reconstructed_input(self, hidden):\n",
    "        \"\"\"Computes the reconstructed input given the values of the\n",
    "        hidden layer\n",
    "\n",
    "        \"\"\"\n",
    "        return T.nnet.sigmoid(T.dot(hidden, self.W_prime) + self.b_prime)\n",
    "\n",
    "    def get_cost_updates(self, corruption_level, learning_rate):\n",
    "        \"\"\" This function computes the cost and the updates for one trainng\n",
    "        step of the dA \"\"\"\n",
    "\n",
    "        tilde_x = self.get_corrupted_input(self.x, corruption_level)\n",
    "        y = self.get_hidden_values(tilde_x)\n",
    "        z = self.get_reconstructed_input(y)\n",
    "        # note : we sum over the size of a datapoint; if we are using\n",
    "        #        minibatches, L will be a vector, with one entry per\n",
    "        #        example in minibatch\n",
    "        L = - T.sum(self.x * T.log(z) + (1 - self.x) * T.log(1 - z), axis=1)\n",
    "        # note : L is now a vector, where each element is the\n",
    "        #        cross-entropy cost of the reconstruction of the\n",
    "        #        corresponding example of the minibatch. We need to\n",
    "        #        compute the average of all these to get the cost of\n",
    "        #        the minibatch\n",
    "        cost = T.mean(L)\n",
    "\n",
    "        # compute the gradients of the cost of the `dA` with respect\n",
    "        # to its parameters\n",
    "        gparams = T.grad(cost, self.params)\n",
    "        # generate the list of updates\n",
    "        updates = [\n",
    "            (param, param - learning_rate * gparam)\n",
    "            for param, gparam in zip(self.params, gparams)\n",
    "        ]\n",
    "\n",
    "        return (cost, updates)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# def test_dA(learning_rate=0.1, training_epochs=15,\n",
    "#             dataset='mnist.pkl.gz',\n",
    "#             batch_size=20, output_folder='dA_plots'):\n",
    "    \n",
    "def train_dA(learning_rate=0.1, training_epochs=15,\n",
    "            dataset=None,\n",
    "            batch_size=20, output_folder='dA_plots'):\n",
    "\n",
    "    \"\"\"\n",
    "    This demo is tested on MNIST\n",
    "\n",
    "    :type learning_rate: float\n",
    "    :param learning_rate: learning rate used for training the DeNosing\n",
    "                          AutoEncoder\n",
    "\n",
    "    :type training_epochs: int\n",
    "    :param training_epochs: number of epochs used for training\n",
    "\n",
    "    :type dataset: string\n",
    "    :param dataset: path to the picked dataset\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "#     datasets = load_data(dataset)\n",
    "    # TODO map train set into hstacked vectors using the word X doc binary matrix \n",
    "    # This means additionally loading the Word X Doc matrix\n",
    "    # WORKING -- move training set initialization outside of this function\n",
    "    \n",
    "    # map through the vector index, then hstack the windows\n",
    "    \n",
    "    # TODO: split out the shared tensor variable creation, because we want to share the hstacked vectors\n",
    "    # NOT the int windows   \n",
    "    \n",
    "#     train_set_x, train_set_y = load_fuel_dataset(dataset, which_sets=('train',))\n",
    "    train_set_x, train_set_y = dataset\n",
    "    \n",
    "    # compute number of minibatches for training, validation and testing\n",
    "    n_train_batches = train_set_x.get_value(borrow=True).shape[0] / batch_size\n",
    "\n",
    "    # allocate symbolic variables for the data\n",
    "    index = T.lscalar()    # index to a [mini]batch\n",
    "    x = T.matrix('x')  # the data is presented as rasterized images\n",
    "\n",
    "    if not os.path.isdir(output_folder):\n",
    "        os.makedirs(output_folder)\n",
    "#     os.chdir(output_folder)\n",
    "\n",
    "    ####################################\n",
    "    # BUILDING THE MODEL NO CORRUPTION #\n",
    "    ####################################\n",
    "\n",
    "    rng = numpy.random.RandomState(123)\n",
    "    theano_rng = RandomStreams(rng.randint(2 ** 30))\n",
    "\n",
    "    da = dA(\n",
    "        numpy_rng=rng,\n",
    "        theano_rng=theano_rng,\n",
    "        input=x,\n",
    "#         n_visible=28 * 28,\n",
    "        n_visible=train_set_x.get_value().shape[1],\n",
    "        n_hidden=50\n",
    "    )\n",
    "\n",
    "    cost, updates = da.get_cost_updates(\n",
    "        corruption_level=0.,\n",
    "        learning_rate=learning_rate\n",
    "    )\n",
    "\n",
    "    train_da = theano.function(\n",
    "        [index],\n",
    "        cost,\n",
    "        updates=updates,\n",
    "        givens={\n",
    "            x: train_set_x[index * batch_size: (index + 1) * batch_size]\n",
    "        }\n",
    "    )\n",
    "\n",
    "    start_time = timeit.default_timer()\n",
    "\n",
    "    ############\n",
    "    # TRAINING #\n",
    "    ############\n",
    "\n",
    "    # go through training epochs\n",
    "    for epoch in xrange(training_epochs):\n",
    "        # go through trainng set\n",
    "        c = []\n",
    "        for batch_index in xrange(n_train_batches):\n",
    "            c.append(train_da(batch_index))\n",
    "\n",
    "        print 'Training epoch %d, cost ' % epoch, numpy.mean(c)\n",
    "\n",
    "    end_time = timeit.default_timer()\n",
    "\n",
    "    training_time = (end_time - start_time)\n",
    "\n",
    "    print('The no corruption code for file ran for %.2fm' % ((training_time) / 60.))\n",
    "    image = Image.fromarray(\n",
    "        tile_raster_images(X=da.W.get_value(borrow=True).T,\n",
    "                           img_shape=(28, 28), tile_shape=(10, 10),\n",
    "                           tile_spacing=(1, 1)))\n",
    "    image.save('filters_corruption_0.png')\n",
    "\n",
    "# Train the model with corruption\n",
    "\n",
    "    rng = numpy.random.RandomState(123)\n",
    "    theano_rng = RandomStreams(rng.randint(2 ** 30))\n",
    "\n",
    "    da = dA(\n",
    "        numpy_rng=rng,\n",
    "        theano_rng=theano_rng,\n",
    "        input=x,\n",
    "        n_visible=28 * 28,\n",
    "        n_hidden=500\n",
    "    )\n",
    "\n",
    "    cost, updates = da.get_cost_updates(\n",
    "        corruption_level=0.3,\n",
    "        learning_rate=learning_rate\n",
    "    )\n",
    "\n",
    "    train_da = theano.function(\n",
    "        [index],\n",
    "        cost,\n",
    "        updates=updates,\n",
    "        givens={\n",
    "            x: train_set_x[index * batch_size: (index + 1) * batch_size]\n",
    "        }\n",
    "    )\n",
    "\n",
    "    start_time = timeit.default_timer()\n",
    "\n",
    "    ############\n",
    "    # TRAINING #\n",
    "    ############\n",
    "\n",
    "    # go through training epochs\n",
    "    for epoch in xrange(training_epochs):\n",
    "        # go through trainng set\n",
    "        c = []\n",
    "        for batch_index in xrange(n_train_batches):\n",
    "            c.append(train_da(batch_index))\n",
    "\n",
    "        print 'Training epoch %d, cost ' % epoch, numpy.mean(c)\n",
    "\n",
    "    end_time = timeit.default_timer()\n",
    "\n",
    "    training_time = (end_time - start_time)\n",
    "# % ((training_time) / 60.)\n",
    "    print('The 30 percent corruption code for file ran for %.2fm' % ((training_time) / 60.))\n",
    "\n",
    "    image = Image.fromarray(tile_raster_images(\n",
    "        X=da.W.get_value(borrow=True).T,\n",
    "        img_shape=(28, 28), tile_shape=(10, 10),\n",
    "        tile_spacing=(1, 1)))\n",
    "    image.save('filters_corruption_30.png')\n",
    "\n",
    "#     os.chdir('../')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 15000*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch 0, cost  27250.3\n",
      "Training epoch 1, cost  24031.7\n",
      "Training epoch 2, cost  21743.5\n",
      "Training epoch 3, cost  20299.9\n",
      "Training epoch 4, cost  19256.5\n",
      "Training epoch 5, cost  18555.2\n",
      "Training epoch 6, cost  22483.2\n",
      "Training epoch 7, cost  22827.8\n",
      "Training epoch 8, cost  19827.8\n",
      "Training epoch 9, cost  19315.7\n",
      "Training epoch 10, cost  18498.0\n",
      "Training epoch 11, cost  21823.8\n",
      "Training epoch 12, cost  19370.8\n",
      "Training epoch 13, cost  18400.0\n",
      "Training epoch 14, cost  18027.2\n",
      "The no corruption code for file ran for 2.20m\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "total size of new array must be unchanged",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-63d54222fa0d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[0mtrain_dA\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset_X\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdataset_y\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-10-26bf741489b6>\u001b[0m in \u001b[0;36mtrain_dA\u001b[1;34m(learning_rate, training_epochs, dataset, batch_size, output_folder)\u001b[0m\n\u001b[0;32m     99\u001b[0m         tile_raster_images(X=da.W.get_value(borrow=True).T,\n\u001b[0;32m    100\u001b[0m                            \u001b[0mimg_shape\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m28\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m28\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtile_shape\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 101\u001b[1;33m                            tile_spacing=(1, 1)))\n\u001b[0m\u001b[0;32m    102\u001b[0m     \u001b[0mimage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'filters_corruption_0.png'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    103\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/chris/projects/dl4mt_labs/dcu_deep_learning/notebooks/day1/utils.pyc\u001b[0m in \u001b[0;36mtile_raster_images\u001b[1;34m(X, img_shape, tile_shape, tile_spacing, scale_rows_to_unit_interval, output_pixel_vals)\u001b[0m\n\u001b[0;32m    125\u001b[0m                         \u001b[1;31m# function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    126\u001b[0m                         this_img = scale_to_unit_interval(\n\u001b[1;32m--> 127\u001b[1;33m                             this_x.reshape(img_shape))\n\u001b[0m\u001b[0;32m    128\u001b[0m                     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    129\u001b[0m                         \u001b[0mthis_img\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mthis_x\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg_shape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: total size of new array must be unchanged"
     ]
    }
   ],
   "source": [
    "# n_visible=784, n_hidden=500,\n",
    "# TODO: the visible units in the model don't need to be parameterized\n",
    "# pass in the dataset directly to the test function, don't load inside the func\n",
    "\n",
    "dataset_X, dataset_y = load_fuel_dataset(POS_DATASET_PATH, which_sets=('train',))\n",
    "\n",
    "train_dA(dataset=(dataset_X, dataset_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# WORKING: how to visualize the filters in an autoencoder for text?\n",
    "# Idea: pass the training data through and store the top N instances which activate this node\n",
    "# at the end, output the top 5 instances for each node (convert window instance back to text)\n",
    "\n",
    "# WORKING: visualize 2d autoencoder output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'DET'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset_X.get_value()[1].nonzero()[0])\n",
    "len(dataset_X.get_value()[1])\n",
    "dataset_X.get_value().shape\n",
    "[corpus_indices['idx2word'][w] for w in train_X[63624]]\n",
    "corpus_indices['idx2tag'][train_y[63624][0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# In a separate notebook, compare 2d autoencoder vs 2d LSI -- visualize with scatter plots\n",
    "# cite Hinton etc for this visualization methodology "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# def load_data(dataset):\n",
    "#     ''' Loads the dataset\n",
    "\n",
    "#     :type dataset: string\n",
    "#     :param dataset: the path to the dataset (here MNIST)\n",
    "#     '''\n",
    "\n",
    "#     #############\n",
    "#     # LOAD DATA #\n",
    "#     #############\n",
    "\n",
    "#     # Download the MNIST dataset if it is not present\n",
    "#     data_dir, data_file = os.path.split(dataset)\n",
    "# #     if data_dir == \"\" and not os.path.isfile(dataset):\n",
    "# #         # Check if dataset is in the data directory.\n",
    "# #         new_path = os.path.join(\n",
    "# #             os.path.split(__file__)[0],\n",
    "# #             \"..\",\n",
    "# #             \"data\",\n",
    "# #             dataset\n",
    "# #         )\n",
    "# #         if os.path.isfile(new_path) or data_file == 'mnist.pkl.gz':\n",
    "# #             dataset = new_path\n",
    "\n",
    "#     if (not os.path.isfile(dataset)) and data_file == 'mnist.pkl.gz':\n",
    "#         import urllib\n",
    "#         origin = (\n",
    "#             'http://www.iro.umontreal.ca/~lisa/deep/data/mnist/mnist.pkl.gz'\n",
    "#         )\n",
    "#         print 'Downloading data from %s' % origin\n",
    "#         urllib.urlretrieve(origin, dataset)\n",
    "\n",
    "#     print '... loading data'\n",
    "\n",
    "#     # Load the dataset\n",
    "#     f = gzip.open(dataset, 'rb')\n",
    "#     train_set, valid_set, test_set = cPickle.load(f)\n",
    "#     f.close()\n",
    "#     #train_set, valid_set, test_set format: tuple(input, target)\n",
    "#     #input is an numpy.ndarray of 2 dimensions (a matrix)\n",
    "#     #where rows correspond to an example. target is a\n",
    "#     #numpy.ndarray of 1 dimensions (vector)) that have the same length as\n",
    "#     #the number of rows in the input. It should give the target\n",
    "#     #target to the example with the same index in the input.\n",
    "\n",
    "#     def shared_dataset(data_xy, borrow=True):\n",
    "#         \"\"\" Function that loads the dataset into shared variables\n",
    "\n",
    "#         The reason we store our dataset in shared variables is to allow\n",
    "#         Theano to copy it into the GPU memory (when code is run on GPU).\n",
    "#         Since copying data into the GPU is slow, copying a minibatch everytime\n",
    "#         is needed (the default behaviour if the data is not in a shared\n",
    "#         variable) would lead to a large decrease in performance.\n",
    "#         \"\"\"\n",
    "#         data_x, data_y = data_xy\n",
    "#         shared_x = theano.shared(numpy.asarray(data_x,\n",
    "#                                                dtype=theano.config.floatX),\n",
    "#                                  borrow=borrow)\n",
    "#         shared_y = theano.shared(numpy.asarray(data_y,\n",
    "#                                                dtype=theano.config.floatX),\n",
    "#                                  borrow=borrow)\n",
    "#         # When storing data on the GPU it has to be stored as floats\n",
    "#         # therefore we will store the labels as ``floatX`` as well\n",
    "#         # (``shared_y`` does exactly that). But during our computations\n",
    "#         # we need them as ints (we use labels as index, and if they are\n",
    "#         # floats it doesn't make sense) therefore instead of returning\n",
    "#         # ``shared_y`` we will have to cast it to int. This little hack\n",
    "#         # lets us get around this issue\n",
    "#         return shared_x, T.cast(shared_y, 'int32')\n",
    "\n",
    "#     test_set_x, test_set_y = shared_dataset(test_set)\n",
    "#     valid_set_x, valid_set_y = shared_dataset(valid_set)\n",
    "#     train_set_x, train_set_y = shared_dataset(train_set)\n",
    "\n",
    "#     rval = [(train_set_x, train_set_y), (valid_set_x, valid_set_y),\n",
    "#             (test_set_x, test_set_y)]\n",
    "#     return rval"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
