{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from __future__ import division, print_function\n",
    "import codecs\n",
    "import os\n",
    "import cPickle\n",
    "from collections import OrderedDict, Counter\n",
    "\n",
    "import numpy as np\n",
    "import nltk\n",
    "import h5py\n",
    "from fuel.datasets import IndexableDataset, H5PYDataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extract_windows(seq, window_size):\n",
    "    padded_seq = [u'_START_']*window_size + seq + [u'_END_']*window_size\n",
    "    return [padded_seq[i-window_size:i+window_size+1] for i in range(window_size, window_size+len(seq))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "MAX_SENTENCES = 10000\n",
    "WINDOW_SIZE = 2\n",
    "UNKNOWN_THRESHOLD = 2\n",
    "UNKNOWN_TOKEN = u'_UNK_'\n",
    "\n",
    "tagged_sentences = nltk.corpus.brown.tagged_sents(tagset='universal')\n",
    "\n",
    "words_by_line, tags_by_line = zip(*[zip(*sen)\n",
    "                                    for sen in list(tagged_sentences)[:MAX_SENTENCES]])\n",
    "\n",
    "# let's make a dictionary of words, and replace words which occur in < threshold sentences with u'_UNK_'\n",
    "word_counts = Counter([w for line in words_by_line for w in line])\n",
    "known_toks = set([k for k,v in word_counts.items() if v >= UNKNOWN_THRESHOLD])\n",
    "\n",
    "def map_token(tok):\n",
    "    if tok in known_toks:\n",
    "        return tok\n",
    "    return UNKNOWN_TOKEN\n",
    "\n",
    "words_by_line = [[map_token(w) for w in line] for line in words_by_line]\n",
    "\n",
    "word_windows, tags = zip(*[(word, tag) for word_seq, tags in zip(words_by_line, tags_by_line) \n",
    "                      for word, tag in zip(extract_windows(list(word_seq), window_size=WINDOW_SIZE), tags)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# cool, lets make our train, dev, and test splits\n",
    "num_instances = len(word_windows)\n",
    "DEV_FRACTION = 0.2\n",
    "TEST_FRACTION = 0.2\n",
    "\n",
    "dev_size = int(np.floor(num_instances * DEV_FRACTION))\n",
    "DEV_SPLIT = num_instances - dev_size\n",
    "TEST_SPLIT = num_instances - int(dev_size + np.floor(num_instances * TEST_FRACTION))\n",
    "\n",
    "# X_train, y_train = zip(*all_instances[:-TEST_SPLIT])\n",
    "# X_test, y_test = zip(*all_instances[-TEST_SPLIT:-DEV_SPLIT])\n",
    "# X_dev, y_dev = zip(*all_instances[-DEV_SPLIT:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "131862"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# split our corpus and get the set of tokens which occur in the training data\n",
    "# do the same thing for tags\n",
    "# this is a necessary preprocessing step in most machine learning scenarios, since we need to be able\n",
    "# to handle the cases where the test data contains things that we never saw during training\n",
    "\n",
    "training_instances = word_windows[:TEST_SPLIT]\n",
    "training_tags = set(tags[:TEST_SPLIT]).union(set([UNKNOWN_TOKEN]))\n",
    "training_tokens = set([w for l in training_instances for w in l]).union(set([UNKNOWN_TOKEN]))\n",
    "len(training_instances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# make a words dict, its reverse, and a tags dict and its reverse\n",
    "idx2word = dict(enumerate(training_tokens))\n",
    "word2idx = {v:k for k,v in idx2word.items()}\n",
    "\n",
    "idx2tag = dict(enumerate(set([t for l in tags_by_line for t in l])))\n",
    "tag2idx = {v:k for k,v in idx2tag.items()}\n",
    "\n",
    "def map_to_index(tok, index):\n",
    "    if tok in index:\n",
    "        return index[tok]\n",
    "    else:\n",
    "        return index[UNKNOWN_TOKEN]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# This should end up in dl4mt_exercises/\n",
    "DATASET_LOCATION = '../../datasets/'\n",
    "try:\n",
    "    os.mkdir(DATASET_LOCATION)\n",
    "except OSError:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# persist the indices \n",
    "corpus_indices = {'idx2word': idx2word, 'word2idx': word2idx, 'idx2tag': idx2tag, 'tag2idx': tag2idx}\n",
    "\n",
    "with open(os.path.join(DATASET_LOCATION, 'brown_pos_dataset.indices'), 'wb') as indices_file:\n",
    "    cPickle.dump(corpus_indices, indices_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# now create the fuel dataset\n",
    "# TODO: add the original data in case we need to map back\n",
    "# TODO: right now we are loading the original data again in later notebooks \n",
    "# -- all of that preparation should be done here\n",
    "\n",
    "iwords = [[map_to_index(w, word2idx) for w in l] for l in word_windows]\n",
    "itags = [map_to_index(t, tag2idx) for t in tags]\n",
    "\n",
    "DATASET_NAME = 'brown_pos_dataset.hdf5'\n",
    "DATASET_PATH = os.path.join(DATASET_LOCATION, DATASET_NAME)\n",
    "\n",
    "f = h5py.File(DATASET_PATH, mode='w')\n",
    "\n",
    "instances = f.create_dataset('instances', (num_instances, WINDOW_SIZE*2+1), dtype='uint32')\n",
    "instances[...] = np.array(iwords)\n",
    "\n",
    "targets = f.create_dataset('targets', (num_instances, 1), dtype='uint32')\n",
    "targets[...] = np.array(itags).reshape((num_instances, 1))\n",
    "\n",
    "instances.dims[0].label = 'batch'\n",
    "instances.dims[1].label = 'features'\n",
    "\n",
    "targets.dims[0].label = 'batch'\n",
    "targets.dims[1].label = 'index'\n",
    "\n",
    "split_dict = {\n",
    "    'train': {'instances': (0, TEST_SPLIT), 'targets': (0, TEST_SPLIT)},\n",
    "    'test' : {'instances': (TEST_SPLIT, DEV_SPLIT), 'targets': (TEST_SPLIT, DEV_SPLIT)},\n",
    "    'dev'  : {'instances': (DEV_SPLIT, num_instances), 'targets': (DEV_SPLIT, num_instances)}\n",
    "}\n",
    "\n",
    "f.attrs['split'] = H5PYDataset.create_split_array(split_dict)\n",
    "f.flush()\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# NOW SOME QUICK TESTING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "131862\n",
      "43954\n",
      "43954\n"
     ]
    }
   ],
   "source": [
    "train_set = H5PYDataset(DATASET_PATH, which_sets=('train',))\n",
    "print(train_set.num_examples)\n",
    "\n",
    "test_set = H5PYDataset(DATASET_PATH, which_sets=('test',))\n",
    "print(test_set.num_examples)\n",
    "\n",
    "dev_set = H5PYDataset(DATASET_PATH, which_sets=('dev',))\n",
    "print(dev_set.num_examples)\n",
    "\n",
    "in_memory_train = H5PYDataset(\n",
    "    DATASET_PATH, which_sets=('train',),\n",
    "    sources=['instances', 'targets'], load_in_memory=True)\n",
    "\n",
    "# train_X, train_y = in_memory_train.data_sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from fuel.streams import DataStream\n",
    "from fuel.schemes import ConstantScheme, SequentialScheme\n",
    "from fuel.transformers import Batch\n",
    "\n",
    "stream = DataStream.default_stream(in_memory_train,\n",
    "                                   iteration_scheme=SequentialScheme(in_memory_train.num_examples, 500))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# to iterate over the examples, you would do something like:\n",
    "\n",
    "# test_iter = stream.get_epoch_iterator()\n",
    "# for e in list(test_iter):\n",
    "#     print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# prep a corpus for POS tagging with DNNs -- use the Brown corpus from NLTK\n",
    "\n",
    "# the baseline taggers are window-based\n",
    "# data prep\n",
    "# separate training, dev, and test datasets\n",
    "\n",
    "\n",
    "# training pipeline\n",
    "# for every training sentence, segment into windows\n",
    "# \n",
    "\n",
    "# prediction pipeline:\n",
    "# input sentence\n",
    "# tokenize\n",
    "# pad left and right, then extract the windows for each token (parameterized by window size)\n",
    "# total vector width is feats*(window_size*2+1)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
