{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import pylab\n",
    "import sys\n",
    "\n",
    "# making sure the dl4mt module is on the path \n",
    "# -- this path depends upon the location where the notebook is running\n",
    "# for saving and import to work, it needs to be the day1 directory\n",
    "sys.path.append('../..')\n",
    "\n",
    "pylab.rcParams['figure.figsize'] = (10.0, 8.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Logistic Regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ../../dl4mt/logistic_regression.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ../../dl4mt/logistic_regression.py\n",
    "# Uncomment to save this cell to file in order to import it in later notebooks\n",
    "\n",
    "import cPickle\n",
    "import gzip\n",
    "import os\n",
    "import sys\n",
    "import timeit\n",
    "\n",
    "import numpy\n",
    "\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "\n",
    "\n",
    "class LogisticRegression(object):\n",
    "    \"\"\"Multi-class Logistic Regression Class\n",
    "\n",
    "    The logistic regression is fully described by a weight matrix :math:`W`\n",
    "    and bias vector :math:`b`. Classification is done by projecting data\n",
    "    points onto a set of hyperplanes, the distance to which is used to\n",
    "    determine a class membership probability.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input, n_in, n_out):\n",
    "        \"\"\" Initialize the Logistic Regression class parameters\n",
    "\n",
    "        :type input: theano.tensor.TensorType\n",
    "        :param input: symbolic variable that describes the input of the\n",
    "                      architecture (one minibatch)\n",
    "\n",
    "        :type n_in: int\n",
    "        :param n_in: number of input units, the dimension of the space in\n",
    "                     which the datapoints lie\n",
    "\n",
    "        :type n_out: int\n",
    "        :param n_out: number of output units, the dimension of the space in\n",
    "                      which the labels lie\n",
    "\n",
    "        \"\"\"\n",
    "  \n",
    "        # initialize with 0 the weights W as a matrix of shape (n_in, n_out)\n",
    "        self.W = theano.shared(\n",
    "            value=numpy.zeros(\n",
    "                (n_in, n_out),\n",
    "                dtype=theano.config.floatX\n",
    "            ),\n",
    "            name='W',\n",
    "            borrow=True\n",
    "        )\n",
    "        # initialize the biases b as a vector of n_out 0s\n",
    "        self.b = theano.shared(\n",
    "            value=numpy.zeros(\n",
    "                (n_out,),\n",
    "                dtype=theano.config.floatX\n",
    "            ),\n",
    "            name='b',\n",
    "            borrow=True\n",
    "        )\n",
    "\n",
    "        # symbolic expression for computing the matrix of class-membership\n",
    "        # probabilities\n",
    "        # Where:\n",
    "        # W is a matrix where column-k represents the separation hyperplane for\n",
    "        # class-k\n",
    "        # x is a matrix where row-j  represents input training sample-j (here input == x)\n",
    "        # b is a vector where element-k represents the free parameter of\n",
    "        # hyperplane-k\n",
    "        self.p_y_given_x = T.nnet.softmax(T.dot(input, self.W) + self.b)\n",
    "\n",
    "        # symbolic description of how to map p_y_given_x to the class with the maximum probability\n",
    "        self.y_pred = T.argmax(self.p_y_given_x, axis=1)\n",
    "    \n",
    "        # parameters of the model\n",
    "        self.params = [self.W, self.b]\n",
    "\n",
    "        # keep track of model input -- this will be useful for creating theano functions later\n",
    "        self.input = input\n",
    "\n",
    "    def predict(self, X):\n",
    "        # predict y_hat given X using the parameters of this model\n",
    "        # Quiz: why do we have this function, as well as self.p_y_given_x above?\n",
    "        return T.nnet.softmax(T.dot(X, self.W) + self.b)\n",
    "        \n",
    "    # TODO: implement this as a challenge -- add tests in the following cell to see if it's correct\n",
    "    def negative_log_likelihood(self, y):\n",
    "        \"\"\"Return the mean of the negative log-likelihood of the prediction\n",
    "        of this model under a given target distribution.\n",
    "\n",
    "        :type y: theano.tensor.TensorType\n",
    "        :param y: corresponds to a vector that gives for each example the\n",
    "                  correct label\n",
    "\n",
    "        Note: we use the mean instead of the sum so that\n",
    "              the learning rate is less dependent on the batch size\n",
    "        \"\"\"\n",
    "        return -T.mean(T.log(self.p_y_given_x)[T.arange(y.shape[0]), y])\n",
    "\n",
    "    def errors(self, y):\n",
    "        \"\"\"Return a float representing the number of errors in the minibatch\n",
    "        over the total number of examples of the minibatch; zero one\n",
    "        loss over the size of the minibatch\n",
    "\n",
    "        :type y: theano.tensor.TensorType\n",
    "        :param y: corresponds to a vector that gives for each example the\n",
    "                  correct label\n",
    "        \"\"\"\n",
    "\n",
    "        # check if y has same dimension of y_pred\n",
    "        if y.ndim != self.y_pred.ndim:\n",
    "            raise TypeError(\n",
    "                'y should have the same shape as self.y_pred',\n",
    "                ('y', y.type, 'y_pred', self.y_pred.type)\n",
    "            )\n",
    "        # check if y is of the correct datatype\n",
    "        if y.dtype.startswith('int'):\n",
    "            # the T.neq operator returns a vector of 0s and 1s, where 1\n",
    "            # represents a mistake in prediction\n",
    "            return T.mean(T.neq(self.y_pred, y))\n",
    "        else:\n",
    "            raise NotImplementedError()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from dl4mt.logistic_regression import LogisticRegression\n",
    "\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "\n",
    "\n",
    "# this function is unique to this model, and initializes the functions and parameters we'll need for training\n",
    "def initialize_logistic_regression(train_dataset, dev_dataset, learning_rate=0.1, batch_size=10, n_out=12):\n",
    "    \n",
    "    \"\"\" Initializes the functions and parameters to train a Logistic Regression Model\n",
    "\n",
    "    :type train_dataset: tuple\n",
    "    :param train_dataset: a tuple consisting of (X,y) for the training dataset\n",
    "                          (X and y are theano shared variables)\n",
    "                          \n",
    "    :type dev_dataset: tuple\n",
    "    :param dev_dataset: a tuple consisting of (X,y) for the dev/validation dataset\n",
    "                        (X and y are theano shared variables)                      \n",
    "    \n",
    "    :type learning_rate: float\n",
    "    :param learning_rate: learning rate used (factor for the stochastic\n",
    "                          gradient)\n",
    "\n",
    "    :type batch_size: int\n",
    "    :param batch_size: the minibatch size for training and validation \n",
    "\n",
    "    :type n_out: int\n",
    "    :param batch_size: the number of output \"classes\"\n",
    "    \"\"\"\n",
    "\n",
    "    train_set_x, train_set_y = train_dataset\n",
    "    valid_set_x, valid_set_y = dev_dataset\n",
    "\n",
    "    # compute number of minibatches for training, validation and testing\n",
    "    n_train_batches = train_set_x.get_value(borrow=True).shape[0] / batch_size\n",
    "    n_valid_batches = valid_set_x.get_value(borrow=True).shape[0] / batch_size\n",
    "\n",
    "    # create the logistic regression model\n",
    "    # allocate symbolic variables for the data\n",
    "    index = T.lscalar()  # index to a [mini]batch\n",
    "\n",
    "    # generate symbolic variables for input (x and y represent a minibatch)\n",
    "    x = T.matrix('x')  # data (one row per training instance)\n",
    "    y = T.ivector('y')  # labels, presented as 1D vector of [int] labels\n",
    "\n",
    "    n_in = train_set_x.get_value().shape[1]\n",
    "    \n",
    "    classifier = LogisticRegression(input=x, n_in=n_in, n_out=n_out)\n",
    "\n",
    "    # the cost we minimize during training is the negative log likelihood of the model\n",
    "    cost = classifier.negative_log_likelihood(y)\n",
    "\n",
    "    validate_model_func = theano.function(\n",
    "        inputs=[index],\n",
    "        outputs=classifier.errors(y),\n",
    "        givens={\n",
    "            x: valid_set_x[index * batch_size: (index + 1) * batch_size],\n",
    "            y: valid_set_y[index * batch_size: (index + 1) * batch_size]\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # compute the gradient of cost with respect to theta = (W,b)\n",
    "    g_W = T.grad(cost=cost, wrt=classifier.W)\n",
    "    g_b = T.grad(cost=cost, wrt=classifier.b)\n",
    "\n",
    "    # specify how to update the parameters of the model as a list of\n",
    "    # (variable, update expression) pairs.\n",
    "    updates = [(classifier.W, classifier.W - learning_rate * g_W),\n",
    "               (classifier.b, classifier.b - learning_rate * g_b)]\n",
    "\n",
    "    # compiling a Theano function `train_model` that returns the cost, and at\n",
    "    # the same time updates the parameter of the model based on the rules\n",
    "    # defined in `updates`\n",
    "    train_model_func = theano.function(\n",
    "        inputs=[index],\n",
    "        outputs=cost,\n",
    "        updates=updates,\n",
    "        givens={\n",
    "            x: train_set_x[index * batch_size: (index + 1) * batch_size],\n",
    "            y: train_set_y[index * batch_size: (index + 1) * batch_size]\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # return the classifier, the training and validation functions, and n_train_batches, n_valid_batches\n",
    "    return (classifier, train_model_func, validate_model_func, n_train_batches, n_valid_batches)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A Generic Training Function that we will reuse in later notebooks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ../../dl4mt/training.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ../../dl4mt/training.py\n",
    "\n",
    "# this function is shared by all day1 models which are trained with SGD\n",
    "import timeit\n",
    "import numpy\n",
    "\n",
    "def train_model(train_model, n_train_batches, validate_model=None, n_valid_batches=0, training_epochs=25):\n",
    "    \n",
    "    # early-stopping parameters\n",
    "    patience = 5000  # use at least this many training examples\n",
    "    patience_increase = 2  # wait this much longer when a new best is found\n",
    "    improvement_threshold = 0.995  # a relative improvement of this much is\n",
    "                                  # considered significant\n",
    "    validation_frequency = min(n_train_batches, patience / 2)\n",
    "                                  # go through this many\n",
    "                                  # minibatches before checking the network\n",
    "                                  # on the validation set; in this case we\n",
    "                                  # check every epoch\n",
    "\n",
    "    best_validation_loss = numpy.inf\n",
    "    test_score = 0.\n",
    "    start_time = timeit.default_timer()\n",
    "\n",
    "    done_looping = False\n",
    "    epoch = 0\n",
    "    while (epoch < training_epochs) and (not done_looping):\n",
    "        epoch = epoch + 1\n",
    "        mini_batch_costs = []\n",
    "        for minibatch_index in xrange(n_train_batches):\n",
    "            \n",
    "            # iteration number\n",
    "            iter = (epoch - 1) * n_train_batches + minibatch_index\n",
    "            \n",
    "            minibatch_avg_cost = train_model(minibatch_index)\n",
    "            mini_batch_costs.append(minibatch_avg_cost)\n",
    "            \n",
    "            if validate_model is not None:\n",
    "                if (iter + 1) % validation_frequency == 0:\n",
    "                    # compute zero-one loss on validation set\n",
    "                    validation_losses = [validate_model(i)\n",
    "                                         for i in xrange(n_valid_batches)]\n",
    "                    this_validation_loss = numpy.mean(validation_losses)\n",
    "\n",
    "                    print(\n",
    "                        'epoch %i, minibatch %i/%i, validation error %f %%' %\n",
    "                        (\n",
    "                            epoch,\n",
    "                            minibatch_index + 1,\n",
    "                            n_train_batches,\n",
    "                            this_validation_loss * 100.\n",
    "                        )\n",
    "                    )\n",
    "\n",
    "                    # if we got the best validation score until now\n",
    "                    if this_validation_loss < best_validation_loss:\n",
    "                        #improve patience if loss improvement is good enough\n",
    "                        if this_validation_loss < best_validation_loss *  \\\n",
    "                           improvement_threshold:\n",
    "                            patience = max(patience, iter * patience_increase)\n",
    "\n",
    "                        best_validation_loss = this_validation_loss\n",
    "\n",
    "\n",
    "                if patience <= iter:\n",
    "                    done_looping = True\n",
    "                    break\n",
    "         \n",
    "        print(\n",
    "                'epoch %i, average training cost %0.2f' %\n",
    "                (\n",
    "                    epoch,\n",
    "                    numpy.mean(mini_batch_costs)\n",
    "                )\n",
    "        )\n",
    "                \n",
    "    end_time = timeit.default_timer()\n",
    "    print('Optimization complete!')\n",
    "    if best_validation_loss < numpy.inf:\n",
    "        print(\"Best validation score: %f %%\") % (best_validation_loss * 100.)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions for preparing the dataset\n",
    "\n",
    "The `prep_dataset` function will be used in other notebooks as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ../../dl4mt/datasets.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ../../dl4mt/datasets.py\n",
    "# Uncomment to save this cell to file in order to import it in later notebooks\n",
    "\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "import numpy as np\n",
    "from fuel.datasets import H5PYDataset\n",
    "\n",
    "def create_hstacked_vectors(index_path, seqs):\n",
    "    # load the index, map the seqs (word indices) through the index, hstack, then cast the results to np.array\n",
    "    index = np.load(index_path)\n",
    "    \n",
    "    working_index = np.load(index_path)\n",
    "    return np.array([np.hstack([index[idx] for idx in seq]) for seq in seqs])\n",
    "\n",
    "# load a portion of a fuel dataset\n",
    "def load_fuel_dataset(filename, which_sets=None):\n",
    "    X, y = H5PYDataset(\n",
    "        filename, which_sets=which_sets,\n",
    "        sources=['instances', 'targets'], load_in_memory=True).data_sources\n",
    "    \n",
    "    # make sure y is 0-dimensional\n",
    "    y = y.ravel()\n",
    "    \n",
    "    return (X,y)\n",
    "    \n",
    "    \n",
    "def shared_dataset(X, y, borrow=True, cast_y=True):\n",
    "    \"\"\" Function that loads the dataset into shared variables\n",
    "    The reason we store our dataset in shared variables is to allow\n",
    "    Theano to copy it into the GPU memory (when code is run on GPU).\n",
    "    Since copying data into the GPU is slow, copying a minibatch everytime\n",
    "    is needed (the default behaviour if the data is not in a shared\n",
    "    variable) would lead to a large decrease in performance.\n",
    "    \"\"\"\n",
    "\n",
    "    shared_x = theano.shared(np.asarray(X,\n",
    "                                           dtype=theano.config.floatX), borrow=borrow)\n",
    "    shared_y = theano.shared(np.asarray(y,\n",
    "                                           dtype=theano.config.floatX), borrow=borrow)\n",
    " \n",
    "    # note that shared_y is cast to int because it is used as a vector of indices\n",
    "    if cast_y:\n",
    "        return shared_x, T.cast(shared_y, 'int32')\n",
    "    return shared_x, shared_y\n",
    "\n",
    "def prep_dataset(path, index_path, which_sets=[], cutoff=None, cast_y=True):\n",
    "    X, y = load_fuel_dataset(path, which_sets=which_sets)\n",
    "    if cutoff is not None:\n",
    "        X = X[:cutoff]\n",
    "        y = y[:cutoff]\n",
    "\n",
    "    # extract windows\n",
    "    X = create_hstacked_vectors(index_path, X)\n",
    "\n",
    "    # make shared dataset\n",
    "    return shared_dataset(X, y, cast_y=cast_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, minibatch 200/200, validation error 25.840000 %\n",
      "epoch 1, average training cost 1.01\n",
      "epoch 2, minibatch 200/200, validation error 22.270000 %\n",
      "epoch 2, average training cost 0.58\n",
      "epoch 3, minibatch 200/200, validation error 20.920000 %\n",
      "epoch 3, average training cost 0.49\n",
      "epoch 4, minibatch 200/200, validation error 20.190000 %\n",
      "epoch 4, average training cost 0.44\n",
      "epoch 5, minibatch 200/200, validation error 19.840000 %\n",
      "epoch 5, average training cost 0.40\n",
      "epoch 6, minibatch 200/200, validation error 19.650000 %\n",
      "epoch 6, average training cost 0.38\n",
      "epoch 7, minibatch 200/200, validation error 19.340000 %\n",
      "epoch 7, average training cost 0.36\n",
      "epoch 8, minibatch 200/200, validation error 19.240000 %\n",
      "epoch 8, average training cost 0.34\n",
      "epoch 9, minibatch 200/200, validation error 19.010000 %\n",
      "epoch 9, average training cost 0.33\n",
      "epoch 10, minibatch 200/200, validation error 18.920000 %\n",
      "epoch 10, average training cost 0.32\n",
      "epoch 11, minibatch 200/200, validation error 18.760000 %\n",
      "epoch 11, average training cost 0.31\n",
      "epoch 12, minibatch 200/200, validation error 18.670000 %\n",
      "epoch 12, average training cost 0.30\n",
      "epoch 13, minibatch 200/200, validation error 18.520000 %\n",
      "epoch 13, average training cost 0.30\n",
      "epoch 14, minibatch 200/200, validation error 18.500000 %\n",
      "epoch 14, average training cost 0.29\n",
      "epoch 15, minibatch 200/200, validation error 18.390000 %\n",
      "epoch 15, average training cost 0.28\n",
      "epoch 16, minibatch 200/200, validation error 18.300000 %\n",
      "epoch 16, average training cost 0.28\n",
      "epoch 17, minibatch 200/200, validation error 18.200000 %\n",
      "epoch 17, average training cost 0.27\n",
      "epoch 18, minibatch 200/200, validation error 18.210000 %\n",
      "epoch 18, average training cost 0.27\n",
      "epoch 19, minibatch 200/200, validation error 18.130000 %\n",
      "epoch 19, average training cost 0.26\n",
      "epoch 20, minibatch 200/200, validation error 18.000000 %\n",
      "epoch 20, average training cost 0.26\n",
      "Optimization complete!\n",
      "Best validation score: 18.000000 %\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "from dl4mt.datasets import prep_dataset\n",
    "from dl4mt.training import train_model\n",
    "\n",
    "# location of our datasets\n",
    "DATASET_LOCATION = '../../datasets/'\n",
    "\n",
    "# the pos dataset consists of windows around words\n",
    "POS_DATASET_NAME = 'brown_pos_dataset.hdf5'\n",
    "POS_DATASET_PATH = os.path.join(DATASET_LOCATION, POS_DATASET_NAME)\n",
    "\n",
    "WORD_BY_WORD_MATRIX = 'brown.word-by-word.normalized.npy'\n",
    "# Just for fun -- a Latent Semantic Indexing (LSI) matrix instead of word-by-word\n",
    "# WORD_BY_WORD_MATRIX = 'brown.lsi-index.npy'\n",
    "VECTOR_INDEX_PATH = os.path.join(DATASET_LOCATION, WORD_BY_WORD_MATRIX)\n",
    "\n",
    "# the cutoff for the maximum number of training and dev examples\n",
    "# change this parameter to use more/fewer examples\n",
    "CUTOFF = 10000\n",
    "  \n",
    "train_dataset = prep_dataset(POS_DATASET_PATH, VECTOR_INDEX_PATH, which_sets=['train'], cutoff=CUTOFF)\n",
    "dev_dataset = prep_dataset(POS_DATASET_PATH, VECTOR_INDEX_PATH, which_sets=['dev'], cutoff=CUTOFF)\n",
    "\n",
    "# get the functions and params that we need for our models\n",
    "initialization_data = initialize_logistic_regression(train_dataset, dev_dataset,\n",
    "                                                     learning_rate=0.1, batch_size=50)\n",
    "\n",
    "classifier, train_model_func, validate_model_func, n_train_batches, n_valid_batches = initialization_data\n",
    "    \n",
    "# load the training function and train the LR model \n",
    "# in general, train_model should return any useful information about the training process\n",
    "train_model(train_model_func,  n_train_batches, validate_model=validate_model_func,\n",
    "            n_valid_batches=n_valid_batches, training_epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ../../dl4mt/predict.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ../../dl4mt/predict.py\n",
    "\n",
    "import theano\n",
    "import cPickle\n",
    "\n",
    "def predict(classifier, test_X):\n",
    "    \"\"\"Get a model's predictions for a given X\n",
    "    \n",
    "    classifier is expected to be either: (1) a theano graph with `input` and `y_pred`,\n",
    "    or a path to a pickled model\n",
    "    \"\"\"\n",
    "\n",
    "    if type(classifier) is str:\n",
    "        # load the saved model at the path specified by 'classifier'\n",
    "        print(\"Loading model from: {}\".format(classifier))\n",
    "        classifier = cPickle.load(open(classifier))\n",
    "\n",
    "    # compile a predictor function\n",
    "    predict_model = theano.function(\n",
    "        inputs=[classifier.input],\n",
    "        outputs=classifier.y_pred)\n",
    "\n",
    "    y_hat = predict_model(test_X)\n",
    "    \n",
    "    return y_hat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.85360000000000003"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import cPickle\n",
    "from dl4mt.predict import predict\n",
    "\n",
    "test_dataset = prep_dataset(POS_DATASET_PATH, VECTOR_INDEX_PATH, which_sets=['test'], cutoff=CUTOFF, cast_y=False)\n",
    "\n",
    "test_X, test_y = test_dataset\n",
    "test_y = test_y.get_value().astype('int32')\n",
    "predictions = predict(classifier, test_X.get_value())\n",
    "\n",
    "CORPUS_INDICES = 'brown_pos_dataset.indices'\n",
    "# Indexes for mapping words and tags <--> ints\n",
    "with open(os.path.join(DATASET_LOCATION, CORPUS_INDICES)) as indices_file:\n",
    "    corpus_indices = cPickle.load(indices_file)\n",
    "\n",
    "# map tag ids back to strings\n",
    "y_test_actual = [corpus_indices['idx2tag'][tag_idx] for tag_idx in test_y]\n",
    "y_test_hat = [corpus_indices['idx2tag'][tag_idx] for tag_idx in predictions]\n",
    "\n",
    "# quick check of our accuracy on the test dataset\n",
    "sum([y==p for y,p in zip(predictions, test_y)]) / float(len(predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ../../dl4mt/save_model.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ../../dl4mt/save_model.py\n",
    "\n",
    "import cPickle\n",
    "\n",
    "def save_model(model, model_path):\n",
    "    # save the model for later\n",
    "    with open(model_path, 'w') as f:\n",
    "        cPickle.dump(model, f)\n",
    "    print(\"Model saved to: {}\".format(model_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to: ../../trained_models/logistic_regression_model.pkl\n"
     ]
    }
   ],
   "source": [
    "from dl4mt.save_model import save_model\n",
    "\n",
    "MODELS_PATH = \"../../trained_models/\"\n",
    "MODEL_NAME = \"logistic_regression_model.pkl\"\n",
    "save_path = os.path.join(MODELS_PATH, MODEL_NAME)\n",
    "\n",
    "save_model(classifier, save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ../../dl4mt/confusion_matrix.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ../../dl4mt/confusion_matrix.py\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def plot_confusion_matrix(cm, class_names, title='Confusion matrix', cmap=plt.cm.Blues):\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(class_names))\n",
    "    plt.xticks(tick_marks, class_names, rotation=45)\n",
    "    plt.yticks(tick_marks, class_names)\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAokAAAJJCAYAAADGP62VAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nPQR2AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f4706d44910>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from dl4mt.confusion_matrix import plot_confusion_matrix\n",
    "\n",
    "# Compute confusion matrix\n",
    "cm = confusion_matrix(y_test_actual, y_test_hat)\n",
    "\n",
    "# get class names\n",
    "class_names = list(set(y_test_actual))\n",
    "\n",
    "# Normalize the confusion matrix by row (i.e by the number of samples in each class)\n",
    "cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "# np.set_printoptions(precision=2)\n",
    "# print('Normalized confusion matrix')\n",
    "# print(cm_normalized)\n",
    "plt.figure()\n",
    "plot_confusion_matrix(cm_normalized, class_names, title='Normalized confusion matrix')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Info about the tagset\n",
    "\n",
    "from https://github.com/slavpetrov/universal-pos-tags\n",
    "\n",
    "- VERB - verbs (all tenses and modes)      \n",
    "- NOUN - nouns (common and proper)      \n",
    "- PRON - pronouns       \n",
    "- ADJ - adjectives      \n",
    "- ADV - adverbs      \n",
    "- ADP - adpositions (prepositions and postpositions)      \n",
    "- CONJ - conjunctions      \n",
    "- DET - determiners      \n",
    "- NUM - cardinal numbers      \n",
    "- PRT - particles or other function words      \n",
    "- X - other: foreign words, typos, abbreviations      \n",
    "- . - punctuation      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Challenges:\n",
    "\n",
    "- Compute the f1 score for each class\n",
    "- Does Peformance improve when you increase the number of training examples?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
