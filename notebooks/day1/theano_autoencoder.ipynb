{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The purpose of this tutorial is to review logistic regression, autoencoders, and the Multilayer Perceptron (MLP)\n",
    "\n",
    "- create a one-layer autoencoder to encode word windows into 100-dim vectors\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import codecs\n",
    "import re\n",
    "import json\n",
    "import random\n",
    "import math\n",
    "import os\n",
    "import gzip\n",
    "import cPickle\n",
    "import sys\n",
    "from collections import Counter, defaultdict\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pylab\n",
    "import pandas as pd\n",
    "from scipy.stats import norm\n",
    "import nltk\n",
    "\n",
    "pylab.rcParams['figure.figsize'] = (10.0, 8.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from fuel.datasets import H5PYDataset\n",
    "\n",
    "DATASET_LOCATION = 'datasets/'\n",
    "\n",
    "# the pos dataset consists of windows around words\n",
    "POS_DATASET_NAME = 'brown_pos_dataset.hdf5'\n",
    "POS_DATASET_PATH = os.path.join(DATASET_LOCATION, POS_DATASET_NAME)\n",
    "\n",
    "CORPUS_INDICES = 'brown_pos_dataset.indices'\n",
    "WORD_BY_WORD_MATRIX = 'brown.word-by-word.normalized.npy'\n",
    "\n",
    "# Indexes for mapping words <--> ints\n",
    "with open(os.path.join(DATASET_LOCATION, CORPUS_INDICES)) as indices_file:\n",
    "    corpus_indices = cPickle.load(indices_file)\n",
    "\n",
    "train_X, train_y = H5PYDataset(\n",
    "    POS_DATASET_PATH, which_sets=('train',),\n",
    "    sources=['instances', 'targets'], load_in_memory=True).data_sources\n",
    "\n",
    "dev_X, dev_y = H5PYDataset(\n",
    "    POS_DATASET_PATH, which_sets=('dev',),\n",
    "    sources=['instances', 'targets'], load_in_memory=True).data_sources\n",
    "\n",
    "test_X, test_y = H5PYDataset(\n",
    "    POS_DATASET_PATH, which_sets=('test',),\n",
    "    sources=['instances', 'targets'], load_in_memory=True).data_sources\n",
    "\n",
    "# make sure the dl4mt module is on the path\n",
    "sys.path.append('../..')\n",
    "# TODO: user needs to set this to the place where they cloned the repo\n",
    "# TODO: user also needs to run the ipython notebook from inside the directory for that day, \n",
    "# otherwise the paths will be messed up\n",
    "sys.path.append('/home/chris/projects/dl4mt_labs/dcu_deep_learning/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ../../dl4mt/autoencoder_layer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ../../dl4mt/autoencoder_layer.py\n",
    "# Uncomment to save this cell to file in order to import it in later notebooks\n",
    "\n",
    "\"\"\"\n",
    " Most of this class was taken from the lisa-lab deeplearning tutorials \n",
    " -- https://github.com/lisa-lab/DeepLearningTutorials\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import timeit\n",
    "\n",
    "import numpy\n",
    "\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "from theano.tensor.shared_randomstreams import RandomStreams\n",
    "\n",
    "class dA(object):\n",
    "    \"\"\"Denoising Auto-Encoder class (dA)\n",
    "\n",
    "    A denoising autoencoders tries to reconstruct the input from a corrupted\n",
    "    version of it by projecting it first in a latent space and reprojecting\n",
    "    it afterwards back in the input space. Please refer to Vincent et al.,2008\n",
    "    for more details. If x is the input then equation (1) computes a partially\n",
    "    destroyed version of x by means of a stochastic mapping q_D. Equation (2)\n",
    "    computes the projection of the input into the latent space. Equation (3)\n",
    "    computes the reconstruction of the input, while equation (4) computes the\n",
    "    reconstruction error.\n",
    "\n",
    "    .. math::\n",
    "\n",
    "        \\tilde{x} ~ q_D(\\tilde{x}|x)                                     (1)\n",
    "\n",
    "        y = s(W \\tilde{x} + b)                                           (2)\n",
    "\n",
    "        x = s(W' y  + b')                                                (3)\n",
    "\n",
    "        L(x,z) = -sum_{k=1}^d [x_k \\log z_k + (1-x_k) \\log( 1-z_k)]      (4)\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        numpy_rng,\n",
    "        theano_rng=None,\n",
    "        input=None,\n",
    "        n_visible=784,\n",
    "        n_hidden=500,\n",
    "        W=None,\n",
    "        W_prime=None,\n",
    "        bhid=None,\n",
    "        bvis=None\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initialize the dA class by specifying the number of visible units (the\n",
    "        dimension d of the input ), the number of hidden units ( the dimension\n",
    "        d' of the latent or hidden space ) and the corruption level. The\n",
    "        constructor also receives symbolic variables for the input, weights and\n",
    "        bias. Such a symbolic variables are useful when, for example the input\n",
    "        is the result of some computations, or when weights are shared between\n",
    "        the dA and an MLP layer. When dealing with SdAs this always happens,\n",
    "        the dA on layer 2 gets as input the output of the dA on layer 1,\n",
    "        and the weights of the dA are used in the second stage of training\n",
    "        to construct an MLP.\n",
    "\n",
    "        :type numpy_rng: numpy.random.RandomState\n",
    "        :param numpy_rng: number random generator used to generate weights\n",
    "\n",
    "        :type theano_rng: theano.tensor.shared_randomstreams.RandomStreams\n",
    "        :param theano_rng: Theano random generator; if None is given one is\n",
    "                     generated based on a seed drawn from `rng`\n",
    "\n",
    "        :type input: theano.tensor.TensorType\n",
    "        :param input: a symbolic description of the input or None for\n",
    "                      standalone dA\n",
    "\n",
    "        :type n_visible: int\n",
    "        :param n_visible: number of visible units\n",
    "\n",
    "        :type n_hidden: int\n",
    "        :param n_hidden:  number of hidden units\n",
    "\n",
    "        :type W: theano.tensor.TensorType\n",
    "        :param W: Theano variable pointing to a set of weights that should be\n",
    "                  shared belong the dA and another architecture; if dA should\n",
    "                  be standalone set this to None\n",
    "                  \n",
    "        :type W_prime: theano.tensor.TensorType\n",
    "        :param W_prime: Theano variable pointing to a set of weights that should be\n",
    "                  shared belong the dA and another architecture; if dA should\n",
    "                  be standalone set this to None\n",
    "\n",
    "        :type bhid: theano.tensor.TensorType\n",
    "        :param bhid: Theano variable pointing to a set of biases values (for\n",
    "                     hidden units) that should be shared belong dA and another\n",
    "                     architecture; if dA should be standalone set this to None\n",
    "\n",
    "        :type bvis: theano.tensor.TensorType\n",
    "        :param bvis: Theano variable pointing to a set of biases values (for\n",
    "                     visible units) that should be shared belong dA and another\n",
    "                     architecture; if dA should be standalone set this to None\n",
    "\n",
    "\n",
    "        \"\"\"\n",
    "        self.n_visible = n_visible\n",
    "        self.n_hidden = n_hidden\n",
    "\n",
    "        # create a Theano random generator that gives symbolic random values\n",
    "        if not theano_rng:\n",
    "            theano_rng = RandomStreams(numpy_rng.randint(2 ** 30))\n",
    "\n",
    "        # note : W' was written as `W_prime` and b' as `b_prime`\n",
    "        if not W:\n",
    "            # W is initialized with `initial_W` which is uniformly sampled\n",
    "            # from -4*sqrt(6./(n_visible+n_hidden)) and\n",
    "            # 4*sqrt(6./(n_hidden+n_visible))the output of uniform if\n",
    "            # converted using asarray to dtype\n",
    "            # theano.config.floatX so that the code is runable on GPU\n",
    "            initial_W = numpy.asarray(\n",
    "                numpy_rng.uniform(\n",
    "                    low=-4 * numpy.sqrt(6. / (n_hidden + n_visible)),\n",
    "                    high=4 * numpy.sqrt(6. / (n_hidden + n_visible)),\n",
    "                    size=(n_visible, n_hidden)\n",
    "                ),\n",
    "                dtype=theano.config.floatX\n",
    "            )\n",
    "            W = theano.shared(value=initial_W, name='W', borrow=True)\n",
    "\n",
    "        # Note that this is not the \"tied-weights\" autoencoder\n",
    "        if not W_prime:\n",
    "            # W is initialized with `initial_W` which is uniformly sampled\n",
    "            # from -4*sqrt(6./(n_visible+n_hidden)) and\n",
    "            # 4*sqrt(6./(n_hidden+n_visible))the output of uniform if\n",
    "            # converted using asarray to dtype\n",
    "            # theano.config.floatX so that the code is runable on GPU\n",
    "            initial_W_prime = numpy.asarray(\n",
    "                numpy_rng.uniform(\n",
    "                    low=-4 * numpy.sqrt(6. / (n_hidden + n_visible)),\n",
    "                    high=4 * numpy.sqrt(6. / (n_hidden + n_visible)),\n",
    "                    size=(n_hidden, n_visible)\n",
    "                ),\n",
    "                dtype=theano.config.floatX\n",
    "            )\n",
    "            W_prime = theano.shared(value=initial_W_prime, name='W', borrow=True)\n",
    "\n",
    "            \n",
    "        if not bvis:\n",
    "            bvis = theano.shared(\n",
    "                value=numpy.zeros(\n",
    "                    n_visible,\n",
    "                    dtype=theano.config.floatX\n",
    "                ),\n",
    "                borrow=True\n",
    "            )\n",
    "\n",
    "        if not bhid:\n",
    "            bhid = theano.shared(\n",
    "                value=numpy.zeros(\n",
    "                    n_hidden,\n",
    "                    dtype=theano.config.floatX\n",
    "                ),\n",
    "                name='b',\n",
    "                borrow=True\n",
    "            )\n",
    "\n",
    "        self.W = W\n",
    "        # b corresponds to the bias of the hidden\n",
    "        self.b = bhid\n",
    "        # b_prime corresponds to the bias of the visible\n",
    "        self.b_prime = bvis\n",
    "        \n",
    "        # TODO -- try untying the weights\n",
    "        # Autoencoder with tied weights, therefore W_prime is W transpose\n",
    "#         self.W_prime = self.W.T\n",
    "        self.W_prime = W_prime\n",
    "        \n",
    "        self.theano_rng = theano_rng\n",
    "        # if no input is given, generate a variable representing the input\n",
    "        if input is None:\n",
    "            # we use a matrix because we expect a minibatch of several\n",
    "            # examples, each example being a row\n",
    "            self.x = T.dmatrix(name='input')\n",
    "        else:\n",
    "            self.x = input\n",
    "\n",
    "        self.params = [self.W, self.b, self.b_prime]\n",
    "\n",
    "    # TODO: user should implement this function -- to start, just return identity\n",
    "    def get_corrupted_input(self, input, corruption_level):\n",
    "        \"\"\"This function keeps ``1-corruption_level`` entries of the inputs the\n",
    "        same and zero-out randomly selected subset of size ``coruption_level``\n",
    "        Note : first argument of theano.rng.binomial is the shape(size) of\n",
    "               random numbers that it should produce\n",
    "               second argument is the number of trials\n",
    "               third argument is the probability of success of any trial\n",
    "\n",
    "                this will produce an array of 0s and 1s where 1 has a\n",
    "                probability of 1 - ``corruption_level`` and 0 with\n",
    "                ``corruption_level``\n",
    "\n",
    "                The binomial function return int64 data type by\n",
    "                default.  int64 multiplicated by the input\n",
    "                type(floatX) always return float64.  To keep all data\n",
    "                in floatX when floatX is float32, we set the dtype of\n",
    "                the binomial to floatX. As in our case the value of\n",
    "                the binomial is always 0 or 1, this don't change the\n",
    "                result. This is needed to allow the gpu to work\n",
    "                correctly as it only support float32 for now.\n",
    "\n",
    "        \"\"\"\n",
    "        return self.theano_rng.binomial(size=input.shape, n=1,\n",
    "                                        p=1 - corruption_level,\n",
    "                                        dtype=theano.config.floatX) * input\n",
    " \n",
    "    def get_hidden_values(self, input):\n",
    "        \"\"\" Computes the values of the hidden layer \"\"\"\n",
    "        return T.nnet.sigmoid(T.dot(input, self.W) + self.b)\n",
    "    \n",
    "    def predict(self, input):\n",
    "        \"\"\" Alias to get_hidden_values for consistent API \"\"\"\n",
    "        return self.get_hidden_values(input)\n",
    "\n",
    "    def get_reconstructed_input(self, hidden):\n",
    "        \"\"\"Computes the reconstructed input given the values of the\n",
    "        hidden layer\n",
    "\n",
    "        \"\"\"\n",
    "        return T.nnet.sigmoid(T.dot(hidden, self.W_prime) + self.b_prime)\n",
    "\n",
    "    def get_cost_updates(self, corruption_level, learning_rate):\n",
    "        \"\"\" This function computes the cost and the updates for one trainng\n",
    "        step of the dA \"\"\"\n",
    "\n",
    "        tilde_x = self.get_corrupted_input(self.x, corruption_level)\n",
    "        y = self.get_hidden_values(tilde_x)\n",
    "        z = self.get_reconstructed_input(y)\n",
    "        \n",
    "        # sum over the columns of each training instance\n",
    "        L = - T.sum(self.x * T.log(z) + (1 - self.x) * T.log(1 - z), axis=1)\n",
    "        \n",
    "        # note : L is now a vector, where each element is the\n",
    "        #        cross-entropy cost of the reconstruction of the\n",
    "        #        corresponding example of the minibatch. We need to\n",
    "        #        compute the average of all these to get the cost of\n",
    "        #        the minibatch\n",
    "        cost = T.mean(L)\n",
    "\n",
    "        # compute the gradients of the cost of the `dA` with respect\n",
    "        # to its parameters\n",
    "        gparams = T.grad(cost, self.params)\n",
    "        # generate the list of updates\n",
    "        updates = [\n",
    "            (param, param - learning_rate * gparam)\n",
    "            for param, gparam in zip(self.params, gparams)\n",
    "        ]\n",
    "\n",
    "        return (cost, updates)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TODO: we need a consistent interface to the functionality of the network for every model\n",
    "# trained models need to be able to predict and ideally persist their params for loading later\n",
    "# the interface to persistence and loading trained models should also be consistent\n",
    "import numpy\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "import timeit\n",
    "from theano.tensor.shared_randomstreams import RandomStreams\n",
    "\n",
    "from dl4mt.autoencoder_layer import dA\n",
    "\n",
    "# TODO: remove training_epochs param\n",
    "def initialize_dA(train_dataset, learning_rate=0.1, corruption_level=0.0, batch_size=10):\n",
    "\n",
    "    \"\"\"\n",
    "    :type learning_rate: float\n",
    "    :param learning_rate: learning rate used for training the Autoencoder\n",
    "\n",
    "    :type training_epochs: int\n",
    "    :param training_epochs: number of epochs used for training\n",
    "\n",
    "    :type dataset: string\n",
    "    :param dataset: path to the picked dataset\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    train_set_x, train_set_y = train_dataset\n",
    "        \n",
    "    # compute number of minibatches for training, validation and testing\n",
    "    n_train_batches = train_set_x.get_value(borrow=True).shape[0] / batch_size\n",
    "\n",
    "    # allocate symbolic variables for the data\n",
    "    index = T.lscalar()    # index to a [mini]batch\n",
    "    x = T.matrix('x')  # the data will be concatenated word vector windows\n",
    "\n",
    "    rng = numpy.random.RandomState(123)\n",
    "    theano_rng = RandomStreams(rng.randint(2 ** 30))\n",
    "\n",
    "    # TODO: move \"n_hidden\" out to a more obvious spot so that the user can play with different vector sizes\n",
    "    da = dA(\n",
    "        numpy_rng=rng,\n",
    "        theano_rng=theano_rng,\n",
    "        input=x,\n",
    "        n_visible=train_set_x.get_value().shape[1],\n",
    "        n_hidden=2\n",
    "    )\n",
    "\n",
    "    cost, updates = da.get_cost_updates(\n",
    "        corruption_level=corruption_level,\n",
    "        learning_rate=learning_rate\n",
    "    )\n",
    "\n",
    "    train_model_func = theano.function(\n",
    "        [index],\n",
    "        cost,\n",
    "        updates=updates,\n",
    "        givens={\n",
    "            x: train_set_x[index * batch_size: (index + 1) * batch_size]\n",
    "        }\n",
    "    )\n",
    "    \n",
    "#     classifier, train_model_func, validate_model_func, n_train_batches, n_valid_batches\n",
    "    return (da, train_model_func, None, n_train_batches, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "#     start_time = timeit.default_timer()\n",
    "    \n",
    "    # TODO: the epoch code below is common to all training loops\n",
    "    # TODO: there should be a function called \"train\" which takes in the training function and any other args\n",
    "    # then it transparently runs the loop below -- or one of the more complex ones from later notebooks\n",
    "    # the advantage here is that we can modify the training loop in one place and it will work anywhere\n",
    "\n",
    "    ############\n",
    "    # TRAINING #\n",
    "    ############\n",
    "\n",
    "#     # go through training epochs applying dropout each time\n",
    "#     for epoch in xrange(training_epochs):\n",
    "#         # go through training set\n",
    "#         c = []\n",
    "#         for batch_index in xrange(n_train_batches):\n",
    "#             c.append(train_da(batch_index))\n",
    "\n",
    "#         print 'Training epoch %d, cost ' % epoch, numpy.mean(c)\n",
    "\n",
    "#     end_time = timeit.default_timer()\n",
    "\n",
    "#     training_time = (end_time - start_time)\n",
    "# % ((training_time) / 60.)\n",
    "#     print('The training code for file ran for %.2fm' % ((training_time) / 60.))\n",
    "\n",
    "#     return da\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, average training cost 403.82\n",
      "epoch 2, average training cost 367.49\n",
      "epoch 3, average training cost 352.36\n",
      "epoch 4, average training cost 344.99\n",
      "epoch 5, average training cost 340.99\n",
      "epoch 6, average training cost 338.65\n",
      "epoch 7, average training cost 337.11\n",
      "epoch 8, average training cost 336.09\n",
      "epoch 9, average training cost 335.33\n",
      "epoch 10, average training cost 334.79\n",
      "Optimization complete!\n"
     ]
    }
   ],
   "source": [
    "from dl4mt.datasets import prep_dataset\n",
    "from dl4mt.training import train_model\n",
    "\n",
    "# load the training data\n",
    "VECTOR_INDEX_PATH = os.path.join(DATASET_LOCATION, WORD_BY_WORD_MATRIX)\n",
    "CUTOFF = 1000\n",
    "  \n",
    "train_dataset = prep_dataset(POS_DATASET_PATH, VECTOR_INDEX_PATH, which_sets=['train'], cutoff=CUTOFF)\n",
    "\n",
    "# initialize the autoencoder model\n",
    "initialization_data = initialize_dA(train_dataset, learning_rate=0.1, corruption_level=0.3, batch_size=50)\n",
    "\n",
    "classifier, train_model_func, validate_model_func, n_train_batches, n_valid_batches = initialization_data\n",
    "    \n",
    "# load the training function and train the LR model \n",
    "# TODO: -- train_model should return the validation error as a list, then we can plot it\n",
    "# in general, train_model should return any useful information about the training process\n",
    "train_model(train_model_func,  n_train_batches, validate_model=validate_model_func,\n",
    "            n_valid_batches=n_valid_batches, training_epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TODO: make the dev and test data loadable in the same way as the training data below\n",
    "\n",
    "# make a theano function to get predictions from a trained model\n",
    "training_data = theano.tensor.matrix('training_X')\n",
    "predictions = classifier.predict(training_data)\n",
    "get_predictions = theano.function([training_data], predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# get predictions and evaluate\n",
    "p = get_predictions(train_dataset[0].get_value())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib qt\n",
    "\n",
    "# import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "\n",
    "# TODO: Set this at the top of the notebook -- this is the visualization cutoff, not the training cutoff\n",
    "CUTOFF_BEGIN=0\n",
    "CUTOFF_END=1000\n",
    "\n",
    "# TODO: this method of visualizing the labels loses the class tags -- we want to know which color is which\n",
    "y_vals = np.array([y[0] for y in train_y[CUTOFF_BEGIN:CUTOFF_END]])\n",
    "norm_y_vals = y_vals / float(np.amax(y_vals))\n",
    "\n",
    "jitter1 = np.random.normal(loc=0.0, scale=0.05, size=CUTOFF_END-CUTOFF_BEGIN)\n",
    "jitter2 = np.random.normal(loc=0.0, scale=0.05, size=CUTOFF_END-CUTOFF_BEGIN)\n",
    "x1 = p[CUTOFF_BEGIN:CUTOFF_END,0] + jitter1\n",
    "x2 = p[CUTOFF_BEGIN:CUTOFF_END,1] + jitter2\n",
    "\n",
    "\n",
    "plt.scatter(x1, x2, c=norm_y_vals, s=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({0: 17,\n",
       "         1: 319,\n",
       "         2: 118,\n",
       "         3: 19,\n",
       "         4: 133,\n",
       "         5: 125,\n",
       "         6: 12,\n",
       "         7: 152,\n",
       "         9: 18,\n",
       "         10: 31,\n",
       "         11: 56})"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_counts = Counter(y_vals)\n",
    "y_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# WORKING: how to visualize the filters in an autoencoder for text?\n",
    "# Idea: pass the training data through and store the top N instances which activate this node\n",
    "# at the end, output the top 5 instances for each node (convert window instance back to text)\n",
    "\n",
    "# WORKING: visualize 2d autoencoder output\n",
    "\n",
    "# WORKING: vizualize 2d stacked vs 2d \"tied weights\" autoencoders\n",
    "\n",
    "# WORKING: Documents don't really give us any information about POS -- we need a better way of representing words\n",
    "# idea: replicate most common 1000 words 4x\n",
    "# encode each word with a 4000-dimensional vector indicating words which occur in POS-2, -1, +1, +2\n",
    "\n",
    "# A stacked autoencoder can be used as part of an MLP, where the layers are \"fine-tuned\" for the task\n",
    "\n",
    "# SIMPLIFICATION IDEA -- ONLY SELECT A SUBSET OF CLASSES (I.e. just noun vs. adj), THIS SHOULD MAKE COMPARISON EASIER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load fuel dataset (POS windows) DONE\n",
    "# Load one-hot Word X Document numpy matrix DONE\n",
    "\n",
    "# train autoencoder, stop when reconstruction performance doesn't improve any more, or after a fixed number of epochs\n",
    "\n",
    "# CHALLENGE: implement denoising/dropout -- i.e. implement the corruption\n",
    "\n",
    "# save the parameters of the first part of the network (the encoder)\n",
    "# use the encoder parameters to build the prediction transformation\n",
    "\n",
    "# evaluate the performance of the autoencoder features vs. hstacked LSI and W2V features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'DET'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset_X.get_value()[1].nonzero()[0])\n",
    "len(dataset_X.get_value()[1])\n",
    "dataset_X.get_value().shape\n",
    "[corpus_indices['idx2word'][w] for w in train_X[63624]]\n",
    "corpus_indices['idx2tag'][train_y[63624][0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Working Notes\n",
    "# In a separate notebook, compare 2d autoencoder vs 2d LSI -- visualize with scatter plots\n",
    "# cite Hinton etc for this visualization methodology "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
