{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# use windows from hdf5 dataset -- extract the distributions around words to build our baseline representation\n",
    "# the idea is to encode each word by the words which occur around it\n",
    "# this should give us a good idea of the distributional tendencies of this word\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import cPickle\n",
    "from collections import Counter, defaultdict\n",
    "from itertools import chain\n",
    "\n",
    "import numpy as np\n",
    "from fuel.datasets import H5PYDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# the constants that we'll need in this notebook\n",
    "\n",
    "# let's find the N most frequent tokens in our corpus \n",
    "TOKEN_FREQ_CUTOFF = 25 # this is an important hyperparameter because it controls the sparsity \n",
    "# of the window representation\n",
    "TOKS_IN_WINDOW = 4 # window size - 1 because we delete the middle token\n",
    "\n",
    "DATASET_LOCATION = '../../datasets/' # the directory where we store datasets\n",
    "\n",
    "# the pos dataset consists of windows around words\n",
    "POS_DATASET_NAME = 'brown_pos_dataset.hdf5'\n",
    "POS_DATASET_PATH = os.path.join(DATASET_LOCATION, POS_DATASET_NAME)\n",
    "\n",
    "CORPUS_INDICES = 'brown_pos_dataset.indices' \n",
    "VECTOR_INDEX = 'brown.word-by-word.normalized.npy' # the name of the index we'll create\n",
    "\n",
    "# Indexes for mapping words <--> ints\n",
    "with open(os.path.join(DATASET_LOCATION, CORPUS_INDICES)) as indices_file:\n",
    "    corpus_indices = cPickle.load(indices_file)\n",
    "\n",
    "UNKNOWN_TOKEN = u'_UNK_'    \n",
    "UNKNOWN_TOKEN_IDX = corpus_indices['word2idx'][UNKNOWN_TOKEN]\n",
    "\n",
    "train_X, train_y = H5PYDataset(\n",
    "    POS_DATASET_PATH, which_sets=('train',),\n",
    "    sources=['instances', 'targets'], load_in_memory=True).data_sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "token_counts = Counter(chain(*[i for i in train_X]))\n",
    "vocab_size = len(token_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "top_N_tokens = set(k for k,v in token_counts.most_common()[:TOKEN_FREQ_CUTOFF])\n",
    "\n",
    "# map to new 0-based indices\n",
    "top_N_index = {old_idx: new_idx for new_idx,old_idx in enumerate(top_N_tokens)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# function to check if tok is in top N, else it's top_N_index[UNKNOWN_TOKEN]\n",
    "def idx_or_unk(index, token):\n",
    "    if token in index:\n",
    "        return index[token]\n",
    "    return index[UNKNOWN_TOKEN_IDX]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# now initialize matrix of zeros\n",
    "# iterate through the window corpus and increment counts\n",
    "# in the slice of the features corresponding to the token's position in the window\n",
    "num_rows = vocab_size\n",
    "\n",
    "num_cols = TOKS_IN_WINDOW * TOKEN_FREQ_CUTOFF\n",
    "\n",
    "word_by_word_index = np.zeros((num_rows, num_cols), dtype=\"float32\")\n",
    "\n",
    "for instance in train_X:\n",
    "    pos_token = instance[2] # TODO: parameterize getting the middle of the window -- this assumes window_size=5\n",
    "    new_idxs = [idx_or_unk(top_N_index, tok) for tok in instance]\n",
    "    # delete the token itself\n",
    "    del new_idxs[2]\n",
    "    \n",
    "    # now iterate over idx and create the one-hot encoding at the right place\n",
    "    for i,idx in enumerate(new_idxs):\n",
    "        word_by_word_index[pos_token, idx + (i * TOKEN_FREQ_CUTOFF)] += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'_UNK_',\n",
       " u'of',\n",
       " u'in',\n",
       " u',',\n",
       " u'to',\n",
       " u'on',\n",
       " u'for',\n",
       " u'at',\n",
       " u'and',\n",
       " u'that',\n",
       " u'with',\n",
       " u'by',\n",
       " u'is',\n",
       " u'as',\n",
       " u'was',\n",
       " u'be',\n",
       " u'_START_',\n",
       " u'``',\n",
       " u\"''\",\n",
       " u'_END_',\n",
       " u'the',\n",
       " u'The',\n",
       " u'he',\n",
       " u'a',\n",
       " u'.']"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# normalize counts by row to 0-1\n",
    "word_by_word_index = np.array([row / float(row.max()) for row in word_by_word_index]).astype('float32')\n",
    "\n",
    "# TESTING\n",
    "# sanity ('the' is = 7524)\n",
    "reverse_top_N = {v:k for k,v in top_N_index.items()}\n",
    "\n",
    "max_idxs = word_by_word_index[7524][TOKEN_FREQ_CUTOFF:TOKEN_FREQ_CUTOFF*2].argsort()[::-1]\n",
    "[corpus_indices['idx2word'][reverse_top_N[idx]] for idx in max_idxs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO: implement real-valued representation -- right now the autoencoder only works with binary\n",
    "# TODO: non-autoencoders can use real-valued features\n",
    "word_by_word_index[word_by_word_index.nonzero()] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# persist the new index\n",
    "with open(os.path.join(DATASET_LOCATION, VECTOR_INDEX), 'wb') as outfile:\n",
    "    np.save(outfile, word_by_word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
