{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create a w2v index from a filtered version of the Brown corpus from NLTK\n",
    "# these vectors will be used as the initial input for our Deep Learning models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from __future__ import division, print_function\n",
    "import codecs\n",
    "import os\n",
    "import cPickle\n",
    "import logging\n",
    "from collections import Counter, defaultdict\n",
    "\n",
    "import nltk\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pylab\n",
    "import pandas as pd\n",
    "from scipy.stats import norm\n",
    "from gensim.models import Word2Vec\n",
    "from fuel.datasets import H5PYDataset\n",
    "\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15667\n"
     ]
    }
   ],
   "source": [
    "# load our version of the brown dataset, and get an iterator over all of the documents\n",
    "DATASET_LOCATION = '../../datasets/'\n",
    "DATASET_NAME = 'brown_pos_dataset.hdf5'\n",
    "DATASET_PATH = os.path.join(DATASET_LOCATION, DATASET_NAME)\n",
    "\n",
    "with open(os.path.join(DATASET_LOCATION, 'brown_pos_dataset.indices')) as indices_file:\n",
    "    corpus_indices = cPickle.load(indices_file)\n",
    "    \n",
    "# ok lets load the brown corpus, and use the indexes to convert it to ints,\n",
    "# then build the W2V index over this corpus\n",
    "UNKNOWN_TOKEN = u'_UNK_'\n",
    "\n",
    "def map_to_unknown(tok, index):\n",
    "    if tok in index:\n",
    "        return tok\n",
    "    else:\n",
    "        return UNKNOWN_TOKEN\n",
    "\n",
    "brown_documents = [[w for p in d for w in p] for d in nltk.corpus.brown.paras()]\n",
    "print(len(brown_documents))\n",
    "\n",
    "# Gensim expects strings, so we'll need to map back to indices to use our vectors consistently\n",
    "# let's also pad with start and end\n",
    "brown_documents = [[u'_START_'] + [map_to_unknown(w, corpus_indices['word2idx']) for w in d] + [u'_END_'] \n",
    "                   for d in brown_documents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_word2vec(text_iterator, model_file='w2v_model', workers=6, vec_size=100, min_count=1):\n",
    "    \"\"\"\n",
    "    Trains word2vec model using the corpus contained in text_iterator\n",
    "  \n",
    "    Parameters:\n",
    "        Model is stored in <model_file>\n",
    "        <workers> controls the number of processors Word2Vec can use\n",
    "        min_count is the minimum number of occurences for a word to be included in the model\n",
    "  \n",
    "    Returns:\n",
    "        The model contains vectors of <vec_size> dimensions (default 100)\n",
    "    \"\"\"\n",
    "\n",
    "    docs = text_iterator\n",
    "\n",
    "    model = Word2Vec(docs, size=vec_size, workers=workers, iter=1, min_count=min_count) \n",
    "    model.save(model_file)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:gensim.models.word2vec:consider setting layer size to a multiple of 4 for greater performance\n"
     ]
    }
   ],
   "source": [
    "EMBEDDING_SIZE = 50\n",
    "\n",
    "w2v_model = train_word2vec(brown_documents,\n",
    "                           model_file=os.path.join(DATASET_LOCATION, 'brown_w2v_model'),\n",
    "                           vec_size=EMBEDDING_SIZE)\n",
    "\n",
    "orig_w2v_vectors = w2v_model.syn0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "set()"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(set(corpus_indices['word2idx'].keys())).difference(w2v_model.vocab.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# reindex w2v_vectors to correspond to our index\n",
    "w2v_index_order = [w2v_model.vocab[w].index \n",
    "                   for w,v in sorted(corpus_indices['word2idx'].items(), key=lambda x: x[1])]\n",
    "\n",
    "final_w2v_vectors = orig_w2v_vectors[w2v_index_order]\n",
    "\n",
    "# now persist this version for later\n",
    "with open(os.path.join(DATASET_LOCATION, 'brown_w2v_vectors.npy'), 'wb') as outfile:\n",
    "    np.save(outfile, final_w2v_vectors)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Challenges: \n",
    "    \n",
    "(1)\n",
    "- try using the word2vec index in some of the experiments from day1 (there is a pre-trained index in dl4mt_exercises/datasets)\n",
    "- how does the performance compare with the simple embedding we were previously using?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
