{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "# make sure the dl4mt module is on the path\n",
    "sys.path.append('../..')\n",
    "sys.path.append('/home/chris/projects/dl4mt_labs/dcu_deep_learning/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ../../dl4mt/logistic_regression.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ../../dl4mt/logistic_regression.py\n",
    "# Uncomment to save this cell to file in order to import it in later notebooks\n",
    "# save this class for use in other notebooks\n",
    "\n",
    "import cPickle\n",
    "import gzip\n",
    "import os\n",
    "import sys\n",
    "import timeit\n",
    "\n",
    "import numpy\n",
    "\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "\n",
    "\n",
    "class LogisticRegression(object):\n",
    "    \"\"\"Multi-class Logistic Regression Class\n",
    "\n",
    "    The logistic regression is fully described by a weight matrix :math:`W`\n",
    "    and bias vector :math:`b`. Classification is done by projecting data\n",
    "    points onto a set of hyperplanes, the distance to which is used to\n",
    "    determine a class membership probability.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input, n_in, n_out):\n",
    "        \"\"\" Initialize the parameters of the logistic regression\n",
    "\n",
    "        :type input: theano.tensor.TensorType\n",
    "        :param input: symbolic variable that describes the input of the\n",
    "                      architecture (one minibatch)\n",
    "\n",
    "        :type n_in: int\n",
    "        :param n_in: number of input units, the dimension of the space in\n",
    "                     which the datapoints lie\n",
    "\n",
    "        :type n_out: int\n",
    "        :param n_out: number of output units, the dimension of the space in\n",
    "                      which the labels lie\n",
    "\n",
    "        \"\"\"\n",
    "  \n",
    "        # initialize with 0 the weights W as a matrix of shape (n_in, n_out)\n",
    "        self.W = theano.shared(\n",
    "            value=numpy.zeros(\n",
    "                (n_in, n_out),\n",
    "                dtype=theano.config.floatX\n",
    "            ),\n",
    "            name='W',\n",
    "            borrow=True\n",
    "        )\n",
    "        # initialize the biases b as a vector of n_out 0s\n",
    "        self.b = theano.shared(\n",
    "            value=numpy.zeros(\n",
    "                (n_out,),\n",
    "                dtype=theano.config.floatX\n",
    "            ),\n",
    "            name='b',\n",
    "            borrow=True\n",
    "        )\n",
    "\n",
    "        # symbolic expression for computing the matrix of class-membership\n",
    "        # probabilities\n",
    "        # Where:\n",
    "        # W is a matrix where column-k represent the separation hyperplane for\n",
    "        # class-k\n",
    "        # x is a matrix where row-j  represents input training sample-j\n",
    "        # b is a vector where element-k represent the free parameter of\n",
    "        # hyperplane-k\n",
    "        self.p_y_given_x = T.nnet.softmax(T.dot(input, self.W) + self.b)\n",
    "\n",
    "        # symbolic description of how to compute prediction as class whose\n",
    "        # probability is maximal\n",
    "        self.y_pred = T.argmax(self.p_y_given_x, axis=1)\n",
    "        # end-snippet-1\n",
    "\n",
    "        # parameters of the model\n",
    "        self.params = [self.W, self.b]\n",
    "\n",
    "        # keep track of model input\n",
    "        self.input = input\n",
    "\n",
    "    # TODO: put math in a separate cell with LaTeX + markdown\n",
    "    def negative_log_likelihood(self, y):\n",
    "        \"\"\"Return the mean of the negative log-likelihood of the prediction\n",
    "        of this model under a given target distribution.\n",
    "\n",
    "        .. math::\n",
    "\n",
    "            \\frac{1}{|\\mathcal{D}|} \\mathcal{L} (\\theta=\\{W,b\\}, \\mathcal{D}) =\n",
    "            \\frac{1}{|\\mathcal{D}|} \\sum_{i=0}^{|\\mathcal{D}|}\n",
    "                \\log(P(Y=y^{(i)}|x^{(i)}, W,b)) \\\\\n",
    "            \\ell (\\theta=\\{W,b\\}, \\mathcal{D})\n",
    "\n",
    "        :type y: theano.tensor.TensorType\n",
    "        :param y: corresponds to a vector that gives for each example the\n",
    "                  correct label\n",
    "\n",
    "        Note: we use the mean instead of the sum so that\n",
    "              the learning rate is less dependent on the batch size\n",
    "        \"\"\"\n",
    "\n",
    "        # y.shape[0] is (symbolically) the number of rows in y, i.e.,\n",
    "        # number of examples (call it n) in the minibatch\n",
    "        # T.arange(y.shape[0]) is a symbolic vector which will contain\n",
    "        # [0,1,2,... n-1] T.log(self.p_y_given_x) is a matrix of\n",
    "        # Log-Probabilities (call it LP) with one row per example and\n",
    "        # one column per class LP[T.arange(y.shape[0]),y] is a vector\n",
    "        # v containing [LP[0,y[0]], LP[1,y[1]], LP[2,y[2]], ...,\n",
    "        # LP[n-1,y[n-1]]] and T.mean(LP[T.arange(y.shape[0]),y]) is\n",
    "        # the mean (across minibatch examples) of the elements in v,\n",
    "        # i.e., the mean log-likelihood across the minibatch.\n",
    "        return -T.mean(T.log(self.p_y_given_x)[T.arange(y.shape[0]), y])\n",
    "\n",
    "    def errors(self, y):\n",
    "        \"\"\"Return a float representing the number of errors in the minibatch\n",
    "        over the total number of examples of the minibatch ; zero one\n",
    "        loss over the size of the minibatch\n",
    "\n",
    "        :type y: theano.tensor.TensorType\n",
    "        :param y: corresponds to a vector that gives for each example the\n",
    "                  correct label\n",
    "        \"\"\"\n",
    "\n",
    "        # check if y has same dimension of y_pred\n",
    "        if y.ndim != self.y_pred.ndim:\n",
    "            raise TypeError(\n",
    "                'y should have the same shape as self.y_pred',\n",
    "                ('y', y.type, 'y_pred', self.y_pred.type)\n",
    "            )\n",
    "        # check if y is of the correct datatype\n",
    "        if y.dtype.startswith('int'):\n",
    "            # the T.neq operator returns a vector of 0s and 1s, where 1\n",
    "            # represents a mistake in prediction\n",
    "            return T.mean(T.neq(self.y_pred, y))\n",
    "        else:\n",
    "            raise NotImplementedError()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from dl4mt.logistic_regression import LogisticRegression\n",
    "\n",
    "def train_logistic_regression(train_dataset, dev_dataset, learning_rate=0.1,\n",
    "                              training_epochs=25, batch_size=10):\n",
    "    \n",
    "    \"\"\"\n",
    "    Demonstrate stochastic gradient descent optimization of a log-linear\n",
    "    model\n",
    "\n",
    "    This is demonstrated on MNIST.\n",
    "\n",
    "    :type learning_rate: float\n",
    "    :param learning_rate: learning rate used (factor for the stochastic\n",
    "                          gradient)\n",
    "\n",
    "    :type n_epochs: int\n",
    "    :param n_epochs: maximal number of epochs to run the optimizer\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    train_set_x, train_set_y = train_dataset\n",
    "    valid_set_x, valid_set_y = dev_dataset\n",
    "\n",
    "    # compute number of minibatches for training, validation and testing\n",
    "    n_train_batches = train_set_x.get_value(borrow=True).shape[0] / batch_size\n",
    "    n_valid_batches = valid_set_x.get_value(borrow=True).shape[0] / batch_size\n",
    "#     n_test_batches = test_set_x.get_value(borrow=True).shape[0] / batch_size\n",
    "\n",
    "    ######################\n",
    "    # BUILD ACTUAL MODEL #\n",
    "    ######################\n",
    "    print '... building the model'\n",
    "\n",
    "    # allocate symbolic variables for the data\n",
    "    index = T.lscalar()  # index to a [mini]batch\n",
    "\n",
    "    # generate symbolic variables for input (x and y represent a minibatch)\n",
    "    x = T.matrix('x')  # data\n",
    "    y = T.ivector('y')  # labels, presented as 1D vector of [int] labels\n",
    "\n",
    "    # TODO: parameterize  n_out outside the training function -- try to get directly from data\n",
    "    n_in = train_set_x.get_value().shape[1]\n",
    "    n_out = 12\n",
    "    \n",
    "    classifier = LogisticRegression(input=x, n_in=n_in, n_out=n_out)\n",
    "\n",
    "    # the cost we minimize during training is the negative log likelihood of\n",
    "    # the model in symbolic format\n",
    "    cost = classifier.negative_log_likelihood(y)\n",
    "\n",
    "    validate_model = theano.function(\n",
    "        inputs=[index],\n",
    "        outputs=classifier.errors(y),\n",
    "        givens={\n",
    "            x: valid_set_x[index * batch_size: (index + 1) * batch_size],\n",
    "            y: valid_set_y[index * batch_size: (index + 1) * batch_size]\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # compute the gradient of cost with respect to theta = (W,b)\n",
    "    g_W = T.grad(cost=cost, wrt=classifier.W)\n",
    "    g_b = T.grad(cost=cost, wrt=classifier.b)\n",
    "\n",
    "    # specify how to update the parameters of the model as a list of\n",
    "    # (variable, update expression) pairs.\n",
    "    updates = [(classifier.W, classifier.W - learning_rate * g_W),\n",
    "               (classifier.b, classifier.b - learning_rate * g_b)]\n",
    "\n",
    "    # compiling a Theano function `train_model` that returns the cost, but in\n",
    "    # the same time updates the parameter of the model based on the rules\n",
    "    # defined in `updates`\n",
    "    train_model = theano.function(\n",
    "        inputs=[index],\n",
    "        outputs=cost,\n",
    "        updates=updates,\n",
    "        givens={\n",
    "            x: train_set_x[index * batch_size: (index + 1) * batch_size],\n",
    "            y: train_set_y[index * batch_size: (index + 1) * batch_size]\n",
    "        }\n",
    "    )\n",
    "\n",
    "#     TODO: refactor this part into a shared function which is the generic training loop\n",
    "\n",
    "    ###############\n",
    "    # TRAIN MODEL #\n",
    "    ###############\n",
    "    print '... training the model'\n",
    "    # early-stopping parameters\n",
    "    patience = 5000  # look as this many examples regardless\n",
    "    patience_increase = 2  # wait this much longer when a new best is\n",
    "                                  # found\n",
    "    improvement_threshold = 0.995  # a relative improvement of this much is\n",
    "                                  # considered significant\n",
    "    validation_frequency = min(n_train_batches, patience / 2)\n",
    "                                  # go through this many\n",
    "                                  # minibatche before checking the network\n",
    "                                  # on the validation set; in this case we\n",
    "                                  # check every epoch\n",
    "\n",
    "    best_validation_loss = numpy.inf\n",
    "    test_score = 0.\n",
    "    start_time = timeit.default_timer()\n",
    "\n",
    "    done_looping = False\n",
    "    epoch = 0\n",
    "    while (epoch < training_epochs) and (not done_looping):\n",
    "        epoch = epoch + 1\n",
    "        for minibatch_index in xrange(n_train_batches):\n",
    "\n",
    "            minibatch_avg_cost = train_model(minibatch_index)\n",
    "            # iteration number\n",
    "            iter = (epoch - 1) * n_train_batches + minibatch_index\n",
    "\n",
    "            if (iter + 1) % validation_frequency == 0:\n",
    "                # compute zero-one loss on validation set\n",
    "                validation_losses = [validate_model(i)\n",
    "                                     for i in xrange(n_valid_batches)]\n",
    "                this_validation_loss = numpy.mean(validation_losses)\n",
    "\n",
    "                print(\n",
    "                    'epoch %i, minibatch %i/%i, validation error %f %%' %\n",
    "                    (\n",
    "                        epoch,\n",
    "                        minibatch_index + 1,\n",
    "                        n_train_batches,\n",
    "                        this_validation_loss * 100.\n",
    "                    )\n",
    "                )\n",
    "\n",
    "                # if we got the best validation score until now\n",
    "                if this_validation_loss < best_validation_loss:\n",
    "                    #improve patience if loss improvement is good enough\n",
    "                    if this_validation_loss < best_validation_loss *  \\\n",
    "                       improvement_threshold:\n",
    "                        patience = max(patience, iter * patience_increase)\n",
    "\n",
    "                    best_validation_loss = this_validation_loss\n",
    "\n",
    "                    # save the best model\n",
    "                    with open('best_model.pkl', 'w') as f:\n",
    "                        cPickle.dump(classifier, f)\n",
    "\n",
    "            if patience <= iter:\n",
    "                done_looping = True\n",
    "                break\n",
    "\n",
    "    end_time = timeit.default_timer()\n",
    "    print(\n",
    "        (\n",
    "            'Optimization complete with best validation score of %f %%'\n",
    "        )\n",
    "        % (best_validation_loss * 100.)\n",
    "    )\n",
    "    \n",
    "    return classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12\n",
      "... building the model\n",
      "... training the model\n",
      "epoch 1, minibatch 200/200, validation error 26.295000 %\n",
      "epoch 2, minibatch 200/200, validation error 22.475000 %\n",
      "epoch 3, minibatch 200/200, validation error 21.030000 %\n",
      "epoch 4, minibatch 200/200, validation error 20.125000 %\n",
      "epoch 5, minibatch 200/200, validation error 19.400000 %\n",
      "epoch 6, minibatch 200/200, validation error 18.915000 %\n",
      "epoch 7, minibatch 200/200, validation error 18.595000 %\n",
      "epoch 8, minibatch 200/200, validation error 18.235000 %\n",
      "epoch 9, minibatch 200/200, validation error 17.945000 %\n",
      "epoch 10, minibatch 200/200, validation error 17.740000 %\n",
      "epoch 11, minibatch 200/200, validation error 17.590000 %\n",
      "epoch 12, minibatch 200/200, validation error 17.430000 %\n",
      "epoch 13, minibatch 200/200, validation error 17.290000 %\n",
      "epoch 14, minibatch 200/200, validation error 17.140000 %\n",
      "epoch 15, minibatch 200/200, validation error 17.050000 %\n",
      "epoch 16, minibatch 200/200, validation error 16.960000 %\n",
      "epoch 17, minibatch 200/200, validation error 16.880000 %\n",
      "epoch 18, minibatch 200/200, validation error 16.780000 %\n",
      "epoch 19, minibatch 200/200, validation error 16.725000 %\n",
      "epoch 20, minibatch 200/200, validation error 16.665000 %\n",
      "epoch 21, minibatch 200/200, validation error 16.620000 %\n",
      "epoch 22, minibatch 200/200, validation error 16.585000 %\n",
      "epoch 23, minibatch 200/200, validation error 16.565000 %\n",
      "epoch 24, minibatch 200/200, validation error 16.510000 %\n",
      "epoch 25, minibatch 200/200, validation error 16.495000 %\n",
      "epoch 26, minibatch 200/200, validation error 16.525000 %\n",
      "epoch 27, minibatch 200/200, validation error 16.500000 %\n",
      "epoch 28, minibatch 200/200, validation error 16.440000 %\n",
      "epoch 29, minibatch 200/200, validation error 16.445000 %\n",
      "epoch 30, minibatch 200/200, validation error 16.415000 %\n",
      "epoch 31, minibatch 200/200, validation error 16.360000 %\n",
      "epoch 32, minibatch 200/200, validation error 16.350000 %\n",
      "epoch 33, minibatch 200/200, validation error 16.335000 %\n",
      "epoch 34, minibatch 200/200, validation error 16.340000 %\n",
      "epoch 35, minibatch 200/200, validation error 16.295000 %\n",
      "Optimization complete with best validation score of 16.295000 %,with test performance 0.000000 %\n"
     ]
    }
   ],
   "source": [
    "# TODO: create prep_dataset in this notebook or allow user to load another distributed representation, such as PCA\n",
    "import os\n",
    "from dl4mt.datasets import prep_dataset\n",
    "\n",
    "# load the training data\n",
    "DATASET_LOCATION = 'datasets/'\n",
    "\n",
    "# the pos dataset consists of windows around words\n",
    "POS_DATASET_NAME = 'brown_pos_dataset.hdf5'\n",
    "POS_DATASET_PATH = os.path.join(DATASET_LOCATION, POS_DATASET_NAME)\n",
    "WORD_BY_WORD_MATRIX = 'brown.word-by-word.normalized.npy'\n",
    "VECTOR_INDEX_PATH = os.path.join(DATASET_LOCATION, WORD_BY_WORD_MATRIX)\n",
    "\n",
    "CUTOFF = 20000\n",
    "  \n",
    "train_dataset = prep_dataset(POS_DATASET_PATH, VECTOR_INDEX_PATH, which_sets=['train'], cutoff=CUTOFF)\n",
    "dev_dataset = prep_dataset(POS_DATASET_PATH, VECTOR_INDEX_PATH, which_sets=['dev'], cutoff=CUTOFF)\n",
    "\n",
    "# train the LR model\n",
    "lr_model = train_logistic_regression(train_dataset, dev_dataset, learning_rate=0.1,\n",
    "                                     training_epochs=200, batch_size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO: visualize the error with different training hyperparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# def predict():\n",
    "#     \"\"\"\n",
    "#     An example of how to load a trained model and use it\n",
    "#     to predict labels.\n",
    "#     \"\"\"\n",
    "\n",
    "#     # load the saved model\n",
    "#     classifier = cPickle.load(open('best_model.pkl'))\n",
    "\n",
    "#     # compile a predictor function\n",
    "#     predict_model = theano.function(\n",
    "#         inputs=[classifier.input],\n",
    "#         outputs=classifier.y_pred)\n",
    "\n",
    "#     # We can test it on some examples from test test\n",
    "#     dataset='mnist.pkl.gz'\n",
    "#     datasets = load_data(dataset)\n",
    "#     test_set_x, test_set_y = datasets[2]\n",
    "#     test_set_x = test_set_x.get_value()\n",
    "\n",
    "#     predicted_values = predict_model(test_set_x[:10])\n",
    "#     print (\"Predicted values for the first 10 examples in test set:\")\n",
    "#     print predicted_values\n",
    "\n",
    "\n",
    "# test it on the test set -- TODO: testing should be done outside of training -- move this\n",
    "#                     test_losses = [test_model(i)\n",
    "#                                    for i in xrange(n_test_batches)]\n",
    "#                     test_score = numpy.mean(test_losses)\n",
    "\n",
    "#                     print(\n",
    "#                         (\n",
    "#                             '     epoch %i, minibatch %i/%i, test error of'\n",
    "#                             ' best model %f %%'\n",
    "#                         ) %\n",
    "#                         (\n",
    "#                             epoch,\n",
    "#                             minibatch_index + 1,\n",
    "#                             n_train_batches,\n",
    "#                             test_score * 100.\n",
    "#                         )\n",
    "#                     )\n",
    "\n",
    "#     print 'The code run for %d epochs, with %f epochs/sec' % (\n",
    "#         epoch, 1. * epoch / (end_time - start_time))\n",
    "#     print >> sys.stderr, ('The code for file ' +\n",
    "#                           os.path.split(__file__)[1] +\n",
    "#                           ' ran for %.1fs' % ((end_time - start_time)))\n",
    "\n",
    "    # compiling a Theano function that computes the mistakes that are made by\n",
    "    # the model on a minibatch\n",
    "#     test_model = theano.function(\n",
    "#         inputs=[index],\n",
    "#         outputs=classifier.errors(y),\n",
    "#         givens={\n",
    "#             x: test_set_x[index * batch_size: (index + 1) * batch_size],\n",
    "#             y: test_set_y[index * batch_size: (index + 1) * batch_size]\n",
    "#         }\n",
    "#     )\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "This tutorial introduces logistic regression using Theano and stochastic\n",
    "gradient descent.\n",
    "\n",
    "Logistic regression is a probabilistic, linear classifier. It is parametrized\n",
    "by a weight matrix :math:`W` and a bias vector :math:`b`. Classification is\n",
    "done by projecting data points onto a set of hyperplanes, the distance to\n",
    "which is used to determine a class membership probability.\n",
    "\n",
    "Mathematically, this can be written as:\n",
    "\n",
    ".. math::\n",
    "  P(Y=i|x, W,b) &= softmax_i(W x + b) \\\\\n",
    "                &= \\frac {e^{W_i x + b_i}} {\\sum_j e^{W_j x + b_j}}\n",
    "\n",
    "\n",
    "The output of the model or prediction is then done by taking the argmax of\n",
    "the vector whose i'th element is P(Y=i|x).\n",
    "\n",
    ".. math::\n",
    "\n",
    "  y_{pred} = argmax_i P(Y=i|x,W,b)\n",
    "\n",
    "\n",
    "This tutorial presents a stochastic gradient descent optimization method\n",
    "suitable for large datasets.\n",
    "\n",
    "\n",
    "References:\n",
    "\n",
    "    - textbooks: \"Pattern Recognition and Machine Learning\" -\n",
    "                 Christopher M. Bishop, section 4.3.2\n",
    "\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
